{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VPFc50fnT1HI",
        "outputId": "a50048bb-b08f-4152-e1b9-8f9f8fc14783"
      },
      "outputs": [],
      "source": [
        "!pip install torch torchvision pandas numpy matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pjDbStcST50f",
        "outputId": "76759b43-0a20-4fbd-e967-9dba81c77a61"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TGdmktGoWGfC",
        "outputId": "05e1ea50-6a5a-4404-8d4b-a26795fcd9c2"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/AlvinWen428/spatial-relation-benchmark.git\n",
        "%cd spatial-relation-benchmark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WAFFO_fVWJod",
        "outputId": "2437eb36-5a2d-4ae3-eaee-2bd8753fd133"
      },
      "outputs": [],
      "source": [
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vJm7cr1xWNBH",
        "outputId": "9c7d2dcb-3e6f-4181-b974-8896bf44afed"
      },
      "outputs": [],
      "source": [
        "%cd /content/spatial-relation-benchmark\n",
        "!mkdir -p data/spatialsense\n",
        "!wget https://zenodo.org/api/records/8104370/files-archive -O spatialsense.zip\n",
        "!unzip spatialsense.zip -d data/spatialsense/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aD1SmJIcZ63E",
        "outputId": "4b9108b5-0020-48da-b7d3-21b5f039548c"
      },
      "outputs": [],
      "source": [
        "!mkdir -p data/spatialsense/images\n",
        "!tar -zxvf data/spatialsense/images.tar.gz -C data/spatialsense/images\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 313
        },
        "id": "v9eWbi1CYmJc",
        "outputId": "fadd1a04-e353-4caf-ab9c-e0c56651dd66"
      },
      "outputs": [],
      "source": [
        "!pip install gdown\n",
        "import gdown\n",
        "\n",
        "# ID du fichier Google Drive pour les annotations SpatialSense+\n",
        "file_id = \"1vIOozqk3OlxkxZgL356pD1EAGt06ZwM4\"\n",
        "output_path = \"data/spatialsense/annots_spatialsenseplus.json\"\n",
        "\n",
        "# Télécharger les annotations\n",
        "url = f\"https://drive.google.com/uc?id={file_id}\"\n",
        "gdown.download(url, output_path, quiet=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jyipv9_PXpmI",
        "outputId": "df00c7b4-17a3-4cf3-c300-ac99aa6be3fa"
      },
      "outputs": [],
      "source": [
        "!ls -l data/spatialsense/\n",
        "!ls -l data/spatialsense/images/\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "0e37bzsweFqy",
        "outputId": "94e73942-4d69-4c06-f9e5-8b1be38b0636"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "\n",
        "class SpatialSensePredicateAnalyzer:\n",
        "    def __init__(self, data_dir=\"data/spatialsense\"):\n",
        "        self.data_dir = data_dir\n",
        "        self.annotations = self._load_annotations()\n",
        "        self.annotations_plus = self._load_annotations_plus()\n",
        "\n",
        "        # Relations SpatialSense+ officielles (selon le tableau)\n",
        "        self.official_spatialsense_relations = [\n",
        "            'above', 'behind', 'in', 'in front of', 'next to',\n",
        "            'on', 'to the left of', 'to the right of', 'under'\n",
        "        ]\n",
        "\n",
        "    def _load_annotations(self):\n",
        "        \"\"\"Charge et parse les annotations originales\"\"\"\n",
        "        try:\n",
        "            with open(os.path.join(self.data_dir, \"annotations.json\"), 'r') as f:\n",
        "                data = json.load(f)\n",
        "                print(f\"   Annotations originales chargées: {len(data) if isinstance(data, list) else 'structure complexe'}\")\n",
        "                return data\n",
        "        except Exception as e:\n",
        "            print(f\"    Erreur chargement annotations.json: {e}\")\n",
        "            return []\n",
        "\n",
        "    def _load_annotations_plus(self):\n",
        "        \"\"\"Charge et parse les annotations SpatialSense+\"\"\"\n",
        "        try:\n",
        "            with open(os.path.join(self.data_dir, \"annots_spatialsenseplus.json\"), 'r') as f:\n",
        "                data = json.load(f)\n",
        "                print(f\"   Annotations SpatialSense+ chargées\")\n",
        "                return data\n",
        "        except Exception as e:\n",
        "            print(f\"       Pas d'annotations SpatialSense+ trouvées: {e}\")\n",
        "            return {}\n",
        "\n",
        "    def extract_all_predicates(self):\n",
        "        \"\"\"Extrait TOUS les prédicats des deux sources\"\"\"\n",
        "        predicates_original = []\n",
        "        predicates_plus = []\n",
        "\n",
        "        print(\"\\n   Extraction des prédicats...\")\n",
        "\n",
        "        # 1. Prédicats des annotations originales\n",
        "        if isinstance(self.annotations, list):\n",
        "            for img_data in self.annotations:\n",
        "                if 'annotations' in img_data:\n",
        "                    for ann in img_data['annotations']:\n",
        "                        if 'predicate' in ann and ann.get('label', False):\n",
        "                            predicates_original.append(ann['predicate'])\n",
        "\n",
        "        # 2. Prédicats des annotations SpatialSense+\n",
        "        if isinstance(self.annotations_plus, dict):\n",
        "            # Adapter selon la structure réelle\n",
        "            for key, value in self.annotations_plus.items():\n",
        "                if isinstance(value, list):\n",
        "                    for item in value:\n",
        "                        if isinstance(item, dict) and 'predicate' in item:\n",
        "                            predicates_plus.append(item['predicate'])\n",
        "                        elif isinstance(item, dict) and 'annotations' in item:\n",
        "                            for ann in item['annotations']:\n",
        "                                if 'predicate' in ann and ann.get('label', False):\n",
        "                                    predicates_plus.append(ann['predicate'])\n",
        "        elif isinstance(self.annotations_plus, list):\n",
        "            for img_data in self.annotations_plus:\n",
        "                if 'annotations' in img_data:\n",
        "                    for ann in img_data['annotations']:\n",
        "                        if 'predicate' in ann and ann.get('label', False):\n",
        "                            predicates_plus.append(ann['predicate'])\n",
        "\n",
        "        return predicates_original, predicates_plus\n",
        "\n",
        "    def analyze_predicates(self):\n",
        "        \"\"\"Analyse complète de tous les prédicats\"\"\"\n",
        "        predicates_original, predicates_plus = self.extract_all_predicates()\n",
        "\n",
        "        # Comptage des prédicats\n",
        "        count_original = Counter(predicates_original)\n",
        "        count_plus = Counter(predicates_plus) if predicates_plus else Counter()\n",
        "\n",
        "        # Combinaison de tous les prédicats uniques\n",
        "        all_predicates = set(predicates_original + predicates_plus)\n",
        "\n",
        "        print(f\"\\n   ANALYSE COMPLETE DES PREDICATS\")\n",
        "        print(\"=\"*60)\n",
        "        print(f\"Total prédicats annotations originales: {len(predicates_original)}\")\n",
        "        print(f\"Total prédicats SpatialSense+: {len(predicates_plus)}\")\n",
        "        print(f\"Prédicats uniques (total): {len(all_predicates)}\")\n",
        "\n",
        "        return {\n",
        "            'predicates_original': count_original,\n",
        "            'predicates_plus': count_plus,\n",
        "            'all_unique_predicates': sorted(all_predicates),\n",
        "            'total_original': len(predicates_original),\n",
        "            'total_plus': len(predicates_plus),\n",
        "            'unique_count': len(all_predicates)\n",
        "        }\n",
        "\n",
        "    def display_all_predicates(self):\n",
        "        \"\"\"Affiche TOUS les prédicats trouvés\"\"\"\n",
        "        results = self.analyze_predicates()\n",
        "\n",
        "        print(f\"\\n   TOUS LES PREDICATS UNIQUES TROUVES ({len(results['all_unique_predicates'])}):\")\n",
        "        print(\"=\"*80)\n",
        "\n",
        "        for i, predicate in enumerate(results['all_unique_predicates'], 1):\n",
        "            # Compter occurrences dans chaque source\n",
        "            count_orig = results['predicates_original'].get(predicate, 0)\n",
        "            count_plus = results['predicates_plus'].get(predicate, 0)\n",
        "            total_count = count_orig + count_plus\n",
        "\n",
        "            # Vérifier si c'est une relation SpatialSense+ officielle\n",
        "            is_official = predicate in self.official_spatialsense_relations\n",
        "            status = \"  \" if is_official else \"❓\"\n",
        "\n",
        "            print(f\"{i:2d}. {status} '{predicate}' → Total: {total_count} \"\n",
        "                  f\"(Orig: {count_orig}, Plus: {count_plus})\")\n",
        "\n",
        "    def analyze_mapping_needs(self):\n",
        "        \"\"\"Analyse quels prédicats nécessitent un mapping vers SpatialSense+\"\"\"\n",
        "        results = self.analyze_predicates()\n",
        "\n",
        "        print(f\"\\n  ANALYSE DU MAPPING NECESSAIRE\")\n",
        "        print(\"=\"*60)\n",
        "\n",
        "        # Classer les prédicats\n",
        "        official_predicates = []\n",
        "        need_mapping = []\n",
        "        unclear = []\n",
        "\n",
        "        for predicate in results['all_unique_predicates']:\n",
        "            count_orig = results['predicates_original'].get(predicate, 0)\n",
        "            count_plus = results['predicates_plus'].get(predicate, 0)\n",
        "            total_count = count_orig + count_plus\n",
        "\n",
        "            if predicate in self.official_spatialsense_relations:\n",
        "                official_predicates.append((predicate, total_count))\n",
        "            else:\n",
        "                # Essayer de déterminer le mapping\n",
        "                mapping = self._suggest_mapping(predicate)\n",
        "                if mapping:\n",
        "                    need_mapping.append((predicate, mapping, total_count))\n",
        "                else:\n",
        "                    unclear.append((predicate, total_count))\n",
        "\n",
        "        # Affichage des résultats\n",
        "        print(f\"\\n   PREDICATS OFFICIELS SPATIALSENSE+ ({len(official_predicates)}):\")\n",
        "        for pred, count in sorted(official_predicates, key=lambda x: x[1], reverse=True):\n",
        "            print(f\"   '{pred}': {count} occurrences\")\n",
        "\n",
        "        print(f\"\\n   PREDICATS NECESSITANT UN MAPPING ({len(need_mapping)}):\")\n",
        "        for pred, mapping, count in sorted(need_mapping, key=lambda x: x[2], reverse=True):\n",
        "            print(f\"   '{pred}' → '{mapping}': {count} occurrences\")\n",
        "\n",
        "        print(f\"\\n❓ PREDICATS AMBIGUS ({len(unclear)}):\")\n",
        "        for pred, count in sorted(unclear, key=lambda x: x[1], reverse=True):\n",
        "            print(f\"   '{pred}': {count} occurrences\")\n",
        "\n",
        "        return {\n",
        "            'official': official_predicates,\n",
        "            'need_mapping': need_mapping,\n",
        "            'unclear': unclear\n",
        "        }\n",
        "\n",
        "    def _suggest_mapping(self, predicate):\n",
        "        \"\"\"Suggère un mapping vers une relation SpatialSense+ officielle\"\"\"\n",
        "        predicate_lower = predicate.lower().strip()\n",
        "\n",
        "        # Mapping suggéré basé sur la sémantique\n",
        "        mapping_suggestions = {\n",
        "            'over': 'above',\n",
        "            'on top of': 'above',\n",
        "            'below': 'under',\n",
        "            'beneath': 'under',\n",
        "            'underneath': 'under',\n",
        "            'upon': 'on',\n",
        "            'front': 'in front of',\n",
        "            'in front': 'in front of',\n",
        "            'beside': 'next to',\n",
        "            'adjacent': 'next to',\n",
        "            'adjacent to': 'next to',\n",
        "            'left of': 'to the left of',\n",
        "            'right of': 'to the right of',\n",
        "            'left': 'to the left of',\n",
        "            'right': 'to the right of',\n",
        "            'to the left': 'to the left of',\n",
        "            'to the right': 'to the right of',\n",
        "            'inside': 'in',\n",
        "            'within': 'in',\n",
        "            'near': 'next to',\n",
        "            'close to': 'next to',\n",
        "            'nearby': 'next to',\n",
        "            'far': 'next to',\n",
        "            'outside': 'next to',\n",
        "            'surrounding': 'next to',\n",
        "            'between': 'next to',\n",
        "            'touching': 'on',\n",
        "            'against': 'on'\n",
        "        }\n",
        "\n",
        "        return mapping_suggestions.get(predicate_lower, None)\n",
        "\n",
        "    def visualize_predicate_distribution(self):\n",
        "        \"\"\"Visualise la distribution des prédicats\"\"\"\n",
        "        results = self.analyze_predicates()\n",
        "\n",
        "        # Préparer les données pour la visualisation\n",
        "        all_counts = {}\n",
        "        for pred in results['all_unique_predicates']:\n",
        "            count_orig = results['predicates_original'].get(pred, 0)\n",
        "            count_plus = results['predicates_plus'].get(pred, 0)\n",
        "            all_counts[pred] = count_orig + count_plus\n",
        "\n",
        "        # Sélectionner les top 20 prédicats\n",
        "        top_predicates = sorted(all_counts.items(), key=lambda x: x[1], reverse=True)[:20]\n",
        "\n",
        "        # Créer la visualisation\n",
        "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 8))\n",
        "\n",
        "        # 1. Distribution des top prédicats\n",
        "        predicates, counts = zip(*top_predicates)\n",
        "        colors = ['green' if pred in self.official_spatialsense_relations else 'orange'\n",
        "                 for pred in predicates]\n",
        "\n",
        "        bars = ax1.bar(range(len(predicates)), counts, color=colors, alpha=0.7)\n",
        "        ax1.set_xlabel('Prédicats')\n",
        "        ax1.set_ylabel('Nombre d\\'occurrences')\n",
        "        ax1.set_title('Top 20 Prédicats par Fréquence')\n",
        "        ax1.set_xticks(range(len(predicates)))\n",
        "        ax1.set_xticklabels(predicates, rotation=45, ha='right')\n",
        "\n",
        "        # Ajouter les valeurs sur les barres\n",
        "        for bar, count in zip(bars, counts):\n",
        "            height = bar.get_height()\n",
        "            ax1.text(bar.get_x() + bar.get_width()/2., height + 5,\n",
        "                    f'{count}', ha='center', va='bottom')\n",
        "\n",
        "        # Légende\n",
        "        ax1.legend(['SpatialSense+ Officiel', 'Nécessite Mapping'],\n",
        "                  loc='upper right')\n",
        "\n",
        "        # 2. Camembert des types de prédicats\n",
        "        mapping_analysis = self.analyze_mapping_needs()\n",
        "\n",
        "        sizes = [\n",
        "            len(mapping_analysis['official']),\n",
        "            len(mapping_analysis['need_mapping']),\n",
        "            len(mapping_analysis['unclear'])\n",
        "        ]\n",
        "        labels = ['Officiels SpatialSense+', 'Nécessitent Mapping', 'Ambigus']\n",
        "        colors = ['green', 'orange', 'red']\n",
        "\n",
        "        ax2.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', startangle=90)\n",
        "        ax2.set_title('Répartition des Types de Prédicats')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig('predicates_analysis.png', dpi=300, bbox_inches='tight')\n",
        "        plt.show()\n",
        "\n",
        "    def generate_mapping_code(self):\n",
        "        \"\"\"Génère le code de mapping automatiquement\"\"\"\n",
        "        mapping_analysis = self.analyze_mapping_needs()\n",
        "\n",
        "        print(f\"\\n   CODE DE MAPPING GENERE AUTOMATIQUEMENT:\")\n",
        "        print(\"=\"*70)\n",
        "        print(\"spatialsense_exact_mapping = {\")\n",
        "\n",
        "        # Relations officielles (identité)\n",
        "        print(\"    # Relations officielles SpatialSense+\")\n",
        "        for pred, _ in mapping_analysis['official']:\n",
        "            print(f\"    '{pred}': '{pred}',\")\n",
        "\n",
        "        print(\"\\n    # Variantes nécessitant un mapping\")\n",
        "        for pred, mapping, _ in mapping_analysis['need_mapping']:\n",
        "            print(f\"    '{pred}': '{mapping}',\")\n",
        "\n",
        "        print(\"\\n    # Relations ambigües (mapping par défaut)\")\n",
        "        for pred, _ in mapping_analysis['unclear']:\n",
        "            print(f\"    '{pred}': 'next to',  # TODO: vérifier ce mapping\")\n",
        "\n",
        "        print(\"}\")\n",
        "\n",
        "    def print_sample_data(self):\n",
        "        \"\"\"Affiche un échantillon des données pour comprendre leur structure\"\"\"\n",
        "        print(\"\\n=== ECHANTILLON DES DONNEES ===\")\n",
        "\n",
        "        print(\"\\n   Structure annotations.json:\")\n",
        "        if self.annotations and len(self.annotations) > 0:\n",
        "            print(\"Premier élément:\")\n",
        "            sample = self.annotations[0]\n",
        "            print(f\"  Clés: {list(sample.keys()) if isinstance(sample, dict) else 'Non-dict'}\")\n",
        "            if 'annotations' in sample and sample['annotations']:\n",
        "                ann_sample = sample['annotations'][0]\n",
        "                print(f\"  Première annotation - Clés: {list(ann_sample.keys())}\")\n",
        "                if 'predicate' in ann_sample:\n",
        "                    print(f\"  Exemple prédicat: '{ann_sample['predicate']}'\")\n",
        "\n",
        "        print(\"\\n   Structure annots_spatialsenseplus.json:\")\n",
        "        if self.annotations_plus:\n",
        "            if isinstance(self.annotations_plus, dict):\n",
        "                print(f\"  Type: Dictionnaire avec clés: {list(self.annotations_plus.keys())}\")\n",
        "            elif isinstance(self.annotations_plus, list):\n",
        "                print(f\"  Type: Liste avec {len(self.annotations_plus)} éléments\")\n",
        "            else:\n",
        "                print(f\"  Type: {type(self.annotations_plus)}\")\n",
        "\n",
        "def main():\n",
        "    \"\"\"Fonction principale d'analyse\"\"\"\n",
        "    print(\"   ANALYSEUR COMPLET DES PREDICATS SPATIALSENSE\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    # Initialiser l'analyseur\n",
        "    analyzer = SpatialSensePredicateAnalyzer()\n",
        "\n",
        "    # 1. Afficher la structure des données\n",
        "    analyzer.print_sample_data()\n",
        "\n",
        "    # 2. Afficher TOUS les prédicats\n",
        "    analyzer.display_all_predicates()\n",
        "\n",
        "    # 3. Analyser les besoins de mapping\n",
        "    analyzer.analyze_mapping_needs()\n",
        "\n",
        "    # 4. Générer le code de mapping\n",
        "    analyzer.generate_mapping_code()\n",
        "\n",
        "    # 5. Créer les visualisations\n",
        "    try:\n",
        "        analyzer.visualize_predicate_distribution()\n",
        "    except Exception as e:\n",
        "        print(f\"       Erreur lors de la visualisation: {e}\")\n",
        "\n",
        "    print(f\"\\n   Analyse terminée!\")\n",
        "    print(f\"   Fichier généré: predicates_analysis.png\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Hszn9fargdFE",
        "outputId": "4c636822-cf03-49e4-ddc1-5180e24ff4aa"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import matplotlib.patches as patches\n",
        "import numpy as np\n",
        "\n",
        "class SpatialVisualizer:\n",
        "    def __init__(self, data_dir=\"data/spatialsense\"):\n",
        "        self.data_dir = data_dir\n",
        "        self.annotations = self._load_annotations()\n",
        "\n",
        "    def _load_annotations(self):\n",
        "        with open(os.path.join(self.data_dir, \"annotations.json\"), 'r') as f:\n",
        "            return json.load(f)\n",
        "\n",
        "    def _find_image_path(self, image_url):\n",
        "        base_dir = os.path.join(self.data_dir, \"images/images\")\n",
        "        filename = os.path.basename(image_url)\n",
        "\n",
        "        if \"staticflickr\" in image_url or len(filename.split('_')) == 2:\n",
        "            return os.path.join(base_dir, \"flickr\", filename)\n",
        "        else:\n",
        "            return os.path.join(base_dir, \"nyu\", filename)\n",
        "\n",
        "    def _draw_bbox(self, ax, bbox, color, name, obj_type):\n",
        "        \"\"\"\n",
        "        Dessine une bounding box avec son label\n",
        "        bbox format: [y1, y2, x1, x2]\n",
        "        \"\"\"\n",
        "        try:\n",
        "            y1, y2, x1, x2 = bbox  # Correct order for the coordinates\n",
        "            width = x2 - x1\n",
        "            height = y2 - y1\n",
        "\n",
        "            # Créer le rectangle en utilisant x1,y1 comme point de départ\n",
        "            rect = patches.Rectangle(\n",
        "                (x1, y1),  # Point de départ (coin supérieur gauche)\n",
        "                width,     # Largeur\n",
        "                height,    # Hauteur\n",
        "                linewidth=2,\n",
        "                edgecolor=color,\n",
        "                facecolor='none'\n",
        "            )\n",
        "            ax.add_patch(rect)\n",
        "\n",
        "            # Ajouter le label au-dessus de la box\n",
        "            plt.text(\n",
        "                x1, y1 - 5,\n",
        "                f\"{obj_type}: {name}\",\n",
        "                color=color,\n",
        "                fontsize=10,\n",
        "                bbox=dict(facecolor='white', alpha=0.7, edgecolor='none')\n",
        "            )\n",
        "        except Exception as e:\n",
        "            print(f\"Erreur lors du dessin de la bbox: {e}\")\n",
        "            print(f\"bbox: {bbox}\")\n",
        "\n",
        "    def visualize_sample(self, index=0):\n",
        "        image_data = self.annotations[index]\n",
        "        print(f\"Image URL: {image_data['url']}\")\n",
        "        print(f\"Dimensions: {image_data['width']}x{image_data['height']}\")\n",
        "        print(f\"Split: {image_data['split']}\")\n",
        "\n",
        "        img_path = self._find_image_path(image_data['url'])\n",
        "        print(f\"\\nChemin de l'image: {img_path}\")\n",
        "\n",
        "        try:\n",
        "            img = Image.open(img_path)\n",
        "            img_array = np.array(img)\n",
        "\n",
        "            valid_annotations = [ann for ann in image_data['annotations'] if ann['label']]\n",
        "            print(f\"Nombre de relations valides: {len(valid_annotations)}\")\n",
        "\n",
        "            for i, ann in enumerate(valid_annotations):\n",
        "                plt.figure(figsize=(12, 8))\n",
        "                plt.imshow(img_array)\n",
        "\n",
        "                # Afficher les coordonnées pour le débogage\n",
        "                print(f\"\\nRelation {i+1}:\")\n",
        "                print(f\"Subject bbox: {ann['subject']['bbox']}\")\n",
        "                print(f\"Object bbox: {ann['object']['bbox']}\")\n",
        "\n",
        "                # Traiter le sujet (rouge)\n",
        "                subject = ann['subject']\n",
        "                self._draw_bbox(plt.gca(), subject['bbox'], 'red', subject['name'], 'subject')\n",
        "\n",
        "                # Traiter l'objet (bleu)\n",
        "                object_ = ann['object']\n",
        "                self._draw_bbox(plt.gca(), object_['bbox'], 'blue', object_['name'], 'object')\n",
        "\n",
        "                plt.title(f\"Relation {i+1}: {subject['name']} → {ann['predicate']} → {object_['name']}\")\n",
        "                plt.axis('off')\n",
        "                plt.tight_layout()\n",
        "                plt.show()\n",
        "\n",
        "                print(f\"Prédicat: {ann['predicate']}\")\n",
        "                print(f\"Sujet: {subject['name']} à ({subject['x']}, {subject['y']})\")\n",
        "                print(f\"Objet: {object_['name']} à ({object_['x']}, {object_['y']})\")\n",
        "                print(\"---\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Erreur lors du chargement de l'image: {e}\")\n",
        "\n",
        "# Utilisation\n",
        "visualizer = SpatialVisualizer()\n",
        "print(\"=== Visualisation de l'exemple ===\")\n",
        "visualizer.visualize_sample(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "bLf2tvINocOt",
        "outputId": "6750b3ea-99df-40ec-b436-bba0a50735f3"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import json\n",
        "import os\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tqdm import tqdm\n",
        "import cv2\n",
        "from collections import Counter, defaultdict\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# =============================================================================\n",
        "# CONFIGURATION GLOBALE\n",
        "# =============================================================================\n",
        "\n",
        "# Configuration GPU\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Device utilisé: {DEVICE}\")\n",
        "\n",
        "# Hyperparamètres (basés sur l'article Haldekar et al. 2017)\n",
        "BATCH_SIZE = 10  # Comme dans l'article\n",
        "LEARNING_RATE = 0.001  # AdamOptimizer avec lr=0.001\n",
        "EPOCHS = 10\n",
        "K_FOLDS = 5\n",
        "IMG_SIZE = 224  # Taille standard pour VGGNet (comme l'article)\n",
        "DROPOUT_RATE = 0.5  # Comme dans l'article\n",
        "\n",
        "# Relations spatiales EXACTES de SpatialSense+ selon le tableau fourni\n",
        "SPATIAL_RELATIONS = [\n",
        "    'above',        # Position plus haute dans la direction de la gravité\n",
        "    'behind',       # Profondeur du sujet plus grande que l'objet\n",
        "    'in',          # Sujet à l'intérieur de l'objet (semi-enclos)\n",
        "    'in front of', # Profondeur du sujet plus petite que l'objet\n",
        "    'next to',     # Sujet proche de l'objet, pas d'obstacle entre eux\n",
        "    'on',          # Sujet sur le dessus de l'objet avec contact\n",
        "    'to the left of',  # Sujet à gauche de l'objet (vue du labeller)\n",
        "    'to the right of', # Sujet à droite de l'objet (vue du labeller)\n",
        "    'under'        # Position plus basse dans la direction de la gravité\n",
        "]\n",
        "\n",
        "print(f\"Relations SpatialSense+ configurées: {len(SPATIAL_RELATIONS)}\")\n",
        "print(\"Relations supportées:\")\n",
        "for i, rel in enumerate(SPATIAL_RELATIONS):\n",
        "    print(f\"  {i}: {rel}\")\n",
        "\n",
        "# =============================================================================\n",
        "# DATASET SPATIAL SENSE+ ADAPTÉ\n",
        "# =============================================================================\n",
        "\n",
        "class SpatialSenseDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Dataset SpatialSense+ avec relations exactes du tableau\n",
        "    Implémentation fidèle à Haldekar et al. 2017\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, data_dir, split='train', transform=None):\n",
        "        \"\"\"\n",
        "        Initialise le dataset SpatialSense+\n",
        "\n",
        "        Args:\n",
        "            data_dir: Répertoire racine contenant les données\n",
        "            split: 'train', 'val', ou 'test'\n",
        "            transform: Transformations PyTorch à appliquer\n",
        "        \"\"\"\n",
        "        self.data_dir = data_dir\n",
        "        self.split = split\n",
        "        self.transform = transform\n",
        "\n",
        "        # Mapping exact selon SpatialSense+ (défini AVANT le chargement)\n",
        "        self.relation_to_idx = {rel: idx for idx, rel in enumerate(SPATIAL_RELATIONS)}\n",
        "        self.idx_to_relation = {idx: rel for rel, idx in self.relation_to_idx.items()}\n",
        "\n",
        "        # Chargement des données\n",
        "        self.annotations = self._load_annotations()\n",
        "        self.data_samples = self._prepare_samples()\n",
        "\n",
        "        print(f\"Dataset SpatialSense+ {split} initialisé avec {len(self.data_samples)} échantillons\")\n",
        "        if len(self.data_samples) > 0:\n",
        "            self._print_statistics()\n",
        "\n",
        "    def _load_annotations(self):\n",
        "        \"\"\"Charge les annotations SpatialSense+\"\"\"\n",
        "        annotations_path = os.path.join(self.data_dir, 'annotations.json')\n",
        "        try:\n",
        "            with open(annotations_path, 'r') as f:\n",
        "                return json.load(f)\n",
        "        except FileNotFoundError:\n",
        "            print(f\"Erreur: Fichier {annotations_path} non trouvé!\")\n",
        "            print(\"Vérifiez que le dataset SpatialSense+ est dans le bon répertoire\")\n",
        "            return []\n",
        "        except json.JSONDecodeError as e:\n",
        "            print(f\"Erreur lors du décodage JSON: {e}\")\n",
        "            return []\n",
        "\n",
        "    def _find_image_path(self, image_url):\n",
        "        \"\"\"Trouve le chemin local d'une image SpatialSense+\"\"\"\n",
        "        base_dir = os.path.join(self.data_dir, \"images\", \"images\")\n",
        "        filename = os.path.basename(image_url)\n",
        "\n",
        "        # Organisation SpatialSense+ : deux dossiers flickr et nyu\n",
        "        if \"staticflickr\" in image_url or len(filename.split('_')) == 2:\n",
        "            return os.path.join(base_dir, \"flickr\", filename)\n",
        "        else:\n",
        "            return os.path.join(base_dir, \"nyu\", filename)\n",
        "\n",
        "    def _should_skip_sample_spatialsense(self, annotation, relation):\n",
        "        \"\"\"\n",
        "        Applique les critères de filtrage SpatialSense+ selon le tableau:\n",
        "        - behind/in front of: skip si objets dans directions très différentes\n",
        "        - to the left of/to the right of: skip si différence depth/height trop grande\n",
        "\n",
        "        Note: Implémentation basique car nécessiterait les bounding boxes détaillées\n",
        "        \"\"\"\n",
        "        # Pour l'instant, on garde tous les échantillons\n",
        "        # Dans une implémentation complète, on vérifierait:\n",
        "        # 1. Pour 'behind'/'in front of': angle entre objets\n",
        "        # 2. Pour 'to the left of'/'to the right of': différence de profondeur/hauteur\n",
        "        return False\n",
        "\n",
        "    def _prepare_samples(self):\n",
        "        \"\"\"\n",
        "        Prépare les échantillons selon SpatialSense+\n",
        "        Filtre selon les critères du tableau\n",
        "        \"\"\"\n",
        "        samples = []\n",
        "        images_not_found = 0\n",
        "        relations_mapped = Counter()\n",
        "        relations_filtered = 0\n",
        "\n",
        "        for img_data in self.annotations:\n",
        "            # Filtrer selon le split\n",
        "            if img_data['split'] != self.split:\n",
        "                continue\n",
        "\n",
        "            img_path = self._find_image_path(img_data['url'])\n",
        "\n",
        "            # Vérifier que l'image existe\n",
        "            if not os.path.exists(img_path):\n",
        "                images_not_found += 1\n",
        "                continue\n",
        "\n",
        "            # Pour chaque annotation positive dans l'image\n",
        "            for ann in img_data['annotations']:\n",
        "                if ann['label']:  # Seulement les annotations positives\n",
        "                    original_relation = ann['predicate']\n",
        "                    relation = original_relation\n",
        "\n",
        "                    relations_mapped[f\"{original_relation} → {relation}\"] += 1\n",
        "\n",
        "                    # Appliquer les critères de filtrage SpatialSense+\n",
        "                    if self._should_skip_sample_spatialsense(ann, relation):\n",
        "                        relations_filtered += 1\n",
        "                        continue\n",
        "\n",
        "                    sample = {\n",
        "                        'image_path': img_path,\n",
        "                        'subject': ann['subject']['name'],\n",
        "                        'object': ann['object']['name'],\n",
        "                        'relation': relation,\n",
        "                        'original_relation': original_relation,\n",
        "                        'subject_bbox': ann['subject'].get('bbox', None),\n",
        "                        'object_bbox': ann['object'].get('bbox', None)\n",
        "                    }\n",
        "                    samples.append(sample)\n",
        "\n",
        "        # Statistiques de preprocessing\n",
        "        if images_not_found > 0:\n",
        "            print(f\"       Images non trouvées: {images_not_found}\")\n",
        "        if relations_filtered > 0:\n",
        "            print(f\"       Relations filtrées (critères SpatialSense+): {relations_filtered}\")\n",
        "\n",
        "        print(f\"  Mapping des relations appliqué (top 5):\")\n",
        "        for mapping, count in relations_mapped.most_common(5):\n",
        "            print(f\"   {mapping}: {count}\")\n",
        "\n",
        "        return samples\n",
        "\n",
        "    def _print_statistics(self):\n",
        "        \"\"\"Affiche des statistiques SpatialSense+\"\"\"\n",
        "        relation_counts = Counter([s['relation'] for s in self.data_samples])\n",
        "        print(f\"\\n   Distribution SpatialSense+ dans {self.split}:\")\n",
        "        total = len(self.data_samples)\n",
        "\n",
        "        # Afficher dans l'ordre défini par SpatialSense+\n",
        "        for relation in SPATIAL_RELATIONS:\n",
        "            count = relation_counts.get(relation, 0)\n",
        "            percentage = count / total * 100 if total > 0 else 0\n",
        "            print(f\"   {relation}: {count} ({percentage:.1f}%)\")\n",
        "\n",
        "        # Vérifier s'il y a des relations non prévues\n",
        "        unexpected = set(relation_counts.keys()) - set(SPATIAL_RELATIONS)\n",
        "        if unexpected:\n",
        "            print(f\"\\n       Relations inattendues détectées: {unexpected}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data_samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        Retourne un échantillon SpatialSense+\n",
        "        Format: image complète (224x224) + label relation + métadonnées\n",
        "        \"\"\"\n",
        "        sample = self.data_samples[idx]\n",
        "\n",
        "        try:\n",
        "            # Chargement image (comme l'article: 224x224)\n",
        "            image = Image.open(sample['image_path']).convert('RGB')\n",
        "\n",
        "            # Transformations (preprocessing VGG)\n",
        "            if self.transform:\n",
        "                image = self.transform(image)\n",
        "\n",
        "            # Label de la relation SpatialSense+\n",
        "            label = self.relation_to_idx[sample['relation']]\n",
        "\n",
        "            # Métadonnées pour analyse\n",
        "            metadata = {\n",
        "                'subject': sample['subject'],\n",
        "                'object': sample['object'],\n",
        "                'relation': sample['relation'],\n",
        "                'original_relation': sample['original_relation']\n",
        "            }\n",
        "\n",
        "            return image, label, metadata\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"    Erreur chargement {sample['image_path']}: {e}\")\n",
        "            # Image par défaut en cas d'erreur\n",
        "            dummy_image = Image.new('RGB', (224, 224), color='gray')\n",
        "            if self.transform:\n",
        "                dummy_image = self.transform(dummy_image)\n",
        "            else:\n",
        "                dummy_image = torch.zeros(3, 224, 224)\n",
        "\n",
        "            return dummy_image, 0, {\n",
        "                'subject': 'error', 'object': 'error',\n",
        "                'relation': 'next to', 'original_relation': 'error'\n",
        "            }\n",
        "\n",
        "# =============================================================================\n",
        "# ARCHITECTURE HALDEKAR ET AL. 2017 - EXACTE\n",
        "# =============================================================================\n",
        "\n",
        "class VGGFeatureExtractor(nn.Module):\n",
        "    \"\"\"\n",
        "    Extracteur de features basé sur VGG16 pré-entraîné\n",
        "    EXACTEMENT comme dans l'article Haldekar et al. 2017\n",
        "    Extrait les features de la couche FC-7 (4096 dimensions)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super(VGGFeatureExtractor, self).__init__()\n",
        "\n",
        "        # Chargement de VGG16 pré-entraîné (comme l'article)\n",
        "        vgg16 = models.vgg16(pretrained=True)\n",
        "\n",
        "        print(\"   Initialisation VGG Feature Extractor (Haldekar-style):\")\n",
        "        print(\"   - Modèle: VGG16 pré-entraîné sur ImageNet\")\n",
        "        print(\"   - Features: Couches convolutionnelles + pooling\")\n",
        "        print(\"   - Output: FC-7 (4096 dimensions)\")\n",
        "\n",
        "        # Extraction des couches convolutionnelles\n",
        "        self.features = vgg16.features\n",
        "        self.avgpool = vgg16.avgpool\n",
        "\n",
        "        # Classifier jusqu'à FC-7 (deuxième couche fully connected)\n",
        "        # VGG16 classifier: FC1(4096) -> ReLU -> Dropout -> FC2(4096) -> ReLU -> Dropout -> FC3(1000)\n",
        "        # Nous prenons jusqu'à FC2 + ReLU + Dropout (FC-7 dans l'article)\n",
        "        classifier_layers = list(vgg16.classifier.children())[:6]  # Jusqu'à FC-7\n",
        "        self.fc7 = nn.Sequential(*classifier_layers)\n",
        "\n",
        "        # Gel des poids pour l'extraction de features (comme l'article)\n",
        "        for param in self.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        print(\"   - Paramètres gelés:    (feature extraction seulement)\")\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"Forward pass pour extraire features FC-7\"\"\"\n",
        "        x = self.features(x)      # Convolutions + pooling\n",
        "        x = self.avgpool(x)       # Average pooling\n",
        "        x = torch.flatten(x, 1)   # Aplatissement\n",
        "        x = self.fc7(x)          # FC-7 (4096 dimensions)\n",
        "        return x\n",
        "\n",
        "class SpatialRelationMLP(nn.Module):\n",
        "    \"\"\"\n",
        "    MLP EXACT de l'article Haldekar et al. 2017\n",
        "    Architecture: 4096 -> 512 -> 256 -> num_relations\n",
        "    ReLU activation + Dropout 0.5\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_dim=4096, hidden1_dim=512, hidden2_dim=256,\n",
        "                 num_relations=len(SPATIAL_RELATIONS), dropout_rate=0.5):\n",
        "        super(SpatialRelationMLP, self).__init__()\n",
        "\n",
        "        print(f\"   Initialisation MLP Haldekar (architecture exacte):\")\n",
        "        print(f\"   - Input: {input_dim} (FC-7 VGG)\")\n",
        "        print(f\"   - Hidden 1 (FC-0): {hidden1_dim}\")\n",
        "        print(f\"   - Hidden 2 (FC-1): {hidden2_dim}\")\n",
        "        print(f\"   - Output: {num_relations} relations SpatialSense+\")\n",
        "        print(f\"   - Dropout: {dropout_rate}\")\n",
        "        print(f\"   - Activation: ReLU\")\n",
        "\n",
        "        # Première couche cachée (FC-0 dans l'article)\n",
        "        self.fc1 = nn.Linear(input_dim, hidden1_dim)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.dropout1 = nn.Dropout(dropout_rate)\n",
        "\n",
        "        # Deuxième couche cachée (FC-1 dans l'article)\n",
        "        self.fc2 = nn.Linear(hidden1_dim, hidden2_dim)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.dropout2 = nn.Dropout(dropout_rate)\n",
        "\n",
        "        # Couche de sortie (classification)\n",
        "        self.fc3 = nn.Linear(hidden2_dim, num_relations)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # FC-0: 4096 -> 512\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu1(x)\n",
        "        x = self.dropout1(x)\n",
        "\n",
        "        # FC-1: 512 -> 256\n",
        "        x = self.fc2(x)\n",
        "        x = self.relu2(x)\n",
        "        x = self.dropout2(x)\n",
        "\n",
        "        # Output: 256 -> num_relations\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "class SpatialRelationModel(nn.Module):\n",
        "    \"\"\"\n",
        "    Modèle complet Haldekar et al. 2017 adapté à SpatialSense+\n",
        "    VGG Feature Extractor (gelé) + MLP (entraînable)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_relations=len(SPATIAL_RELATIONS)):\n",
        "        super(SpatialRelationModel, self).__init__()\n",
        "\n",
        "        print(f\"    Modèle Haldekar et al. 2017 pour SpatialSense+:\")\n",
        "        print(f\"   - Dataset: SpatialSense+ ({num_relations} relations)\")\n",
        "        print(f\"   - Architecture: VGG16 + MLP\")\n",
        "        print(f\"   - Comparaison: Article original sur SUN09 (3 relations)\")\n",
        "\n",
        "        # Extracteur de features VGG (gelé, comme l'article)\n",
        "        self.feature_extractor = VGGFeatureExtractor()\n",
        "\n",
        "        # Classifieur MLP (entraînable, architecture exacte)\n",
        "        self.classifier = SpatialRelationMLP(num_relations=num_relations)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Extraction des features VGG FC-7 (sans gradient comme l'article)\n",
        "        with torch.no_grad():\n",
        "            features = self.feature_extractor(x)\n",
        "\n",
        "        # Classification MLP (avec gradient)\n",
        "        output = self.classifier(features)\n",
        "        return output\n",
        "\n",
        "    def get_features(self, x):\n",
        "        \"\"\"Méthode utilitaire pour obtenir les features FC-7\"\"\"\n",
        "        return self.feature_extractor(x)\n",
        "\n",
        "# =============================================================================\n",
        "# TRANSFORMATIONS (identiques à l'article)\n",
        "# =============================================================================\n",
        "\n",
        "def create_transforms():\n",
        "    \"\"\"\n",
        "    Créé les transformations pour VGG (comme l'article)\n",
        "    Normalisation ImageNet standard\n",
        "    \"\"\"\n",
        "    # Normalisation ImageNet (utilisée par VGG pré-entraîné)\n",
        "    normalize = transforms.Normalize(\n",
        "        mean=[0.485, 0.456, 0.406],  # ImageNet mean\n",
        "        std=[0.229, 0.224, 0.225]    # ImageNet std\n",
        "    )\n",
        "\n",
        "    # Transformations d'entraînement\n",
        "    train_transform = transforms.Compose([\n",
        "        transforms.Resize((IMG_SIZE, IMG_SIZE)),  # 224x224 comme l'article\n",
        "        transforms.RandomHorizontalFlip(p=0.5),  # Augmentation simple\n",
        "        transforms.ToTensor(),\n",
        "        normalize\n",
        "    ])\n",
        "\n",
        "    # Transformations de validation/test (pas d'augmentation)\n",
        "    val_transform = transforms.Compose([\n",
        "        transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "        transforms.ToTensor(),\n",
        "        normalize\n",
        "    ])\n",
        "\n",
        "    return train_transform, val_transform\n",
        "\n",
        "# =============================================================================\n",
        "# FONCTIONS D'ENTRAÎNEMENT (identiques, mais commentées pour SpatialSense+)\n",
        "# =============================================================================\n",
        "\n",
        "def train_one_epoch(model, dataloader, criterion, optimizer, device):\n",
        "    \"\"\"Entraîne le modèle pour une époque (Haldekar-style) avec gestion flexible\"\"\"\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    progress_bar = tqdm(dataloader, desc='Training SpatialSense+')\n",
        "    for batch_idx, batch_data in enumerate(progress_bar):\n",
        "        # Gestion flexible du format de batch\n",
        "        if len(batch_data) >= 2:\n",
        "            images, labels = batch_data[0], batch_data[1]\n",
        "        else:\n",
        "            continue\n",
        "\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Statistiques\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "        # Mise à jour de la barre de progression\n",
        "        progress_bar.set_postfix({\n",
        "            'loss': running_loss / (batch_idx + 1),\n",
        "            'acc': 100. * correct / total\n",
        "        })\n",
        "\n",
        "    epoch_loss = running_loss / len(dataloader)\n",
        "    epoch_acc = 100. * correct / total\n",
        "\n",
        "    return epoch_loss, epoch_acc\n",
        "\n",
        "def evaluate(model, dataloader, criterion, device):\n",
        "    \"\"\"Évalue le modèle (SpatialSense+) avec gestion flexible des métadonnées\"\"\"\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_data in tqdm(dataloader, desc='Evaluating SpatialSense+'):\n",
        "            # Gestion flexible du format de batch\n",
        "            if len(batch_data) >= 2:\n",
        "                images, labels = batch_data[0], batch_data[1]\n",
        "            else:\n",
        "                continue\n",
        "\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    avg_loss = running_loss / len(dataloader)\n",
        "    accuracy = 100. * correct / total\n",
        "\n",
        "    return avg_loss, accuracy\n",
        "\n",
        "# =============================================================================\n",
        "# NOUVELLE FONCTION: MATRICE DE CONFUSION ET ANALYSE D'ERREURS\n",
        "# =============================================================================\n",
        "\n",
        "def generate_confusion_matrix_and_analysis(model, dataloader, device, save_prefix=\"spatialsense\"):\n",
        "    \"\"\"\n",
        "    Génère une matrice de confusion détaillée et analyse les erreurs du modèle\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    # Collecte des prédictions et métadonnées\n",
        "    all_predictions = []\n",
        "    all_true_labels = []\n",
        "    all_metadata = []\n",
        "    error_examples = defaultdict(list)  # Pour stocker des exemples d'erreurs\n",
        "\n",
        "    print(\"   Génération matrice de confusion et analyse d'erreurs...\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_data in tqdm(dataloader, desc='Analyzing predictions'):\n",
        "            # Gestion flexible du format de batch\n",
        "            if len(batch_data) == 3:\n",
        "                images, labels, metadata = batch_data\n",
        "            else:\n",
        "                images, labels = batch_data[:2]\n",
        "                metadata = [{'subject': 'unknown', 'object': 'unknown', 'relation': SPATIAL_RELATIONS[labels[i].item()]} for i in range(len(labels))]\n",
        "\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            outputs = model(images)\n",
        "            probabilities = torch.softmax(outputs, dim=1)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "            # Stocker les résultats\n",
        "            for i in range(len(labels)):\n",
        "                pred_idx = predicted[i].item()\n",
        "                true_idx = labels[i].item()\n",
        "\n",
        "                all_predictions.append(pred_idx)\n",
        "                all_true_labels.append(true_idx)\n",
        "\n",
        "                # Gestion des métadonnées\n",
        "                if isinstance(metadata, list) and i < len(metadata):\n",
        "                    current_metadata = metadata[i]\n",
        "                elif isinstance(metadata, dict):\n",
        "                    # Si metadata est un dict avec des listes\n",
        "                    current_metadata = {\n",
        "                        'subject': metadata.get('subject', ['unknown'] * len(labels))[i] if 'subject' in metadata else 'unknown',\n",
        "                        'object': metadata.get('object', ['unknown'] * len(labels))[i] if 'object' in metadata else 'unknown',\n",
        "                        'relation': metadata.get('relation', [SPATIAL_RELATIONS[true_idx]] * len(labels))[i] if 'relation' in metadata else SPATIAL_RELATIONS[true_idx]\n",
        "                    }\n",
        "                else:\n",
        "                    current_metadata = {'subject': 'unknown', 'object': 'unknown', 'relation': SPATIAL_RELATIONS[true_idx]}\n",
        "\n",
        "                all_metadata.append(current_metadata)\n",
        "\n",
        "                # Stocker les erreurs avec leurs probabilités\n",
        "                if pred_idx != true_idx:\n",
        "                    true_rel = SPATIAL_RELATIONS[true_idx]\n",
        "                    pred_rel = SPATIAL_RELATIONS[pred_idx]\n",
        "                    confidence = probabilities[i, pred_idx].item()\n",
        "\n",
        "                    error_info = {\n",
        "                        'metadata': current_metadata,\n",
        "                        'predicted_relation': pred_rel,\n",
        "                        'true_relation': true_rel,\n",
        "                        'confidence': confidence,\n",
        "                        'probabilities': probabilities[i].cpu().numpy()\n",
        "                    }\n",
        "                    error_examples[(true_rel, pred_rel)].append(error_info)\n",
        "\n",
        "    # Calcul de la matrice de confusion\n",
        "    cm = confusion_matrix(all_true_labels, all_predictions)\n",
        "\n",
        "    # Visualisation de la matrice de confusion\n",
        "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(20, 16))\n",
        "\n",
        "    # 1. Matrice de confusion avec nombres absolus\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                xticklabels=SPATIAL_RELATIONS, yticklabels=SPATIAL_RELATIONS,\n",
        "                ax=ax1, cbar_kws={'shrink': 0.8})\n",
        "    ax1.set_title('Matrice de Confusion - Nombres Absolus', fontsize=14, fontweight='bold')\n",
        "    ax1.set_xlabel('Prédictions')\n",
        "    ax1.set_ylabel('Vérité Terrain')\n",
        "    ax1.tick_params(axis='x', rotation=45)\n",
        "    ax1.tick_params(axis='y', rotation=0)\n",
        "\n",
        "    # 2. Matrice de confusion normalisée (par ligne)\n",
        "    cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "    cm_normalized = np.nan_to_num(cm_normalized)  # Gérer les divisions par zéro\n",
        "\n",
        "    sns.heatmap(cm_normalized, annot=True, fmt='.2f', cmap='Reds',\n",
        "                xticklabels=SPATIAL_RELATIONS, yticklabels=SPATIAL_RELATIONS,\n",
        "                ax=ax2, cbar_kws={'shrink': 0.8})\n",
        "    ax2.set_title('Matrice de Confusion - Normalisée par Ligne', fontsize=14, fontweight='bold')\n",
        "    ax2.set_xlabel('Prédictions')\n",
        "    ax2.set_ylabel('Vérité Terrain')\n",
        "    ax2.tick_params(axis='x', rotation=45)\n",
        "    ax2.tick_params(axis='y', rotation=0)\n",
        "\n",
        "    # 3. Analyse des erreurs les plus fréquentes\n",
        "    error_counts = Counter()\n",
        "    for (true_rel, pred_rel), examples in error_examples.items():\n",
        "        if true_rel != pred_rel:\n",
        "            error_counts[(true_rel, pred_rel)] = len(examples)\n",
        "\n",
        "    top_errors = error_counts.most_common(10)\n",
        "    if top_errors:\n",
        "        error_pairs, error_freqs = zip(*top_errors)\n",
        "        error_labels = [f\"{true_rel}\\n→\\n{pred_rel}\" for true_rel, pred_rel in error_pairs]\n",
        "\n",
        "        bars = ax3.bar(range(len(error_labels)), error_freqs, color='lightcoral', alpha=0.8)\n",
        "        ax3.set_title('Top 10 des Erreurs les Plus Fréquentes', fontsize=14, fontweight='bold')\n",
        "        ax3.set_xlabel('Type d\\'Erreur (Vrai → Prédit)')\n",
        "        ax3.set_ylabel('Nombre d\\'Erreurs')\n",
        "        ax3.set_xticks(range(len(error_labels)))\n",
        "        ax3.set_xticklabels(error_labels, rotation=45, ha='right')\n",
        "        ax3.grid(True, axis='y', alpha=0.3)\n",
        "\n",
        "        # Ajouter les valeurs sur les barres\n",
        "        for bar, freq in zip(bars, error_freqs):\n",
        "            height = bar.get_height()\n",
        "            ax3.annotate(f'{freq}',\n",
        "                        xy=(bar.get_x() + bar.get_width() / 2, height),\n",
        "                        xytext=(0, 3), textcoords=\"offset points\",\n",
        "                        ha='center', va='bottom', fontsize=9)\n",
        "\n",
        "    # 4. Distribution des confiances pour les erreurs vs succès\n",
        "    correct_confidences = []\n",
        "    error_confidences = []\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for images, labels, metadata in dataloader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            probabilities = torch.softmax(outputs, dim=1)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "            for i in range(len(labels)):\n",
        "                confidence = probabilities[i, predicted[i]].item()\n",
        "                if predicted[i] == labels[i]:\n",
        "                    correct_confidences.append(confidence)\n",
        "                else:\n",
        "                    error_confidences.append(confidence)\n",
        "\n",
        "    ax4.hist(correct_confidences, bins=30, alpha=0.7, label='Prédictions Correctes',\n",
        "             color='lightgreen', density=True)\n",
        "    ax4.hist(error_confidences, bins=30, alpha=0.7, label='Erreurs',\n",
        "             color='lightcoral', density=True)\n",
        "    ax4.set_title('Distribution des Confiances', fontsize=14, fontweight='bold')\n",
        "    ax4.set_xlabel('Confiance de Prédiction')\n",
        "    ax4.set_ylabel('Densité')\n",
        "    ax4.legend()\n",
        "    ax4.grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f'{save_prefix}_confusion_matrix_analysis.png', dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "    # Rapport de classification détaillé\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"   RAPPORT DE CLASSIFICATION DÉTAILLÉ\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    report = classification_report(all_true_labels, all_predictions,\n",
        "                                 target_names=SPATIAL_RELATIONS,\n",
        "                                 output_dict=True, zero_division=0)\n",
        "\n",
        "    # Affichage formaté du rapport\n",
        "    print(f\"{'Relation':<15} {'Precision':<10} {'Recall':<10} {'F1-Score':<10} {'Support':<10}\")\n",
        "    print(\"-\" * 65)\n",
        "\n",
        "    for i, relation in enumerate(SPATIAL_RELATIONS):\n",
        "        if relation in report:\n",
        "            metrics = report[relation]\n",
        "            print(f\"{relation:<15} {metrics['precision']:<10.3f} {metrics['recall']:<10.3f} \"\n",
        "                  f\"{metrics['f1-score']:<10.3f} {int(metrics['support']):<10}\")\n",
        "\n",
        "    print(\"-\" * 65)\n",
        "    print(f\"{'Accuracy':<15} {'':<10} {'':<10} {report['accuracy']:<10.3f} {int(report['macro avg']['support']):<10}\")\n",
        "    print(f\"{'Macro avg':<15} {report['macro avg']['precision']:<10.3f} {report['macro avg']['recall']:<10.3f} \"\n",
        "          f\"{report['macro avg']['f1-score']:<10.3f} {int(report['macro avg']['support']):<10}\")\n",
        "    print(f\"{'Weighted avg':<15} {report['weighted avg']['precision']:<10.3f} {report['weighted avg']['recall']:<10.3f} \"\n",
        "          f\"{report['weighted avg']['f1-score']:<10.3f} {int(report['weighted avg']['support']):<10}\")\n",
        "\n",
        "    return cm, error_examples, report\n",
        "\n",
        "def analyze_error_patterns(error_examples, save_prefix=\"spatialsense\"):\n",
        "    \"\"\"\n",
        "    Analyse détaillée des patterns d'erreurs\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"   ANALYSE DÉTAILLÉE DES PATTERNS D'ERREURS\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    # 1. Analyse des confusions par catégorie sémantique\n",
        "    semantic_groups = {\n",
        "        'vertical': ['above', 'under', 'on'],\n",
        "        'horizontal': ['to the left of', 'to the right of', 'next to'],\n",
        "        'depth': ['behind', 'in front of'],\n",
        "        'containment': ['in']\n",
        "    }\n",
        "\n",
        "    # Matrice de confusion inter-groupes\n",
        "    group_confusion = defaultdict(lambda: defaultdict(int))\n",
        "\n",
        "    for (true_rel, pred_rel), examples in error_examples.items():\n",
        "        true_group = None\n",
        "        pred_group = None\n",
        "\n",
        "        for group, relations in semantic_groups.items():\n",
        "            if true_rel in relations:\n",
        "                true_group = group\n",
        "            if pred_rel in relations:\n",
        "                pred_group = group\n",
        "\n",
        "        if true_group and pred_group:\n",
        "            group_confusion[true_group][pred_group] += len(examples)\n",
        "\n",
        "    print(\"\\n   Confusions entre Groupes Sémantiques:\")\n",
        "    print(f\"{'Vrai→Prédit':<20} {'Vertical':<10} {'Horizontal':<12} {'Depth':<8} {'Containment':<12}\")\n",
        "    print(\"-\" * 70)\n",
        "\n",
        "    for true_group in semantic_groups.keys():\n",
        "        row = f\"{true_group:<20}\"\n",
        "        for pred_group in semantic_groups.keys():\n",
        "            count = group_confusion[true_group][pred_group]\n",
        "            row += f\"{count:<10}\"\n",
        "        print(row)\n",
        "\n",
        "    # 2. Analyse des objets/sujets les plus problématiques\n",
        "    print(\"\\n  Objets/Sujets les Plus Problématiques:\")\n",
        "\n",
        "    subject_errors = defaultdict(int)\n",
        "    object_errors = defaultdict(int)\n",
        "\n",
        "    for (true_rel, pred_rel), examples in error_examples.items():\n",
        "        for example in examples:\n",
        "            subject_errors[example['metadata']['subject']] += 1\n",
        "            object_errors[example['metadata']['object']] += 1\n",
        "\n",
        "    print(\"\\nTop 10 Sujets avec le Plus d'Erreurs:\")\n",
        "    for subject, count in Counter(subject_errors).most_common(10):\n",
        "        print(f\"   • {subject}: {count} erreurs\")\n",
        "\n",
        "    print(\"\\nTop 10 Objets avec le Plus d'Erreurs:\")\n",
        "    for obj, count in Counter(object_errors).most_common(10):\n",
        "        print(f\"   • {obj}: {count} erreurs\")\n",
        "\n",
        "    # 3. Analyse des patterns de confusion spécifiques\n",
        "    print(\"\\n   Patterns de Confusion Spécifiques:\")\n",
        "\n",
        "    # Confusions directionnelles\n",
        "    directional_confusions = {\n",
        "        ('to the left of', 'to the right of'): 'Inversion gauche-droite',\n",
        "        ('to the right of', 'to the left of'): 'Inversion droite-gauche',\n",
        "        ('above', 'under'): 'Inversion haut-bas',\n",
        "        ('under', 'above'): 'Inversion bas-haut',\n",
        "        ('behind', 'in front of'): 'Inversion profondeur',\n",
        "        ('in front of', 'behind'): 'Inversion profondeur inverse'\n",
        "    }\n",
        "\n",
        "    confusion_analysis = {}\n",
        "    for (true_rel, pred_rel), description in directional_confusions.items():\n",
        "        if (true_rel, pred_rel) in error_examples:\n",
        "            count = len(error_examples[(true_rel, pred_rel)])\n",
        "            confusion_analysis[description] = count\n",
        "\n",
        "    print(\"\\nConfusions Directionnelles:\")\n",
        "    for pattern, count in sorted(confusion_analysis.items(), key=lambda x: x[1], reverse=True):\n",
        "        print(f\"   • {pattern}: {count} cas\")\n",
        "\n",
        "    # 4. Analyse de la confiance dans les erreurs\n",
        "    print(\"\\n       Analyse de Confiance dans les Erreurs:\")\n",
        "\n",
        "    high_confidence_errors = []\n",
        "    low_confidence_errors = []\n",
        "\n",
        "    for (true_rel, pred_rel), examples in error_examples.items():\n",
        "        for example in examples:\n",
        "            if example['confidence'] > 0.7:\n",
        "                high_confidence_errors.append((true_rel, pred_rel, example['confidence']))\n",
        "            elif example['confidence'] < 0.3:\n",
        "                low_confidence_errors.append((true_rel, pred_rel, example['confidence']))\n",
        "\n",
        "    print(f\"\\nErreurs Haute Confiance (>70%): {len(high_confidence_errors)}\")\n",
        "    if high_confidence_errors:\n",
        "        high_conf_counter = Counter([(true_rel, pred_rel) for true_rel, pred_rel, _ in high_confidence_errors])\n",
        "        for (true_rel, pred_rel), count in high_conf_counter.most_common(5):\n",
        "            avg_conf = np.mean([conf for t, p, conf in high_confidence_errors if t == true_rel and p == pred_rel])\n",
        "            print(f\"   • {true_rel} → {pred_rel}: {count} cas (conf. moy: {avg_conf:.2f})\")\n",
        "\n",
        "    print(f\"\\nErreurs Basse Confiance (<30%): {len(low_confidence_errors)}\")\n",
        "    if low_confidence_errors:\n",
        "        low_conf_counter = Counter([(true_rel, pred_rel) for true_rel, pred_rel, _ in low_confidence_errors])\n",
        "        for (true_rel, pred_rel), count in low_conf_counter.most_common(5):\n",
        "            avg_conf = np.mean([conf for t, p, conf in low_confidence_errors if t == true_rel and p == pred_rel])\n",
        "            print(f\"   • {true_rel} → {pred_rel}: {count} cas (conf. moy: {avg_conf:.2f})\")\n",
        "\n",
        "    # 5. Suggestions d'amélioration\n",
        "    print(\"\\n       SUGGESTIONS D'AMÉLIORATION:\")\n",
        "\n",
        "    # Analyser les patterns pour suggérer des améliorations\n",
        "    suggestions = []\n",
        "\n",
        "    # Vérifier les confusions directionnelles\n",
        "    directional_error_rate = sum(confusion_analysis.values()) / sum(len(examples) for examples in error_examples.values()) * 100\n",
        "    if directional_error_rate > 20:\n",
        "        suggestions.append(\"Augmentation de données avec flips horizontaux/verticaux\")\n",
        "        suggestions.append(\"Pré-traitement pour normaliser l'orientation des images\")\n",
        "\n",
        "    # Vérifier la distribution des erreurs par groupe\n",
        "    max_group_errors = max(sum(group_confusion[group].values()) for group in semantic_groups.keys())\n",
        "    if max_group_errors > 50:\n",
        "        suggestions.append(\"Équilibrage du dataset par groupe sémantique\")\n",
        "        suggestions.append(\"Perte pondérée pour compenser le déséquilibre\")\n",
        "\n",
        "    # Vérifier les erreurs haute confiance\n",
        "    if len(high_confidence_errors) > 10:\n",
        "        suggestions.append(\"Révision des annotations pour les erreurs haute confiance\")\n",
        "        suggestions.append(\"Ensemble de modèles pour réduire la sur-confiance\")\n",
        "\n",
        "    # Vérifier la performance sur les objets spécifiques\n",
        "    if len(subject_errors) > 0:\n",
        "        max_subject_errors = max(subject_errors.values())\n",
        "        if max_subject_errors > 5:\n",
        "            suggestions.append(\"Features spécifiques aux objets problématiques\")\n",
        "            suggestions.append(\"Augmentation ciblée pour les objets difficiles\")\n",
        "\n",
        "    print(\"Recommandations basées sur l'analyse:\")\n",
        "    for i, suggestion in enumerate(suggestions, 1):\n",
        "        print(f\"   {i}. {suggestion}\")\n",
        "\n",
        "    if not suggestions:\n",
        "        print(\"      Le modèle montre des patterns d'erreurs acceptables\")\n",
        "\n",
        "    return group_confusion, subject_errors, object_errors\n",
        "\n",
        "def visualize_error_examples(model, dataset, error_examples, device, num_examples=6, save_prefix=\"spatialsense\"):\n",
        "    \"\"\"\n",
        "    Visualise des exemples d'erreurs spécifiques avec heatmaps\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    # Sélectionner des erreurs intéressantes\n",
        "    selected_errors = []\n",
        "\n",
        "    # Prendre les erreurs les plus fréquentes\n",
        "    error_counts = Counter()\n",
        "    for (true_rel, pred_rel), examples in error_examples.items():\n",
        "        error_counts[(true_rel, pred_rel)] = len(examples)\n",
        "\n",
        "    # Sélectionner des exemples variés\n",
        "    for (true_rel, pred_rel), count in error_counts.most_common():\n",
        "        if len(selected_errors) >= num_examples:\n",
        "            break\n",
        "        if error_examples[(true_rel, pred_rel)]:\n",
        "            # Prendre l'exemple avec la plus haute confiance (erreur la plus \"convaincue\")\n",
        "            best_example = max(error_examples[(true_rel, pred_rel)], key=lambda x: x['confidence'])\n",
        "            selected_errors.append(best_example)\n",
        "\n",
        "    if not selected_errors:\n",
        "        print(\"    Aucun exemple d'erreur à visualiser\")\n",
        "        return\n",
        "\n",
        "    fig, axes = plt.subplots(len(selected_errors), 4, figsize=(16, 4*len(selected_errors)))\n",
        "    if len(selected_errors) == 1:\n",
        "        axes = axes.reshape(1, -1)\n",
        "\n",
        "    print(f\"   Visualisation de {len(selected_errors)} exemples d'erreurs...\")\n",
        "\n",
        "    for i, error_example in enumerate(selected_errors):\n",
        "        # Rechercher l'échantillon correspondant dans le dataset\n",
        "        target_metadata = error_example['metadata']\n",
        "        sample_found = False\n",
        "\n",
        "        for j in range(len(dataset)):\n",
        "            image, label, metadata = dataset[j]\n",
        "            if (metadata['subject'] == target_metadata['subject'] and\n",
        "                metadata['object'] == target_metadata['object'] and\n",
        "                metadata['relation'] == target_metadata['relation']):\n",
        "\n",
        "                sample_found = True\n",
        "\n",
        "                # Dénormaliser pour affichage\n",
        "                img_display = image.clone()\n",
        "                mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
        "                std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n",
        "                img_display = img_display * std + mean\n",
        "                img_display = img_display.clamp(0, 1).permute(1, 2, 0)\n",
        "\n",
        "                # Prédiction\n",
        "                with torch.no_grad():\n",
        "                    image_tensor = image.unsqueeze(0).to(device)\n",
        "                    output = model(image_tensor)\n",
        "                    probs = torch.softmax(output, dim=1)\n",
        "                    pred_idx = torch.argmax(probs)\n",
        "\n",
        "                # Heatmap pour cette erreur\n",
        "                heatmap = generate_heatmap_haldekar_style(model, image, label, device)\n",
        "\n",
        "                # 1. Image originale\n",
        "                axes[i, 0].imshow(img_display)\n",
        "                axes[i, 0].set_title(f\"    Erreur #{i+1}\\n{metadata['subject']} - {metadata['object']}\")\n",
        "                axes[i, 0].axis('off')\n",
        "\n",
        "                # 2. Heatmap\n",
        "                im = axes[i, 1].imshow(heatmap, cmap='hot')\n",
        "                axes[i, 1].set_title('Heatmap Attention')\n",
        "                axes[i, 1].axis('off')\n",
        "                plt.colorbar(im, ax=axes[i, 1], shrink=0.8)\n",
        "\n",
        "                # 3. Superposition\n",
        "                axes[i, 2].imshow(img_display)\n",
        "                axes[i, 2].imshow(heatmap, cmap='jet', alpha=0.5)\n",
        "                axes[i, 2].set_title('Superposition')\n",
        "                axes[i, 2].axis('off')\n",
        "\n",
        "                # 4. Analyse détaillée\n",
        "                axes[i, 3].axis('off')\n",
        "\n",
        "                # Probabilités top-3\n",
        "                top3_indices = torch.topk(probs[0], 3).indices\n",
        "                top3_probs = torch.topk(probs[0], 3).values\n",
        "\n",
        "                prob_text = \"Top-3 Prédictions:\\n\"\n",
        "                for k, (idx, prob) in enumerate(zip(top3_indices, top3_probs)):\n",
        "                    rel_name = SPATIAL_RELATIONS[idx]\n",
        "                    marker = \" \" if k == 0 else \"  \"\n",
        "                    prob_text += f\"{marker} {rel_name}: {prob:.1%}\\n\"\n",
        "\n",
        "                info_text = f\"\"\"\n",
        "Erreur d'Analyse:\n",
        "\n",
        "Vérité: {error_example['true_relation']}\n",
        "Prédiction: {error_example['predicted_relation']}\n",
        "Confiance: {error_example['confidence']:.1%}\n",
        "\n",
        "{prob_text}\n",
        "\n",
        "Type d'Erreur:\n",
        "{get_error_type(error_example['true_relation'], error_example['predicted_relation'])}\n",
        "\n",
        "Gravité: {'🔴 Haute' if error_example['confidence'] > 0.7 else '🟡 Moyenne' if error_example['confidence'] > 0.4 else '🟢 Faible'}\n",
        "                \"\"\"\n",
        "\n",
        "                color = 'lightcoral' if error_example['confidence'] > 0.7 else 'lightyellow'\n",
        "                axes[i, 3].text(0.1, 0.5, info_text, transform=axes[i, 3].transAxes,\n",
        "                               fontsize=9, verticalalignment='center',\n",
        "                               bbox=dict(boxstyle='round', facecolor=color, alpha=0.8))\n",
        "                break\n",
        "\n",
        "        if not sample_found:\n",
        "            print(f\"       Échantillon d'erreur {i+1} non trouvé dans le dataset\")\n",
        "            for j in range(4):\n",
        "                axes[i, j].axis('off')\n",
        "                axes[i, j].text(0.5, 0.5, 'Échantillon\\nnon trouvé',\n",
        "                               ha='center', va='center', transform=axes[i, j].transAxes)\n",
        "\n",
        "    plt.suptitle('Analyse Détaillée des Erreurs les Plus Significatives',\n",
        "                 fontsize=16, fontweight='bold')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f'{save_prefix}_error_examples_analysis.png', dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "def get_error_type(true_relation, predicted_relation):\n",
        "    \"\"\"\n",
        "    Détermine le type d'erreur commise\n",
        "    \"\"\"\n",
        "    # Groupes sémantiques\n",
        "    vertical_relations = ['above', 'under', 'on']\n",
        "    horizontal_relations = ['to the left of', 'to the right of', 'next to']\n",
        "    depth_relations = ['behind', 'in front of']\n",
        "    containment_relations = ['in']\n",
        "\n",
        "    def get_group(relation):\n",
        "        if relation in vertical_relations:\n",
        "            return 'vertical'\n",
        "        elif relation in horizontal_relations:\n",
        "            return 'horizontal'\n",
        "        elif relation in depth_relations:\n",
        "            return 'depth'\n",
        "        elif relation in containment_relations:\n",
        "            return 'containment'\n",
        "        return 'unknown'\n",
        "\n",
        "    true_group = get_group(true_relation)\n",
        "    pred_group = get_group(predicted_relation)\n",
        "\n",
        "    # Analyse du type d'erreur\n",
        "    if true_group == pred_group:\n",
        "        # Erreur dans le même groupe\n",
        "        if true_group == 'vertical':\n",
        "            if (true_relation == 'above' and predicted_relation == 'under') or \\\n",
        "               (true_relation == 'under' and predicted_relation == 'above'):\n",
        "                return \"   Inversion verticale\"\n",
        "            else:\n",
        "                return \"   Confusion verticale\"\n",
        "        elif true_group == 'horizontal':\n",
        "            if (true_relation == 'to the left of' and predicted_relation == 'to the right of') or \\\n",
        "               (true_relation == 'to the right of' and predicted_relation == 'to the left of'):\n",
        "                return \"   Inversion horizontale\"\n",
        "            else:\n",
        "                return \"   Confusion horizontale\"\n",
        "        elif true_group == 'depth':\n",
        "            return \"   Inversion profondeur\"\n",
        "        else:\n",
        "            return f\"   Confusion {true_group}\"\n",
        "    else:\n",
        "        # Erreur entre groupes différents\n",
        "        return f\"   Confusion inter-groupe\\n({true_group} → {pred_group})\"\n",
        "\n",
        "# =============================================================================\n",
        "# GÉNÉRATION DE HEATMAPS (méthode de l'article)\n",
        "# =============================================================================\n",
        "\n",
        "def generate_heatmap_haldekar_style(model, image, label, device, mask_size=16, stride=8):\n",
        "    \"\"\"\n",
        "    Génère une heatmap EXACTEMENT comme dans l'article Haldekar et al. 2017\n",
        "    Méthode: masking séquentiel + mesure d'influence (entropy change)\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    # Préparer l'image\n",
        "    if len(image.shape) == 3:\n",
        "        image = image.unsqueeze(0)\n",
        "    image = image.to(device)\n",
        "\n",
        "    # Prédiction sur l'image originale (baseline)\n",
        "    with torch.no_grad():\n",
        "        original_output = model(image)\n",
        "        original_probs = torch.softmax(original_output, dim=1)\n",
        "        original_prob = original_probs[0, label].item()\n",
        "\n",
        "    # Dimensions de l'image\n",
        "    _, _, h, w = image.shape\n",
        "\n",
        "    # Matrice d'influence (comme Figure 2 de l'article)\n",
        "    influence_map = np.zeros((h // stride, w // stride))\n",
        "\n",
        "    # Application séquentielle du masque (comme l'article)\n",
        "    for i in range(0, h - mask_size, stride):\n",
        "        for j in range(0, w - mask_size, stride):\n",
        "            # Image masquée (région grise comme l'article)\n",
        "            masked_image = image.clone()\n",
        "            masked_image[:, :, i:i+mask_size, j:j+mask_size] = 0.5  # Masque gris\n",
        "\n",
        "            # Prédiction avec masque\n",
        "            with torch.no_grad():\n",
        "                masked_output = model(masked_image)\n",
        "                masked_probs = torch.softmax(masked_output, dim=1)\n",
        "                masked_prob = masked_probs[0, label].item()\n",
        "\n",
        "            # Calcul de l'influence (comme équation 2 de l'article)\n",
        "            influence = original_prob - masked_prob\n",
        "            influence_map[i // stride, j // stride] = influence\n",
        "\n",
        "    # Redimensionner à la taille originale\n",
        "    heatmap = cv2.resize(influence_map, (w, h), interpolation=cv2.INTER_CUBIC)\n",
        "\n",
        "    return heatmap\n",
        "\n",
        "# =============================================================================\n",
        "# VALIDATION K-FOLD POUR SPATIALSENSE+\n",
        "# =============================================================================\n",
        "\n",
        "def kfold_cross_validation_spatialsense(data_dir, k_folds=5, epochs=20):\n",
        "    \"\"\"\n",
        "    Validation croisée K-fold sur SpatialSense+\n",
        "    Comparaison avec les résultats Haldekar et al. 2017\n",
        "    \"\"\"\n",
        "    print(\"     Validation K-fold Haldekar et al. 2017 sur SpatialSense+\")\n",
        "    print(\"=\"*70)\n",
        "    print(f\"Dataset: SpatialSense+ ({len(SPATIAL_RELATIONS)} relations)\")\n",
        "    print(f\"Architecture: VGG16 FC-7 + MLP (4096→512→256→{len(SPATIAL_RELATIONS)})\")\n",
        "    print(f\"Comparaison: Article original SUN09 (3 relations, 55.98% test)\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    # Transformations\n",
        "    train_transform, val_transform = create_transforms()\n",
        "\n",
        "    # Dataset complet\n",
        "    full_dataset = SpatialSenseDataset(\n",
        "        data_dir=data_dir,\n",
        "        split='train',\n",
        "        transform=train_transform\n",
        "    )\n",
        "\n",
        "    if len(full_dataset) == 0:\n",
        "        print(\"    Dataset SpatialSense+ vide!\")\n",
        "        return []\n",
        "\n",
        "    # Configuration K-fold\n",
        "    kfold = KFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
        "    fold_results = []\n",
        "\n",
        "    # Pour chaque fold\n",
        "    for fold, (train_indices, val_indices) in enumerate(kfold.split(full_dataset)):\n",
        "        print(f\"\\n{'='*50}\")\n",
        "        print(f\"FOLD {fold + 1}/{k_folds} - SPATIALSENSE+\")\n",
        "        print(f\"{'='*50}\")\n",
        "\n",
        "        # Sous-ensembles\n",
        "        train_subset = torch.utils.data.Subset(full_dataset, train_indices)\n",
        "        val_subset = torch.utils.data.Subset(full_dataset, val_indices)\n",
        "\n",
        "        # DataLoaders\n",
        "        train_loader = DataLoader(\n",
        "            train_subset, batch_size=BATCH_SIZE, shuffle=True,\n",
        "            num_workers=2, pin_memory=True\n",
        "        )\n",
        "\n",
        "        val_loader = DataLoader(\n",
        "            val_subset, batch_size=BATCH_SIZE, shuffle=False,\n",
        "            num_workers=2, pin_memory=True\n",
        "        )\n",
        "\n",
        "        # Nouveau modèle Haldekar pour ce fold\n",
        "        model = SpatialRelationModel(num_relations=len(SPATIAL_RELATIONS))\n",
        "        model = model.to(DEVICE)\n",
        "\n",
        "        # Optimiseur (comme l'article: Adam sur le classifieur seulement)\n",
        "        optimizer = optim.Adam(model.classifier.parameters(), lr=LEARNING_RATE)\n",
        "        criterion = nn.CrossEntropyLoss()  # Cross entropy comme l'article\n",
        "\n",
        "        # Historique d'entraînement\n",
        "        train_losses, val_losses = [], []\n",
        "        train_accs, val_accs = [], []\n",
        "        best_val_acc = 0.0\n",
        "\n",
        "        # Boucle d'entraînement\n",
        "        for epoch in range(epochs):\n",
        "            print(f\"\\nEpoch {epoch+1}/{epochs}\")\n",
        "\n",
        "            # Phase d'entraînement\n",
        "            train_loss, train_acc = train_one_epoch(\n",
        "                model, train_loader, criterion, optimizer, DEVICE\n",
        "            )\n",
        "            train_losses.append(train_loss)\n",
        "            train_accs.append(train_acc)\n",
        "\n",
        "            # Phase de validation\n",
        "            val_loss, val_acc = evaluate(model, val_loader, criterion, DEVICE)\n",
        "            val_losses.append(val_loss)\n",
        "            val_accs.append(val_acc)\n",
        "\n",
        "            print(f\"Train - Loss: {train_loss:.4f}, Acc: {train_acc:.2f}%\")\n",
        "            print(f\"Val   - Loss: {val_loss:.4f}, Acc: {val_acc:.2f}%\")\n",
        "\n",
        "            # Sauvegarde si meilleur modèle\n",
        "            if val_acc > best_val_acc:\n",
        "                best_val_acc = val_acc\n",
        "                torch.save(model.state_dict(), f'best_spatialsense_model_fold_{fold+1}.pth')\n",
        "                print(f\"  Nouveau meilleur modèle sauvé: {val_acc:.2f}%\")\n",
        "\n",
        "        # Résultats du fold\n",
        "        fold_results.append({\n",
        "            'fold': fold + 1,\n",
        "            'train_losses': train_losses,\n",
        "            'val_losses': val_losses,\n",
        "            'train_accs': train_accs,\n",
        "            'val_accs': val_accs,\n",
        "            'best_val_acc': best_val_acc,\n",
        "            'final_train_acc': train_accs[-1],\n",
        "            'final_val_acc': val_accs[-1],\n",
        "            'model': model\n",
        "        })\n",
        "\n",
        "        # Sauvegarde modèle final du fold\n",
        "        torch.save(model.state_dict(), f'spatialsense_model_fold_{fold+1}_final.pth')\n",
        "\n",
        "        print(f\"\\n   Résumé Fold {fold + 1}:\")\n",
        "        print(f\"   - Meilleure val accuracy: {best_val_acc:.2f}%\")\n",
        "        print(f\"   - Train accuracy finale: {train_accs[-1]:.2f}%\")\n",
        "        print(f\"   - Val accuracy finale: {val_accs[-1]:.2f}%\")\n",
        "\n",
        "    return fold_results\n",
        "\n",
        "# =============================================================================\n",
        "# VISUALISATIONS SPATIALSENSE+ AVEC COMPARAISON ARTICLE\n",
        "# =============================================================================\n",
        "\n",
        "def visualize_spatialsense_results_with_comparison(fold_results):\n",
        "    \"\"\"\n",
        "    Visualise les résultats SpatialSense+ avec comparaison à Haldekar et al. 2017\n",
        "    \"\"\"\n",
        "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))\n",
        "\n",
        "    # 1. Courbes de loss\n",
        "    for result in fold_results:\n",
        "        fold = result['fold']\n",
        "        ax1.plot(result['train_losses'], label=f'Fold {fold} Train', linewidth=2)\n",
        "        ax1.plot(result['val_losses'], '--', label=f'Fold {fold} Val', alpha=0.8)\n",
        "\n",
        "    ax1.set_title('Loss SpatialSense+ (Architecture Haldekar)', fontsize=14, fontweight='bold')\n",
        "    ax1.set_xlabel('Époque')\n",
        "    ax1.set_ylabel('Loss')\n",
        "    ax1.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "    ax1.grid(True, alpha=0.3)\n",
        "\n",
        "    # 2. Courbes d'accuracy\n",
        "    for result in fold_results:\n",
        "        fold = result['fold']\n",
        "        ax2.plot(result['train_accs'], label=f'Fold {fold} Train', linewidth=2)\n",
        "        ax2.plot(result['val_accs'], '--', label=f'Fold {fold} Val', alpha=0.8)\n",
        "\n",
        "    # Ligne de référence article original\n",
        "    ax2.axhline(y=55.98, color='red', linestyle='-', alpha=0.7,\n",
        "               label='Haldekar et al. 2017 (55.98%)')\n",
        "\n",
        "    ax2.set_title('Accuracy SpatialSense+ vs Article Original', fontsize=14, fontweight='bold')\n",
        "    ax2.set_xlabel('Époque')\n",
        "    ax2.set_ylabel('Accuracy (%)')\n",
        "    ax2.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "    ax2.grid(True, alpha=0.3)\n",
        "\n",
        "    # 3. Comparaison des performances finales\n",
        "    folds = [r['fold'] for r in fold_results]\n",
        "    train_accs = [r['final_train_acc'] for r in fold_results]\n",
        "    val_accs = [r['final_val_acc'] for r in fold_results]\n",
        "    best_val_accs = [r['best_val_acc'] for r in fold_results]\n",
        "\n",
        "    x = np.arange(len(folds))\n",
        "    width = 0.25\n",
        "\n",
        "    bars1 = ax3.bar(x - width, train_accs, width, label='Train Final', alpha=0.8)\n",
        "    bars2 = ax3.bar(x, val_accs, width, label='Val Final', alpha=0.8)\n",
        "    bars3 = ax3.bar(x + width, best_val_accs, width, label='Val Best', alpha=0.8)\n",
        "\n",
        "    # Ligne de référence article\n",
        "    ax3.axhline(y=55.98, color='red', linestyle='--', alpha=0.7,\n",
        "               label='Haldekar SUN09 (55.98%)')\n",
        "\n",
        "    ax3.set_title('Performance SpatialSense+ par Fold vs Article', fontsize=14, fontweight='bold')\n",
        "    ax3.set_xlabel('Fold')\n",
        "    ax3.set_ylabel('Accuracy (%)')\n",
        "    ax3.set_xticks(x)\n",
        "    ax3.set_xticklabels(folds)\n",
        "    ax3.legend()\n",
        "    ax3.grid(True, axis='y', alpha=0.3)\n",
        "\n",
        "    # Ajouter les valeurs sur les barres\n",
        "    def autolabel(bars):\n",
        "        for bar in bars:\n",
        "            height = bar.get_height()\n",
        "            ax3.annotate(f'{height:.1f}',\n",
        "                        xy=(bar.get_x() + bar.get_width() / 2, height),\n",
        "                        xytext=(0, 3), textcoords=\"offset points\",\n",
        "                        ha='center', va='bottom', fontsize=9)\n",
        "\n",
        "    autolabel(bars1)\n",
        "    autolabel(bars2)\n",
        "    autolabel(bars3)\n",
        "\n",
        "    # 4. Résumé comparatif avec l'article\n",
        "    ax4.axis('off')\n",
        "\n",
        "    mean_train = np.mean(train_accs)\n",
        "    std_train = np.std(train_accs)\n",
        "    mean_val = np.mean(val_accs)\n",
        "    std_val = np.std(val_accs)\n",
        "    mean_best = np.mean(best_val_accs)\n",
        "    std_best = np.std(best_val_accs)\n",
        "\n",
        "    best_fold = np.argmax(best_val_accs) + 1\n",
        "\n",
        "    # Comparaison avec l'article original\n",
        "    improvement = mean_best - 55.98\n",
        "    improvement_sign = \"      \" if improvement > 0 else \"  \" if improvement < -2 else \"➡️\"\n",
        "\n",
        "    summary_text = f\"\"\"\n",
        "    COMPARAISON HALDEKAR ET AL. 2017\n",
        "\n",
        "    Article Original (SUN09):\n",
        "    • Dataset: 4468 train, 4955 test\n",
        "    • Relations: 3 (above, beside, behind)\n",
        "    • Architecture: VGG FC-7 + MLP\n",
        "    • Performance: 71.97% train, 55.98% test\n",
        "\n",
        "    Notre Implementation (SpatialSense+):\n",
        "    • Dataset: SpatialSense+\n",
        "    • Relations: {len(SPATIAL_RELATIONS)} ({', '.join(SPATIAL_RELATIONS[:3])}...)\n",
        "    • Architecture: Identique (VGG FC-7 + MLP)\n",
        "    • Performance: {mean_best:.2f}% ± {std_best:.2f}% (val)\n",
        "\n",
        "    Résultat: {improvement_sign} {improvement:+.2f}% vs article\n",
        "\n",
        "    Défis SpatialSense+:\n",
        "    • +{len(SPATIAL_RELATIONS)-3} relations supplémentaires\n",
        "    • Tâche plus complexe\n",
        "    • Dataset différent\n",
        "\n",
        "    Meilleur Fold: {best_fold} ({best_val_accs[best_fold-1]:.2f}%)\n",
        "    \"\"\"\n",
        "\n",
        "    color = 'lightgreen' if improvement > 0 else 'lightyellow' if improvement > -5 else 'lightcoral'\n",
        "    ax4.text(0.05, 0.95, summary_text, transform=ax4.transAxes,\n",
        "             fontsize=11, verticalalignment='top',\n",
        "             bbox=dict(boxstyle='round,pad=0.5', facecolor=color, alpha=0.8))\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('spatialsense_haldekar_comparison.png', dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "def visualize_predictions_with_heatmaps_spatialsense(model, dataset, device, num_samples=6):\n",
        "    \"\"\"\n",
        "    Visualise prédictions SpatialSense+ avec heatmaps style Haldekar\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    fig, axes = plt.subplots(num_samples, 4, figsize=(16, 4*num_samples))\n",
        "    if num_samples == 1:\n",
        "        axes = axes.reshape(1, -1)\n",
        "\n",
        "    for i in range(min(num_samples, len(dataset))):\n",
        "        # Échantillon SpatialSense+\n",
        "        image, label, metadata = dataset[i]\n",
        "\n",
        "        # Dénormaliser pour affichage\n",
        "        img_display = image.clone()\n",
        "        mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
        "        std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n",
        "        img_display = img_display * std + mean\n",
        "        img_display = img_display.clamp(0, 1).permute(1, 2, 0)\n",
        "\n",
        "        # Prédiction\n",
        "        with torch.no_grad():\n",
        "            image_tensor = image.unsqueeze(0).to(device)\n",
        "            output = model(image_tensor)\n",
        "            probs = torch.softmax(output, dim=1)\n",
        "            pred_idx = torch.argmax(probs)\n",
        "            pred_prob = probs[0, pred_idx].item()\n",
        "            true_prob = probs[0, label].item()\n",
        "\n",
        "        pred_relation = dataset.idx_to_relation[pred_idx.item()]\n",
        "        true_relation = metadata['relation']\n",
        "\n",
        "        # Heatmap style Haldekar\n",
        "        heatmap = generate_heatmap_haldekar_style(model, image, label, device)\n",
        "\n",
        "        # 1. Image originale\n",
        "        axes[i, 0].imshow(img_display)\n",
        "        axes[i, 0].set_title(f\"{metadata['subject']} - {metadata['object']}\")\n",
        "        axes[i, 0].axis('off')\n",
        "\n",
        "        # 2. Heatmap (comme Figure 2 de l'article)\n",
        "        axes[i, 1].imshow(heatmap, cmap='hot', alpha=0.8)\n",
        "        axes[i, 1].set_title('Heatmap Haldekar-style')\n",
        "        axes[i, 1].axis('off')\n",
        "\n",
        "        # 3. Superposition (comme l'article)\n",
        "        axes[i, 2].imshow(img_display)\n",
        "        axes[i, 2].imshow(heatmap, cmap='jet', alpha=0.5)\n",
        "        axes[i, 2].set_title('Superposition')\n",
        "        axes[i, 2].axis('off')\n",
        "\n",
        "        # 4. Informations détaillées\n",
        "        axes[i, 3].axis('off')\n",
        "        info_text = f\"\"\"\n",
        "        SpatialSense+ Sample:\n",
        "\n",
        "        Vérité: {true_relation}\n",
        "        Prédiction: {pred_relation}\n",
        "        Original: {metadata.get('original_relation', 'N/A')}\n",
        "\n",
        "        Confiance prédiction: {pred_prob:.2%}\n",
        "        Confiance vérité: {true_prob:.2%}\n",
        "\n",
        "        Résultat: {'   CORRECT' if pred_relation == true_relation else '    INCORRECT'}\n",
        "\n",
        "        Heatmap (influence):\n",
        "        Max: {heatmap.max():.3f}\n",
        "        Min: {heatmap.min():.3f}\n",
        "        \"\"\"\n",
        "        color = 'lightgreen' if pred_relation == true_relation else 'lightcoral'\n",
        "        axes[i, 3].text(0.1, 0.5, info_text, transform=axes[i, 3].transAxes,\n",
        "                       fontsize=10, verticalalignment='center',\n",
        "                       bbox=dict(boxstyle='round', facecolor=color, alpha=0.8))\n",
        "\n",
        "    plt.suptitle('Prédictions SpatialSense+ - Méthode Haldekar et al. 2017',\n",
        "                 fontsize=16, fontweight='bold')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('spatialsense_predictions_haldekar.png', dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "# =============================================================================\n",
        "# ANALYSE DE PERFORMANCE PAR RELATION SPATIALSENSE+\n",
        "# =============================================================================\n",
        "\n",
        "def analyze_spatialsense_performance_per_relation(model, dataloader, device):\n",
        "    \"\"\"Analyse la performance par relation SpatialSense+\"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    # Matrices pour chaque relation SpatialSense+\n",
        "    relation_correct = {rel: 0 for rel in SPATIAL_RELATIONS}\n",
        "    relation_total = {rel: 0 for rel in SPATIAL_RELATIONS}\n",
        "    relation_predictions = {rel: [] for rel in SPATIAL_RELATIONS}\n",
        "\n",
        "    print(\"   Analyse performance par relation SpatialSense+...\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_data in tqdm(dataloader, desc='Analyzing relations'):\n",
        "            # Gestion flexible du format de batch\n",
        "            if len(batch_data) == 3:\n",
        "                images, labels, metadata = batch_data\n",
        "            else:\n",
        "                images, labels = batch_data[:2]\n",
        "                metadata = [{'relation': SPATIAL_RELATIONS[labels[i].item()]} for i in range(len(labels))]\n",
        "\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "            for i in range(len(labels)):\n",
        "                # Gestion des métadonnées\n",
        "                if isinstance(metadata, list) and i < len(metadata):\n",
        "                    true_rel = metadata[i].get('relation', SPATIAL_RELATIONS[labels[i].item()])\n",
        "                elif isinstance(metadata, dict):\n",
        "                    relation_list = metadata.get('relation', [SPATIAL_RELATIONS[labels[j].item()] for j in range(len(labels))])\n",
        "                    true_rel = relation_list[i] if i < len(relation_list) else SPATIAL_RELATIONS[labels[i].item()]\n",
        "                else:\n",
        "                    true_rel = SPATIAL_RELATIONS[labels[i].item()]\n",
        "\n",
        "                pred_idx = predicted[i].item()\n",
        "                pred_rel = SPATIAL_RELATIONS[pred_idx]\n",
        "\n",
        "                relation_total[true_rel] += 1\n",
        "                relation_predictions[true_rel].append(pred_rel)\n",
        "\n",
        "                if pred_rel == true_rel:\n",
        "                    relation_correct[true_rel] += 1\n",
        "\n",
        "    # Calcul accuracies par relation\n",
        "    relation_accuracies = {}\n",
        "    for rel in SPATIAL_RELATIONS:\n",
        "        if relation_total[rel] > 0:\n",
        "            relation_accuracies[rel] = relation_correct[rel] / relation_total[rel] * 100\n",
        "        else:\n",
        "            relation_accuracies[rel] = 0.0\n",
        "\n",
        "    # Visualisation\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "    # 1. Accuracy par relation SpatialSense+\n",
        "    relations = SPATIAL_RELATIONS  # Ordre défini\n",
        "    accuracies = [relation_accuracies[rel] for rel in relations]\n",
        "    counts = [relation_total[rel] for rel in relations]\n",
        "\n",
        "    bars = ax1.bar(range(len(relations)), accuracies,\n",
        "                   color='skyblue', alpha=0.8)\n",
        "    ax1.set_xlabel('Relations SpatialSense+')\n",
        "    ax1.set_ylabel('Accuracy (%)')\n",
        "    ax1.set_title('Performance par Relation SpatialSense+\\n(Architecture Haldekar)')\n",
        "    ax1.set_xticks(range(len(relations)))\n",
        "    ax1.set_xticklabels(relations, rotation=45, ha='right')\n",
        "    ax1.grid(True, axis='y', alpha=0.3)\n",
        "\n",
        "    # Ligne de référence article (moyenne)\n",
        "    ax1.axhline(y=55.98, color='red', linestyle='--', alpha=0.7,\n",
        "               label='Haldekar SUN09 (55.98%)')\n",
        "    ax1.legend()\n",
        "\n",
        "    # Annotations avec nombre d'échantillons\n",
        "    for i, (bar, acc, count) in enumerate(zip(bars, accuracies, counts)):\n",
        "        height = bar.get_height()\n",
        "        ax1.annotate(f'{acc:.1f}%\\n(n={count})',\n",
        "                    xy=(bar.get_x() + bar.get_width() / 2, height),\n",
        "                    xytext=(0, 3), textcoords=\"offset points\",\n",
        "                    ha='center', va='bottom', fontsize=8)\n",
        "\n",
        "    # 2. Distribution des échantillons SpatialSense+\n",
        "    ax2.bar(range(len(relations)), counts, color='lightcoral', alpha=0.8)\n",
        "    ax2.set_xlabel('Relations SpatialSense+')\n",
        "    ax2.set_ylabel('Nombre d\\'échantillons')\n",
        "    ax2.set_title('Distribution Échantillons SpatialSense+')\n",
        "    ax2.set_xticks(range(len(relations)))\n",
        "    ax2.set_xticklabels(relations, rotation=45, ha='right')\n",
        "    ax2.grid(True, axis='y', alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('spatialsense_performance_per_relation.png', dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "    # Résumé textuel\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"ANALYSE PERFORMANCE PAR RELATION SPATIALSENSE+\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    sorted_relations = sorted(zip(relations, accuracies, counts),\n",
        "                             key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    print(f\"\\n    Meilleures performances:\")\n",
        "    for rel, acc, count in sorted_relations[:3]:\n",
        "        status = \"🟢\" if acc > 55.98 else \"🟡\" if acc > 40 else \"🔴\"\n",
        "        print(f\"   {status} {rel}: {acc:.2f}% (n={count})\")\n",
        "\n",
        "    print(f\"\\n   Pires performances:\")\n",
        "    for rel, acc, count in sorted_relations[-3:]:\n",
        "        status = \"🟢\" if acc > 55.98 else \"🟡\" if acc > 40 else \"🔴\"\n",
        "        print(f\"   {status} {rel}: {acc:.2f}% (n={count})\")\n",
        "\n",
        "    overall_acc = sum(relation_correct.values()) / sum(relation_total.values()) * 100\n",
        "    print(f\"\\n   Statistiques globales:\")\n",
        "    print(f\"   - Accuracy globale: {overall_acc:.2f}%\")\n",
        "    print(f\"   - Relations au-dessus Haldekar: {sum(1 for acc in accuracies if acc > 55.98)}/{len(SPATIAL_RELATIONS)}\")\n",
        "    print(f\"   - Échantillons total: {sum(relation_total.values())}\")\n",
        "\n",
        "    return relation_accuracies, relation_total\n",
        "\n",
        "# =============================================================================\n",
        "# FONCTION PRINCIPALE SPATIALSENSE+ HALDEKAR AVEC ANALYSE COMPLÈTE\n",
        "# =============================================================================\n",
        "\n",
        "def main_spatialsense_haldekar():\n",
        "    \"\"\"\n",
        "    Fonction principale: Implémentation exacte Haldekar et al. 2017 sur SpatialSense+\n",
        "    Avec analyse complète incluant matrice de confusion et analyse d'erreurs\n",
        "    \"\"\"\n",
        "    DATA_DIR = \"data/spatialsense\"  # À adapter selon votre environnement\n",
        "\n",
        "    print(\"=\"*80)\n",
        "    print(\"IMPLÉMENTATION EXACTE HALDEKAR ET AL. 2017 SUR SPATIALSENSE+\")\n",
        "    print(\"AVEC ANALYSE COMPLÈTE DES ERREURS\")\n",
        "    print(\"=\"*80)\n",
        "    print(\"   Article original: 'Identifying Spatial Relations in Images using CNNs'\")\n",
        "    print(\"   Dataset original: SUN09 (3 relations, 55.98% test accuracy)\")\n",
        "    print(\"   Notre adaptation: SpatialSense+ (9 relations)\")\n",
        "    print(\"    Architecture: VGG16 FC-7 + MLP (identique à l'article)\")\n",
        "    print(\"   Nouveauté: Matrice de confusion + analyse détaillée des erreurs\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    print(f\"\\nConfiguration:\")\n",
        "    print(f\"   - Device: {DEVICE}\")\n",
        "    print(f\"   - Dataset: SpatialSense+ ({len(SPATIAL_RELATIONS)} relations)\")\n",
        "    print(f\"   - Architecture: VGG16 FC-7 (4096) + MLP (512→256→{len(SPATIAL_RELATIONS)})\")\n",
        "    print(f\"   - Hyperparamètres: batch_size={BATCH_SIZE}, lr={LEARNING_RATE}, epochs={EPOCHS}\")\n",
        "    print(f\"   - Relations: {', '.join(SPATIAL_RELATIONS)}\")\n",
        "\n",
        "    # Vérification des données\n",
        "    if not os.path.exists(DATA_DIR):\n",
        "        print(f\"\\n    ERREUR: Le répertoire {DATA_DIR} n'existe pas!\")\n",
        "        print(\"\\n   Pour utiliser ce code:\")\n",
        "        print(\"1. Téléchargez le dataset SpatialSense+\")\n",
        "        print(\"2. Extrayez-le dans le répertoire spécifié\")\n",
        "        print(\"3. Structure attendue:\")\n",
        "        print(\"   data/spatialsense/\")\n",
        "        print(\"   ├── annotations.json\")\n",
        "        print(\"   └── images/images/{flickr,nyu}/\")\n",
        "        return\n",
        "\n",
        "    # Test rapide du dataset\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"🧪 Test du Dataset SpatialSense+\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    try:\n",
        "        # Créer le dataset de test AVEC transform\n",
        "        train_transform, test_transform = create_transforms()\n",
        "        test_dataset = SpatialSenseDataset(DATA_DIR, split='train', transform=train_transform)\n",
        "        print(f\"   Dataset d'entraînement: {len(test_dataset)} échantillons\")\n",
        "\n",
        "        if len(test_dataset) > 0:\n",
        "            img, label, meta = test_dataset[0]\n",
        "            print(f\"   Premier échantillon:\")\n",
        "            print(f\"   - Relation: {meta['relation']} (mappée de '{meta['original_relation']}')\")\n",
        "            print(f\"   - Sujet: {meta['subject']}\")\n",
        "            print(f\"   - Objet: {meta['object']}\")\n",
        "\n",
        "            # Correction: vérifier le type avant d'accéder à shape\n",
        "            if isinstance(img, torch.Tensor):\n",
        "                print(f\"   - Image shape: {img.shape}\")\n",
        "            else:\n",
        "                print(f\"   - Image size: {img.size} (PIL Image)\")\n",
        "        else:\n",
        "            print(\"    Dataset vide!\")\n",
        "            return\n",
        "    except Exception as e:\n",
        "        print(f\"    Erreur chargement dataset: {e}\")\n",
        "        return\n",
        "\n",
        "    # Phase 1: Validation K-fold Haldekar-style\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"     PHASE 1: Validation K-fold (Haldekar et al. 2017)\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    fold_results = kfold_cross_validation_spatialsense(\n",
        "        data_dir=DATA_DIR,\n",
        "        k_folds=K_FOLDS,\n",
        "        epochs=EPOCHS\n",
        "    )\n",
        "\n",
        "    if not fold_results:\n",
        "        print(\"    Erreur pendant la validation K-fold\")\n",
        "        return\n",
        "\n",
        "    # Visualisation des résultats avec comparaison\n",
        "    print(\"\\n   Visualisation résultats avec comparaison article...\")\n",
        "    visualize_spatialsense_results_with_comparison(fold_results)\n",
        "\n",
        "    # Sélection du meilleur modèle\n",
        "    best_fold_idx = np.argmax([r['best_val_acc'] for r in fold_results])\n",
        "    best_model = fold_results[best_fold_idx]['model']\n",
        "    best_acc = fold_results[best_fold_idx]['best_val_acc']\n",
        "\n",
        "    print(f\"\\n    Meilleur modèle: Fold {best_fold_idx + 1}\")\n",
        "    print(f\"   - Validation accuracy: {best_acc:.2f}%\")\n",
        "    print(f\"   - Comparaison Haldekar: {best_acc - 55.98:+.2f}%\")\n",
        "\n",
        "    # Sauvegarde du meilleur modèle\n",
        "    torch.save(best_model.state_dict(), 'best_spatialsense_haldekar_model.pth')\n",
        "    print(\"   Modèle sauvegardé: best_spatialsense_haldekar_model.pth\")\n",
        "\n",
        "    # Phase 2: Évaluation sur le test set avec analyse complète\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"  PHASE 2: Évaluation Test Set avec Analyse Complète\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    try:\n",
        "        # Dataset de test\n",
        "        _, test_transform = create_transforms()\n",
        "        test_dataset = SpatialSenseDataset(DATA_DIR, split='test', transform=test_transform)\n",
        "\n",
        "        print(f\"Dataset de test: {len(test_dataset)} échantillons\")\n",
        "\n",
        "        if len(test_dataset) == 0:\n",
        "            print(\"       Dataset test vide, utilisation du validation set\")\n",
        "            test_dataset = SpatialSenseDataset(DATA_DIR, split='val', transform=test_transform)\n",
        "            print(f\"Dataset de validation: {len(test_dataset)} échantillons\")\n",
        "\n",
        "        if len(test_dataset) > 0:\n",
        "            test_loader = DataLoader(\n",
        "                test_dataset, batch_size=BATCH_SIZE, shuffle=False,\n",
        "                num_workers=2, pin_memory=True\n",
        "            )\n",
        "\n",
        "            # Évaluation finale\n",
        "            criterion = nn.CrossEntropyLoss()\n",
        "            test_loss, test_acc = evaluate(best_model, test_loader, criterion, DEVICE)\n",
        "\n",
        "            print(f\"\\n   Résultats Test Set:\")\n",
        "            print(f\"   - Loss: {test_loss:.4f}\")\n",
        "            print(f\"   - Accuracy: {test_acc:.2f}%\")\n",
        "            print(f\"   - Comparaison Haldekar: {test_acc - 55.98:+.2f}%\")\n",
        "\n",
        "            # NOUVELLE PHASE: Analyse complète des erreurs\n",
        "            print(\"\\n\" + \"=\"*50)\n",
        "            print(\"   PHASE 3: Analyse Complète des Erreurs\")\n",
        "            print(\"=\"*50)\n",
        "\n",
        "            # Génération de la matrice de confusion et analyse d'erreurs\n",
        "            print(\"   Génération matrice de confusion...\")\n",
        "            cm, error_examples, classification_report = generate_confusion_matrix_and_analysis(\n",
        "                best_model, test_loader, DEVICE, save_prefix=\"spatialsense\"\n",
        "            )\n",
        "\n",
        "            # Analyse des patterns d'erreurs\n",
        "            print(\"\\n   Analyse des patterns d'erreurs...\")\n",
        "            group_confusion, subject_errors, object_errors = analyze_error_patterns(\n",
        "                error_examples, save_prefix=\"spatialsense\"\n",
        "            )\n",
        "\n",
        "            # Visualisation d'exemples d'erreurs\n",
        "            print(\"\\n   Visualisation d'exemples d'erreurs...\")\n",
        "            visualize_error_examples(\n",
        "                best_model, test_dataset, error_examples, DEVICE,\n",
        "                num_examples=6, save_prefix=\"spatialsense\"\n",
        "            )\n",
        "\n",
        "            # Phase 4: Visualisations Haldekar-style\n",
        "            print(\"\\n\" + \"=\"*50)\n",
        "            print(\"   PHASE 4: Visualisations Haldekar-style\")\n",
        "            print(\"=\"*50)\n",
        "\n",
        "            print(\"   Génération heatmaps style article...\")\n",
        "            visualize_predictions_with_heatmaps_spatialsense(\n",
        "                best_model, test_dataset, DEVICE, num_samples=6\n",
        "            )\n",
        "\n",
        "            # Phase 5: Analyse par relation\n",
        "            print(\"       Analyse performance par relation...\")\n",
        "            relation_accs, relation_counts = analyze_spatialsense_performance_per_relation(\n",
        "                best_model, test_loader, DEVICE\n",
        "            )\n",
        "\n",
        "        else:\n",
        "            print(\"    Aucun échantillon de test disponible\")\n",
        "            test_acc = best_acc\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"    Erreur évaluation test: {e}\")\n",
        "        test_acc = best_acc\n",
        "\n",
        "    # Résumé final avec analyse d'erreurs\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"   RÉSUMÉ FINAL - COMPARAISON HALDEKAR + ANALYSE D'ERREURS\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    # Statistiques K-fold\n",
        "    mean_train_acc = np.mean([r['final_train_acc'] for r in fold_results])\n",
        "    mean_val_acc = np.mean([r['final_val_acc'] for r in fold_results])\n",
        "    std_val_acc = np.std([r['final_val_acc'] for r in fold_results])\n",
        "    mean_best_acc = np.mean([r['best_val_acc'] for r in fold_results])\n",
        "\n",
        "    print(f\"\\n   Résultats Validation K-fold:\")\n",
        "    print(f\"   - Train accuracy moyenne: {mean_train_acc:.2f}%\")\n",
        "    print(f\"   - Val accuracy moyenne: {mean_val_acc:.2f}% ± {std_val_acc:.2f}%\")\n",
        "    print(f\"   - Best val accuracy moyenne: {mean_best_acc:.2f}%\")\n",
        "    print(f\"   - Meilleur fold: {best_fold_idx + 1} ({best_acc:.2f}%)\")\n",
        "\n",
        "    if 'test_acc' in locals() and test_acc != best_acc:\n",
        "        print(f\"\\n  Test final:\")\n",
        "        print(f\"   - Test accuracy: {test_acc:.2f}%\")\n",
        "        final_comparison = test_acc\n",
        "    else:\n",
        "        final_comparison = best_acc\n",
        "\n",
        "    print(f\"\\n   Comparaison avec Article Original:\")\n",
        "    print(f\"   - Haldekar et al. 2017 (SUN09): 55.98% test\")\n",
        "    print(f\"   - Notre implémentation (SpatialSense+): {final_comparison:.2f}%\")\n",
        "    print(f\"   - Différence: {final_comparison - 55.98:+.2f}%\")\n",
        "\n",
        "    if final_comparison > 55.98:\n",
        "        print(f\"   -         AMÉLIORATION malgré +{len(SPATIAL_RELATIONS)-3} relations!\")\n",
        "    elif final_comparison > 50:\n",
        "        print(f\"   -    PERFORMANCE SOLIDE avec {len(SPATIAL_RELATIONS)} relations vs 3\")\n",
        "    else:\n",
        "        print(f\"   -        Performance à améliorer (tâche plus complexe)\")\n",
        "\n",
        "    print(f\"\\n   Facteurs de Complexité:\")\n",
        "    print(f\"   - Relations: {len(SPATIAL_RELATIONS)} vs 3 (+{len(SPATIAL_RELATIONS)-3})\")\n",
        "    print(f\"   - Dataset: SpatialSense+ vs SUN09 (différent)\")\n",
        "    print(f\"   - Architecture: Identique (VGG16 FC-7 + MLP)\")\n",
        "    print(f\"   - Méthode heatmap: Identique (masking séquentiel)\")\n",
        "\n",
        "    # Analyse d'erreurs résumée\n",
        "    if 'error_examples' in locals():\n",
        "        total_errors = sum(len(examples) for examples in error_examples.values())\n",
        "        error_types = len(error_examples)\n",
        "        print(f\"\\n   Analyse d'Erreurs:\")\n",
        "        print(f\"   - Total erreurs analysées: {total_errors}\")\n",
        "        print(f\"   - Types d'erreurs différents: {error_types}\")\n",
        "\n",
        "        # Top 3 erreurs\n",
        "        error_counts = Counter()\n",
        "        for (true_rel, pred_rel), examples in error_examples.items():\n",
        "            error_counts[(true_rel, pred_rel)] = len(examples)\n",
        "\n",
        "        top_3_errors = error_counts.most_common(3)\n",
        "        print(f\"   - Top 3 erreurs fréquentes:\")\n",
        "        for i, ((true_rel, pred_rel), count) in enumerate(top_3_errors, 1):\n",
        "            print(f\"     {i}. {true_rel} → {pred_rel}: {count} cas\")\n",
        "\n",
        "    return fold_results\n",
        "\n",
        "# =============================================================================\n",
        "# POINT D'ENTRÉE\n",
        "# =============================================================================\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main_spatialsense_haldekar()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "4fc028900f30408d959af35b17e1f49f",
            "4735f545d6a64610a667ff5e97ed2926",
            "1a851ed291684256b758eb406c286c32",
            "61cea817de014e5db5007231d6f34347",
            "afaa66bd1eac43d5bd8cf1d79b7b4829",
            "49b5f04a51b54083a8e0b0154e816833",
            "0ca959a218bd449ea6fbaef3a2c09208",
            "6752d61de50f4a8486004541c1fe0e33",
            "83029fc1e0e44556902f018a5ca0acdd",
            "ca4fdce253c74ffdafa3197789acfcc3",
            "157b49dddff3488e8527d762b4b757fb",
            "6e1e153e2f99470da86c55bc3a4c43b1",
            "2e34544b5514446e9b1944c40d4e771c",
            "1af981fd170e43fc89aec44d6ebd700e",
            "29d1a46de0bb433db358ff9d3a23e66c",
            "9576ce5bf660433aaad0685f73dd826b",
            "e4b05b0367e0474ca4a350b0e5152e0e",
            "76cccf964fb9404386e05ade2507ee60",
            "887951614ed745d2a12c40cb166f8ae0",
            "c3de2f853cf2464a8475d663ed08caef",
            "909cc227af554e28896b685171819c9e",
            "69b7d93bb20446c19d673e6368f8ea2d"
          ]
        },
        "id": "vwiFtDFkdZ-P",
        "outputId": "5448ac89-0e97-40f9-c362-260fb2ac0114"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import ViTModel, ViTConfig\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import json\n",
        "import os\n",
        "from sklearn.model_selection import KFold\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import cv2\n",
        "from collections import Counter\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# =============================================================================\n",
        "# CONFIGURATION AMÉLIORÉE\n",
        "# =============================================================================\n",
        "\n",
        "# Configuration GPU\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Device utilisé: {DEVICE}\")\n",
        "\n",
        "# HYPERPARAMÈTRES OPTIMISÉS POUR AMÉLIORER LES PERFORMANCES\n",
        "BATCH_SIZE = 16          # Augmenté de 8 à 16 (plus stable)\n",
        "LEARNING_RATE = 2e-5     # Augmenté de 1e-6 à 2e-5 (convergence plus rapide)\n",
        "EPOCHS = 15              # Augmenté de 10 à 15 (plus d'entraînement)\n",
        "K_FOLDS = 5\n",
        "IMG_SIZE = 224\n",
        "DROPOUT_RATE = 0.3       # Réduit de 0.5 à 0.3 (moins de régularisation)\n",
        "\n",
        "# Nouveaux hyperparamètres pour optimisation\n",
        "WEIGHT_DECAY = 0.05      # Augmenté pour meilleure régularisation\n",
        "WARMUP_EPOCHS = 3        # Warmup pour stabilité\n",
        "MIN_LR = 1e-7           # Learning rate minimum\n",
        "SCHEDULER_PATIENCE = 3   # Patience pour ReduceLROnPlateau\n",
        "\n",
        "# Relations spatiales EXACTES de SpatialSense+\n",
        "SPATIAL_RELATIONS = [\n",
        "    'above', 'behind', 'in', 'in front of', 'next to',\n",
        "    'on', 'to the left of', 'to the right of', 'under'\n",
        "]\n",
        "\n",
        "print(f\"Relations SpatialSense+ : {len(SPATIAL_RELATIONS)} relations\")\n",
        "print(f\"Hyperparamètres optimisés :\")\n",
        "print(f\"  - Batch size: {BATCH_SIZE}\")\n",
        "print(f\"  - Learning rate: {LEARNING_RATE}\")\n",
        "print(f\"  - Epochs: {EPOCHS}\")\n",
        "print(f\"  - Dropout: {DROPOUT_RATE}\")\n",
        "print(f\"  - Weight decay: {WEIGHT_DECAY}\")\n",
        "\n",
        "# =============================================================================\n",
        "# ARCHITECTURE AMÉLIORÉE\n",
        "# =============================================================================\n",
        "\n",
        "class ImprovedSpatialRelationMLP(nn.Module):\n",
        "    \"\"\"\n",
        "    MLP amélioré avec techniques modernes pour meilleures performances\n",
        "    \"\"\"\n",
        "    def __init__(self, input_dim=4096, hidden1_dim=512, hidden2_dim=256,\n",
        "                 num_relations=len(SPATIAL_RELATIONS), dropout_rate=0.3):\n",
        "        super(ImprovedSpatialRelationMLP, self).__init__()\n",
        "\n",
        "        # Première couche avec BatchNorm\n",
        "        self.fc1 = nn.Linear(input_dim, hidden1_dim)\n",
        "        self.bn1 = nn.BatchNorm1d(hidden1_dim)  # Ajout BatchNorm\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.dropout1 = nn.Dropout(dropout_rate)\n",
        "\n",
        "        # Deuxième couche avec BatchNorm\n",
        "        self.fc2 = nn.Linear(hidden1_dim, hidden2_dim)\n",
        "        self.bn2 = nn.BatchNorm1d(hidden2_dim)  # Ajout BatchNorm\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.dropout2 = nn.Dropout(dropout_rate)\n",
        "\n",
        "        # Couche de sortie\n",
        "        self.fc3 = nn.Linear(hidden2_dim, num_relations)\n",
        "\n",
        "        # Initialisation Xavier/Glorot pour meilleure convergence\n",
        "        nn.init.xavier_uniform_(self.fc1.weight)\n",
        "        nn.init.xavier_uniform_(self.fc2.weight)\n",
        "        nn.init.xavier_uniform_(self.fc3.weight)\n",
        "\n",
        "        print(f\"MLP Amélioré avec BatchNorm:\")\n",
        "        print(f\"  - Input: {input_dim}\")\n",
        "        print(f\"  - Hidden 1: {hidden1_dim} + BatchNorm\")\n",
        "        print(f\"  - Hidden 2: {hidden2_dim} + BatchNorm\")\n",
        "        print(f\"  - Output: {num_relations}\")\n",
        "        print(f\"  - Dropout: {dropout_rate}\")\n",
        "\n",
        "    def forward(self, x):\n",
        "        # FC-0 layer avec BatchNorm\n",
        "        x = self.fc1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu1(x)\n",
        "        x = self.dropout1(x)\n",
        "\n",
        "        # FC-1 layer avec BatchNorm\n",
        "        x = self.fc2(x)\n",
        "        x = self.bn2(x)\n",
        "        x = self.relu2(x)\n",
        "        x = self.dropout2(x)\n",
        "\n",
        "        # Output layer\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "class ImprovedViTFeatureExtractor(nn.Module):\n",
        "    \"\"\"\n",
        "    Extracteur ViT amélioré avec fine-tuning partiel\n",
        "    \"\"\"\n",
        "    def __init__(self, model_name='google/vit-base-patch16-224',\n",
        "                 freeze_features=False, fine_tune_layers=4):\n",
        "        super(ImprovedViTFeatureExtractor, self).__init__()\n",
        "\n",
        "        self.vit = ViTModel.from_pretrained(model_name)\n",
        "        self.feature_dim = self.vit.config.hidden_size\n",
        "\n",
        "        # Projection améliorée avec résiduel\n",
        "        self.feature_projection = nn.Sequential(\n",
        "            nn.Linear(self.feature_dim, 2048),\n",
        "            nn.LayerNorm(2048),  # LayerNorm au lieu de BatchNorm\n",
        "            nn.GELU(),           # GELU au lieu de ReLU\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(2048, 4096),\n",
        "            nn.LayerNorm(4096),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(0.1)\n",
        "        )\n",
        "\n",
        "        # Fine-tuning partiel des dernières couches\n",
        "        if not freeze_features:\n",
        "            # Dégel des dernières couches pour fine-tuning\n",
        "            for param in self.vit.parameters():\n",
        "                param.requires_grad = False\n",
        "\n",
        "            # Dégel des dernières couches seulement\n",
        "            for layer in self.vit.encoder.layer[-fine_tune_layers:]:\n",
        "                for param in layer.parameters():\n",
        "                    param.requires_grad = True\n",
        "\n",
        "            print(f\"Fine-tuning des {fine_tune_layers} dernières couches ViT\")\n",
        "        else:\n",
        "            for param in self.vit.parameters():\n",
        "                param.requires_grad = False\n",
        "            print(\"Toutes les features ViT gelées\")\n",
        "\n",
        "    def forward(self, x):\n",
        "        outputs = self.vit(pixel_values=x)\n",
        "        cls_token = outputs.last_hidden_state[:, 0]\n",
        "        features = self.feature_projection(cls_token)\n",
        "        return features\n",
        "\n",
        "    def get_attention_weights(self, x):\n",
        "        outputs = self.vit(pixel_values=x, output_attentions=True)\n",
        "        attention_weights = outputs.attentions[-1][:, -1, 0, 1:]\n",
        "        return attention_weights\n",
        "\n",
        "class ImprovedViTSpatialRelationModel(nn.Module):\n",
        "    \"\"\"\n",
        "    Modèle complet amélioré\n",
        "    \"\"\"\n",
        "    def __init__(self, num_relations=len(SPATIAL_RELATIONS),\n",
        "                 vit_model='google/vit-base-patch16-224',\n",
        "                 freeze_vit=False, fine_tune_layers=4):\n",
        "        super(ImprovedViTSpatialRelationModel, self).__init__()\n",
        "\n",
        "        self.feature_extractor = ImprovedViTFeatureExtractor(\n",
        "            model_name=vit_model,\n",
        "            freeze_features=freeze_vit,\n",
        "            fine_tune_layers=fine_tune_layers\n",
        "        )\n",
        "\n",
        "        self.classifier = ImprovedSpatialRelationMLP(\n",
        "            input_dim=4096,\n",
        "            hidden1_dim=512,\n",
        "            hidden2_dim=256,\n",
        "            num_relations=num_relations,\n",
        "            dropout_rate=DROPOUT_RATE\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        features = self.feature_extractor(x)\n",
        "        output = self.classifier(features)\n",
        "        return output\n",
        "\n",
        "    def get_attention_visualization(self, x):\n",
        "        return self.feature_extractor.get_attention_weights(x)\n",
        "\n",
        "# =============================================================================\n",
        "# DATASET ADAPTÉ POUR SPATIALSENSE+\n",
        "# =============================================================================\n",
        "\n",
        "class SpatialSenseDataset(Dataset):\n",
        "    \"\"\"Dataset SpatialSense+ avec les relations exactes du tableau\"\"\"\n",
        "\n",
        "    def __init__(self, data_dir, split='train', transform=None):\n",
        "        self.data_dir = data_dir\n",
        "        self.split = split\n",
        "        self.transform = transform\n",
        "\n",
        "        # Mapping exact selon le tableau SpatialSense+\n",
        "        self.relation_to_idx = {rel: idx for idx, rel in enumerate(SPATIAL_RELATIONS)}\n",
        "        self.idx_to_relation = {idx: rel for rel, idx in self.relation_to_idx.items()}\n",
        "\n",
        "        self.annotations = self._load_annotations()\n",
        "        self.data_samples = self._prepare_samples()\n",
        "\n",
        "        print(f\"Dataset SpatialSense+ {split} initialisé avec {len(self.data_samples)} échantillons\")\n",
        "        if len(self.data_samples) > 0:\n",
        "            self._print_statistics()\n",
        "\n",
        "    def _load_annotations(self):\n",
        "        \"\"\"Charge les annotations SpatialSense+\"\"\"\n",
        "        annotations_path = os.path.join(self.data_dir, 'annotations.json')\n",
        "        try:\n",
        "            with open(annotations_path, 'r') as f:\n",
        "                return json.load(f)\n",
        "        except FileNotFoundError:\n",
        "            print(f\"Erreur: Fichier {annotations_path} non trouvé!\")\n",
        "            return []\n",
        "        except json.JSONDecodeError as e:\n",
        "            print(f\"Erreur lors du décodage JSON: {e}\")\n",
        "            return []\n",
        "\n",
        "    def _find_image_path(self, image_url):\n",
        "        \"\"\"Trouve le chemin local d'une image SpatialSense+\"\"\"\n",
        "        base_dir = os.path.join(self.data_dir, \"images\", \"images\")\n",
        "        filename = os.path.basename(image_url)\n",
        "\n",
        "        if \"staticflickr\" in image_url or len(filename.split('_')) == 2:\n",
        "            return os.path.join(base_dir, \"flickr\", filename)\n",
        "        else:\n",
        "            return os.path.join(base_dir, \"nyu\", filename)\n",
        "\n",
        "    def _prepare_samples(self):\n",
        "        \"\"\"Prépare les échantillons selon SpatialSense+\"\"\"\n",
        "        samples = []\n",
        "        images_not_found = 0\n",
        "        relations_filtered = 0\n",
        "\n",
        "        for img_data in self.annotations:\n",
        "            if img_data['split'] != self.split:\n",
        "                continue\n",
        "\n",
        "            img_path = self._find_image_path(img_data['url'])\n",
        "\n",
        "            if not os.path.exists(img_path):\n",
        "                images_not_found += 1\n",
        "                continue\n",
        "\n",
        "            for ann in img_data['annotations']:\n",
        "                if ann['label']:  # Seulement les annotations positives\n",
        "                    original_relation = ann['predicate']\n",
        "\n",
        "                    # Utilisation directe de la relation sans mapping\n",
        "                    if original_relation.lower().strip() in [rel.lower() for rel in SPATIAL_RELATIONS]:\n",
        "                        # Trouver la relation exacte (gestion de la casse)\n",
        "                        relation = None\n",
        "                        for rel in SPATIAL_RELATIONS:\n",
        "                            if rel.lower() == original_relation.lower().strip():\n",
        "                                relation = rel\n",
        "                                break\n",
        "\n",
        "                        if relation:\n",
        "                            sample = {\n",
        "                                'image_path': img_path,\n",
        "                                'subject': ann['subject']['name'],\n",
        "                                'object': ann['object']['name'],\n",
        "                                'relation': relation,\n",
        "                                'original_relation': original_relation,\n",
        "                                'subject_bbox': ann['subject'].get('bbox', None),\n",
        "                                'object_bbox': ann['object'].get('bbox', None)\n",
        "                            }\n",
        "                            samples.append(sample)\n",
        "                        else:\n",
        "                            relations_filtered += 1\n",
        "                    else:\n",
        "                        relations_filtered += 1\n",
        "\n",
        "        if images_not_found > 0:\n",
        "            print(f\"Images non trouvées: {images_not_found}\")\n",
        "        if relations_filtered > 0:\n",
        "            print(f\"Relations filtrées: {relations_filtered}\")\n",
        "\n",
        "        return samples\n",
        "\n",
        "    def _print_statistics(self):\n",
        "        \"\"\"Affiche les statistiques SpatialSense+\"\"\"\n",
        "        relation_counts = Counter([s['relation'] for s in self.data_samples])\n",
        "        print(f\"\\nDistribution SpatialSense+ dans {self.split}:\")\n",
        "        total = len(self.data_samples)\n",
        "        for relation in SPATIAL_RELATIONS:  # Ordre défini\n",
        "            count = relation_counts.get(relation, 0)\n",
        "            percentage = count / total * 100 if total > 0 else 0\n",
        "            print(f\"  {relation}: {count} ({percentage:.1f}%)\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data_samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"Retourne échantillon SpatialSense+\"\"\"\n",
        "        sample = self.data_samples[idx]\n",
        "\n",
        "        try:\n",
        "            # Chargement image (224x224 comme l'article)\n",
        "            image = Image.open(sample['image_path']).convert('RGB')\n",
        "\n",
        "            if self.transform:\n",
        "                image = self.transform(image)\n",
        "\n",
        "            label = self.relation_to_idx[sample['relation']]\n",
        "\n",
        "            metadata = {\n",
        "                'subject': sample['subject'],\n",
        "                'object': sample['object'],\n",
        "                'relation': sample['relation'],\n",
        "                'original_relation': sample['original_relation']\n",
        "            }\n",
        "\n",
        "            return image, label, metadata\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Erreur chargement {sample['image_path']}: {e}\")\n",
        "            # Image par défaut en cas d'erreur\n",
        "            dummy_image = Image.new('RGB', (224, 224), color='gray')\n",
        "            if self.transform:\n",
        "                dummy_image = self.transform(dummy_image)\n",
        "            else:\n",
        "                dummy_image = torch.zeros(3, 224, 224)\n",
        "\n",
        "            return dummy_image, 0, {\n",
        "                'subject': 'error', 'object': 'error',\n",
        "                'relation': 'next to', 'original_relation': 'error'\n",
        "            }\n",
        "\n",
        "# =============================================================================\n",
        "# TRANSFORMATIONS AMÉLIORÉES\n",
        "# =============================================================================\n",
        "\n",
        "def create_improved_vit_transforms():\n",
        "    \"\"\"Transformations améliorées avec augmentations plus fortes\"\"\"\n",
        "    normalize = transforms.Normalize(\n",
        "        mean=[0.5, 0.5, 0.5],\n",
        "        std=[0.5, 0.5, 0.5]\n",
        "    )\n",
        "\n",
        "    # Augmentations plus fortes pour l'entraînement\n",
        "    train_transform = transforms.Compose([\n",
        "        transforms.Resize((256, 256)),  # Resize plus grand\n",
        "        transforms.RandomCrop((224, 224)),  # Random crop\n",
        "        transforms.RandomHorizontalFlip(p=0.5),\n",
        "        transforms.RandomRotation(degrees=15),  # Rotation ajoutée\n",
        "        transforms.ColorJitter(brightness=0.2, contrast=0.2,\n",
        "                              saturation=0.2, hue=0.1),\n",
        "        transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),  # Translation\n",
        "        transforms.ToTensor(),\n",
        "        normalize\n",
        "    ])\n",
        "\n",
        "    val_transform = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "        normalize\n",
        "    ])\n",
        "\n",
        "    return train_transform, val_transform\n",
        "\n",
        "# =============================================================================\n",
        "# FONCTIONS D'ENTRAÎNEMENT AMÉLIORÉES\n",
        "# =============================================================================\n",
        "\n",
        "def get_improved_optimizer_and_scheduler(model, train_loader_len):\n",
        "    \"\"\"Optimiseur et scheduler améliorés\"\"\"\n",
        "\n",
        "    # Paramètres différentiés pour ViT et MLP\n",
        "    vit_params = []\n",
        "    mlp_params = []\n",
        "\n",
        "    for name, param in model.named_parameters():\n",
        "        if param.requires_grad:\n",
        "            if 'vit' in name:\n",
        "                vit_params.append(param)\n",
        "            else:\n",
        "                mlp_params.append(param)\n",
        "\n",
        "    # Learning rates différents\n",
        "    optimizer = optim.AdamW([\n",
        "        {'params': vit_params, 'lr': LEARNING_RATE * 0.1},  # LR plus petit pour ViT\n",
        "        {'params': mlp_params, 'lr': LEARNING_RATE}         # LR normal pour MLP\n",
        "    ], weight_decay=WEIGHT_DECAY)\n",
        "\n",
        "    # Scheduler avec warmup\n",
        "    total_steps = train_loader_len * EPOCHS\n",
        "    warmup_steps = train_loader_len * WARMUP_EPOCHS\n",
        "\n",
        "    def lr_lambda(step):\n",
        "        if step < warmup_steps:\n",
        "            return step / warmup_steps\n",
        "        else:\n",
        "            return max(MIN_LR / LEARNING_RATE,\n",
        "                      0.5 * (1 + np.cos(np.pi * (step - warmup_steps) / (total_steps - warmup_steps))))\n",
        "\n",
        "    scheduler = optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
        "\n",
        "    return optimizer, scheduler\n",
        "\n",
        "def train_one_epoch_improved(model, dataloader, criterion, optimizer, scheduler, device, epoch):\n",
        "    \"\"\"Entraînement amélioré avec mixed precision et gradient clipping\"\"\"\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    # Scaler pour mixed precision (si supporté)\n",
        "    scaler = torch.cuda.amp.GradScaler() if device.type == 'cuda' else None\n",
        "\n",
        "    progress_bar = tqdm(dataloader, desc=f'Epoch {epoch+1}')\n",
        "    for batch_idx, (images, labels, _) in enumerate(progress_bar):\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Mixed precision si disponible\n",
        "        if scaler is not None:\n",
        "            with torch.cuda.amp.autocast():\n",
        "                outputs = model(images)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.unscale_(optimizer)\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "        else:\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "            optimizer.step()\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "        current_lr = optimizer.param_groups[0]['lr']\n",
        "        progress_bar.set_postfix({\n",
        "            'loss': running_loss / (batch_idx + 1),\n",
        "            'acc': 100. * correct / total,\n",
        "            'lr': f'{current_lr:.2e}'\n",
        "        })\n",
        "\n",
        "    return running_loss / len(dataloader), 100. * correct / total\n",
        "\n",
        "def evaluate(model, dataloader, criterion, device):\n",
        "    \"\"\"Évalue le modèle\"\"\"\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels, _ in tqdm(dataloader, desc='Evaluating'):\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    return running_loss / len(dataloader), 100. * correct / total\n",
        "\n",
        "# =============================================================================\n",
        "# CORRECTION ANALYSE PERFORMANCE PAR RELATION\n",
        "# =============================================================================\n",
        "\n",
        "def analyze_performance_per_relation_fixed(model, dataloader, device):\n",
        "    \"\"\"Version corrigée de l'analyse par relation\"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    relation_correct = {rel: 0 for rel in SPATIAL_RELATIONS}\n",
        "    relation_total = {rel: 0 for rel in SPATIAL_RELATIONS}\n",
        "    relation_predictions = {rel: [] for rel in SPATIAL_RELATIONS}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels, metadata_batch in tqdm(dataloader, desc='Analyzing per relation'):\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "            # CORRECTION: Gestion correcte des métadonnées en batch\n",
        "            batch_size = len(labels)\n",
        "\n",
        "            for i in range(batch_size):\n",
        "                # Extraction correcte des métadonnées selon la structure\n",
        "                if isinstance(metadata_batch, dict):\n",
        "                    # Cas où metadata_batch est un dict avec des listes\n",
        "                    true_rel = metadata_batch['relation'][i] if 'relation' in metadata_batch else None\n",
        "                elif isinstance(metadata_batch, (list, tuple)):\n",
        "                    # Cas où metadata_batch est une liste/tuple\n",
        "                    true_rel = metadata_batch[i]['relation'] if i < len(metadata_batch) else None\n",
        "                else:\n",
        "                    # Cas imprévu\n",
        "                    print(f\"Structure métadonnées inattendue: {type(metadata_batch)}\")\n",
        "                    continue\n",
        "\n",
        "                if true_rel is None:\n",
        "                    continue\n",
        "\n",
        "                pred_idx = predicted[i].item()\n",
        "                pred_rel = SPATIAL_RELATIONS[pred_idx]\n",
        "\n",
        "                relation_total[true_rel] += 1\n",
        "                relation_predictions[true_rel].append(pred_rel)\n",
        "\n",
        "                if pred_rel == true_rel:\n",
        "                    relation_correct[true_rel] += 1\n",
        "\n",
        "    # Calcul accuracies\n",
        "    relation_accuracies = {}\n",
        "    for rel in SPATIAL_RELATIONS:\n",
        "        if relation_total[rel] > 0:\n",
        "            relation_accuracies[rel] = relation_correct[rel] / relation_total[rel] * 100\n",
        "        else:\n",
        "            relation_accuracies[rel] = 0.0\n",
        "\n",
        "    # Visualisation\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "    # 1. Accuracy par relation\n",
        "    relations = list(relation_accuracies.keys())\n",
        "    accuracies = list(relation_accuracies.values())\n",
        "    counts = [relation_total[rel] for rel in relations]\n",
        "\n",
        "    bars = ax1.bar(range(len(relations)), accuracies,\n",
        "                   color='skyblue', alpha=0.8)\n",
        "    ax1.set_xlabel('Relations SpatialSense+')\n",
        "    ax1.set_ylabel('Accuracy (%)')\n",
        "    ax1.set_title('Performance par Relation SpatialSense+')\n",
        "    ax1.set_xticks(range(len(relations)))\n",
        "    ax1.set_xticklabels(relations, rotation=45, ha='right')\n",
        "    ax1.grid(True, axis='y', alpha=0.3)\n",
        "\n",
        "    # Annotations avec nombre d'échantillons\n",
        "    for i, (bar, acc, count) in enumerate(zip(bars, accuracies, counts)):\n",
        "        height = bar.get_height()\n",
        "        ax1.annotate(f'{acc:.1f}%\\n(n={count})',\n",
        "                    xy=(bar.get_x() + bar.get_width() / 2, height),\n",
        "                    xytext=(0, 3), textcoords=\"offset points\",\n",
        "                    ha='center', va='bottom', fontsize=8)\n",
        "\n",
        "    # 2. Distribution des échantillons\n",
        "    ax2.bar(range(len(relations)), counts, color='lightcoral', alpha=0.8)\n",
        "    ax2.set_xlabel('Relations SpatialSense+')\n",
        "    ax2.set_ylabel('Nombre d\\'échantillons')\n",
        "    ax2.set_title('Distribution des Échantillons par Relation')\n",
        "    ax2.set_xticks(range(len(relations)))\n",
        "    ax2.set_xticklabels(relations, rotation=45, ha='right')\n",
        "    ax2.grid(True, axis='y', alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('spatialsense_performance_per_relation.png', dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "    return relation_accuracies, relation_total\n",
        "\n",
        "# =============================================================================\n",
        "# HEATMAP GÉNÉRATION (inspirée de l'article)\n",
        "# =============================================================================\n",
        "\n",
        "def generate_attention_heatmap_like_article(model, image, target_class, device):\n",
        "    \"\"\"\n",
        "    Génère une heatmap d'attention style Haldekar et al. 2017\n",
        "    Utilise l'attention ViT comme équivalent du masking de l'article\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        image_tensor = image.unsqueeze(0).to(device)\n",
        "\n",
        "        # Prédiction normale\n",
        "        output = model(image_tensor)\n",
        "\n",
        "        # Attention ViT (équivalent heatmap article)\n",
        "        attention_weights = model.get_attention_visualization(image_tensor)\n",
        "\n",
        "        # Reshape attention pour visualisation (14x14 patches pour ViT-Base)\n",
        "        attention_map = attention_weights[0].cpu().numpy()\n",
        "        patch_size = int(np.sqrt(len(attention_map)))\n",
        "        attention_2d = attention_map.reshape(patch_size, patch_size)\n",
        "\n",
        "        # Redimensionner à 224x224 (taille image article)\n",
        "        attention_resized = np.kron(attention_2d, np.ones((224//patch_size, 224//patch_size)))\n",
        "\n",
        "        return attention_resized, output\n",
        "\n",
        "def visualize_spatialsense_predictions(model, dataset, device, num_samples=6):\n",
        "    \"\"\"\n",
        "    Visualise les prédictions SpatialSense+ avec heatmaps (style article Haldekar)\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    fig, axes = plt.subplots(num_samples, 4, figsize=(16, 4*num_samples))\n",
        "    if num_samples == 1:\n",
        "        axes = axes.reshape(1, -1)\n",
        "\n",
        "    for i in range(min(num_samples, len(dataset))):\n",
        "        # Échantillon SpatialSense+\n",
        "        image, label, metadata = dataset[i]\n",
        "\n",
        "        # Dénormaliser pour affichage\n",
        "        img_display = image.clone()\n",
        "        img_display = (img_display + 1) / 2  # ViT dénormalization\n",
        "        img_display = img_display.clamp(0, 1).permute(1, 2, 0)\n",
        "\n",
        "        # Prédiction avec heatmap\n",
        "        attention_map, output = generate_attention_heatmap_like_article(\n",
        "            model, image, label, device\n",
        "        )\n",
        "\n",
        "        probs = torch.softmax(output, dim=1)\n",
        "        pred_idx = torch.argmax(probs)\n",
        "        pred_prob = probs[0, pred_idx].item()\n",
        "\n",
        "        pred_relation = dataset.idx_to_relation[pred_idx.item()]\n",
        "        true_relation = metadata['relation']\n",
        "\n",
        "        # 1. Image originale\n",
        "        axes[i, 0].imshow(img_display)\n",
        "        axes[i, 0].set_title(f\"{metadata['subject']} - {metadata['object']}\")\n",
        "        axes[i, 0].axis('off')\n",
        "\n",
        "        # 2. Heatmap d'attention (équivalent masking article)\n",
        "        axes[i, 1].imshow(attention_map, cmap='hot', alpha=0.8)\n",
        "        axes[i, 1].set_title('Attention Heatmap\\n(équivalent article)')\n",
        "        axes[i, 1].axis('off')\n",
        "\n",
        "        # 3. Superposition (comme Figure 2 de l'article)\n",
        "        axes[i, 2].imshow(img_display)\n",
        "        axes[i, 2].imshow(attention_map, cmap='jet', alpha=0.4)\n",
        "        axes[i, 2].set_title('Superposition\\n(style Figure 2)')\n",
        "        axes[i, 2].axis('off')\n",
        "\n",
        "        # 4. Informations prédiction\n",
        "        axes[i, 3].axis('off')\n",
        "        info_text = f\"\"\"\n",
        "        SpatialSense+ Sample:\n",
        "\n",
        "        Vérité: {true_relation}\n",
        "        Prédiction: {pred_relation}\n",
        "        Original: {metadata.get('original_relation', 'N/A')}\n",
        "\n",
        "        Confiance: {pred_prob:.2%}\n",
        "\n",
        "        Résultat: {'✓ CORRECT' if pred_relation == true_relation else '✗ INCORRECT'}\n",
        "\n",
        "        Attention:\n",
        "        Max: {attention_map.max():.3f}\n",
        "        Min: {attention_map.min():.3f}\n",
        "        \"\"\"\n",
        "        color = 'lightgreen' if pred_relation == true_relation else 'lightcoral'\n",
        "        axes[i, 3].text(0.1, 0.5, info_text, transform=axes[i, 3].transAxes,\n",
        "                       fontsize=10, verticalalignment='center',\n",
        "                       bbox=dict(boxstyle='round', facecolor=color, alpha=0.8))\n",
        "\n",
        "    plt.suptitle('Analyse SpatialSense+ - Style Haldekar et al. 2017', fontsize=16, fontweight='bold')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('spatialsense_predictions_analysis.png', dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "# =============================================================================\n",
        "# VISUALISATIONS RÉSULTATS K-FOLD\n",
        "# =============================================================================\n",
        "\n",
        "def visualize_spatialsense_results(fold_results):\n",
        "    \"\"\"Visualise les résultats SpatialSense+\"\"\"\n",
        "\n",
        "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))\n",
        "\n",
        "    # 1. Courbes de loss\n",
        "    for result in fold_results:\n",
        "        fold = result['fold']\n",
        "        epochs_range = range(1, len(result['train_losses']) + 1)\n",
        "\n",
        "        ax1.plot(epochs_range, result['train_losses'],\n",
        "                label=f'Fold {fold} Train', linewidth=2)\n",
        "        ax1.plot(epochs_range, result['val_losses'], '--',\n",
        "                label=f'Fold {fold} Val', alpha=0.8)\n",
        "\n",
        "    ax1.set_title('Loss SpatialSense+', fontsize=14, fontweight='bold')\n",
        "    ax1.set_xlabel('Époque')\n",
        "    ax1.set_ylabel('Loss')\n",
        "    ax1.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "    ax1.grid(True, alpha=0.3)\n",
        "\n",
        "    # 2. Courbes d'accuracy\n",
        "    for result in fold_results:\n",
        "        fold = result['fold']\n",
        "        epochs_range = range(1, len(result['train_accs']) + 1)\n",
        "\n",
        "        ax2.plot(epochs_range, result['train_accs'],\n",
        "                label=f'Fold {fold} Train', linewidth=2)\n",
        "        ax2.plot(epochs_range, result['val_accs'], '--',\n",
        "                label=f'Fold {fold} Val', alpha=0.8)\n",
        "\n",
        "    ax2.set_title('Accuracy SpatialSense+', fontsize=14, fontweight='bold')\n",
        "    ax2.set_xlabel('Époque')\n",
        "    ax2.set_ylabel('Accuracy (%)')\n",
        "    ax2.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "    ax2.grid(True, alpha=0.3)\n",
        "\n",
        "    # 3. Performance par fold\n",
        "    folds = [r['fold'] for r in fold_results]\n",
        "    best_val_accs = [r['best_val_acc'] for r in fold_results]\n",
        "\n",
        "    x = np.arange(len(folds))\n",
        "    bars = ax3.bar(x, best_val_accs, color='lightblue', alpha=0.8)\n",
        "\n",
        "    ax3.set_title('Performance SpatialSense+ par Fold', fontsize=14, fontweight='bold')\n",
        "    ax3.set_xlabel('Fold')\n",
        "    ax3.set_ylabel('Best Validation Accuracy (%)')\n",
        "    ax3.set_xticks(x)\n",
        "    ax3.set_xticklabels(folds)\n",
        "    ax3.grid(True, axis='y', alpha=0.3)\n",
        "\n",
        "    # Annotations\n",
        "    for i, (bar, acc) in enumerate(zip(bars, best_val_accs)):\n",
        "        height = bar.get_height()\n",
        "        ax3.annotate(f'{acc:.1f}%',\n",
        "                    xy=(bar.get_x() + bar.get_width() / 2, height),\n",
        "                    xytext=(0, 3), textcoords=\"offset points\",\n",
        "                    ha='center', va='bottom', fontsize=9)\n",
        "\n",
        "    # 4. Résumé statistique SpatialSense+\n",
        "    ax4.axis('off')\n",
        "\n",
        "    mean_best = np.mean(best_val_accs)\n",
        "    std_best = np.std(best_val_accs)\n",
        "\n",
        "    summary_text = f\"\"\"\n",
        "    RÉSULTATS SPATIALSENSE+\n",
        "\n",
        "    Dataset:\n",
        "    • Relations: {len(SPATIAL_RELATIONS)} classes\n",
        "    • Architecture: ViT + MLP Haldekar-style\n",
        "    • Modèle: ViT-Base-224 + MLP(4096→512→256→{len(SPATIAL_RELATIONS)})\n",
        "\n",
        "    Performance:\n",
        "    • Accuracy moyenne: {mean_best:.2f}% ± {std_best:.2f}%\n",
        "    • Min: {min(best_val_accs):.2f}%, Max: {max(best_val_accs):.2f}%\n",
        "\n",
        "    Paramètres:\n",
        "    • Epochs: {EPOCHS}\n",
        "    • K-folds: {len(fold_results)}\n",
        "    • Learning rate: {LEARNING_RATE}\n",
        "\n",
        "    Comparaison article Haldekar:\n",
        "    • Article (SUN09): 55.98% test accuracy\n",
        "    • Notre approche: {mean_best:.2f}% (ViT vs VGG)\n",
        "    \"\"\"\n",
        "\n",
        "    color = 'lightgreen' if mean_best > 55.98 else 'lightyellow'\n",
        "    ax4.text(0.05, 0.95, summary_text, transform=ax4.transAxes,\n",
        "             fontsize=11, verticalalignment='top',\n",
        "             bbox=dict(boxstyle='round,pad=0.5', facecolor=color, alpha=0.8))\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('spatialsense_results_analysis.png', dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "# =============================================================================\n",
        "# FONCTION D'ENTRAÎNEMENT PRINCIPALE AMÉLIORÉE\n",
        "# =============================================================================\n",
        "\n",
        "def train_improved_spatialsense_model(data_dir, epochs=EPOCHS, k_folds=K_FOLDS):\n",
        "    \"\"\"Entraînement amélioré avec tous les optimisations\"\"\"\n",
        "\n",
        "    print(\"=\"*60)\n",
        "    print(\"ENTRAÎNEMENT AMÉLIORÉ HALDEKAR-STYLE + ViT\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # Transformations améliorées\n",
        "    train_transform, val_transform = create_improved_vit_transforms()\n",
        "\n",
        "    # Dataset\n",
        "    full_dataset = SpatialSenseDataset(\n",
        "        data_dir=data_dir,\n",
        "        split='train',\n",
        "        transform=train_transform\n",
        "    )\n",
        "\n",
        "    if len(full_dataset) == 0:\n",
        "        print(\"Erreur: Dataset vide!\")\n",
        "        return []\n",
        "\n",
        "    # K-fold validation\n",
        "    kfold = KFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
        "    fold_results = []\n",
        "\n",
        "    for fold, (train_indices, val_indices) in enumerate(kfold.split(full_dataset)):\n",
        "        print(f\"\\n{'='*40}\")\n",
        "        print(f\"FOLD {fold + 1}/{k_folds} - AMÉLIORÉ\")\n",
        "        print(f\"{'='*40}\")\n",
        "\n",
        "        # Sous-ensembles\n",
        "        train_subset = torch.utils.data.Subset(full_dataset, train_indices)\n",
        "        val_subset = torch.utils.data.Subset(full_dataset, val_indices)\n",
        "\n",
        "        # DataLoaders\n",
        "        train_loader = DataLoader(\n",
        "            train_subset, batch_size=BATCH_SIZE, shuffle=True,\n",
        "            num_workers=4, pin_memory=True, persistent_workers=True\n",
        "        )\n",
        "\n",
        "        val_loader = DataLoader(\n",
        "            val_subset, batch_size=BATCH_SIZE, shuffle=False,\n",
        "            num_workers=4, pin_memory=True, persistent_workers=True\n",
        "        )\n",
        "\n",
        "        # Modèle amélioré avec fine-tuning partiel\n",
        "        model = ImprovedViTSpatialRelationModel(\n",
        "            num_relations=len(SPATIAL_RELATIONS),\n",
        "            vit_model='google/vit-base-patch16-224',\n",
        "            freeze_vit=False,  # Fine-tuning activé\n",
        "            fine_tune_layers=4\n",
        "        )\n",
        "        model = model.to(DEVICE)\n",
        "\n",
        "        # Optimiseur et scheduler améliorés\n",
        "        optimizer, scheduler = get_improved_optimizer_and_scheduler(\n",
        "            model, len(train_loader)\n",
        "        )\n",
        "\n",
        "        # Loss avec smoothing\n",
        "        criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
        "\n",
        "        # Historique\n",
        "        train_losses, val_losses = [], []\n",
        "        train_accs, val_accs = [], []\n",
        "        best_val_acc = 0.0\n",
        "\n",
        "        # Boucle d'entraînement améliorée\n",
        "        for epoch in range(epochs):\n",
        "            print(f\"\\nEpoch {epoch+1}/{epochs}\")\n",
        "\n",
        "            # Entraînement avec améliorations\n",
        "            train_loss, train_acc = train_one_epoch_improved(\n",
        "                model, train_loader, criterion, optimizer, scheduler, DEVICE, epoch\n",
        "            )\n",
        "            train_losses.append(train_loss)\n",
        "            train_accs.append(train_acc)\n",
        "\n",
        "            # Validation\n",
        "            val_loss, val_acc = evaluate(model, val_loader, criterion, DEVICE)\n",
        "            val_losses.append(val_loss)\n",
        "            val_accs.append(val_acc)\n",
        "\n",
        "            print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n",
        "            print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n",
        "            print(f\"LR: {optimizer.param_groups[0]['lr']:.2e}\")\n",
        "\n",
        "            # Sauvegarde si meilleur\n",
        "            if val_acc > best_val_acc:\n",
        "                best_val_acc = val_acc\n",
        "                torch.save(model.state_dict(),\n",
        "                          f'improved_spatialsense_model_fold_{fold+1}.pth')\n",
        "\n",
        "        # Résultats du fold\n",
        "        fold_result = {\n",
        "            'fold': fold + 1,\n",
        "            'train_losses': train_losses,\n",
        "            'val_losses': val_losses,\n",
        "            'train_accs': train_accs,\n",
        "            'val_accs': val_accs,\n",
        "            'best_val_acc': best_val_acc,\n",
        "            'final_train_acc': train_accs[-1],\n",
        "            'final_val_acc': val_accs[-1],\n",
        "            'total_epochs': len(train_losses),\n",
        "            'model': model\n",
        "        }\n",
        "\n",
        "        fold_results.append(fold_result)\n",
        "        print(f\"\\nRésumé Fold {fold + 1}: Meilleure acc: {best_val_acc:.2f}%\")\n",
        "\n",
        "    return fold_results\n",
        "\n",
        "# =============================================================================\n",
        "# FONCTION PRINCIPALE AMÉLIORÉE\n",
        "# =============================================================================\n",
        "\n",
        "def main_improved_spatialsense_experiment():\n",
        "    \"\"\"Expérience principale avec toutes les améliorations\"\"\"\n",
        "\n",
        "    DATA_DIR = \"data/spatialsense\"\n",
        "\n",
        "\n",
        "    if not os.path.exists(DATA_DIR):\n",
        "        print(f\"\\nERREUR: Le répertoire {DATA_DIR} n'existe pas!\")\n",
        "        return\n",
        "\n",
        "    # Entraînement amélioré\n",
        "    print(\"\\n     Entraînement avec améliorations\")\n",
        "    results = train_improved_spatialsense_model(DATA_DIR)\n",
        "\n",
        "    if not results:\n",
        "        print(\"Erreur pendant l'entraînement\")\n",
        "        return\n",
        "\n",
        "    # Analyse des résultats\n",
        "    print(\"\\n   Analyse des résultats améliorés\")\n",
        "    visualize_spatialsense_results(results)\n",
        "\n",
        "    # Test avec meilleur modèle\n",
        "    print(\"\\n   Test du meilleur modèle\")\n",
        "    best_fold_idx = np.argmax([r['best_val_acc'] for r in results])\n",
        "    best_model = results[best_fold_idx]['model']\n",
        "\n",
        "    # Dataset de test\n",
        "    _, test_transform = create_improved_vit_transforms()\n",
        "    test_dataset = SpatialSenseDataset(DATA_DIR, split='test', transform=test_transform)\n",
        "\n",
        "    if len(test_dataset) == 0:\n",
        "        test_dataset = SpatialSenseDataset(DATA_DIR, split='val', transform=test_transform)\n",
        "\n",
        "    if len(test_dataset) > 0:\n",
        "        test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE,\n",
        "                               shuffle=False, num_workers=4, pin_memory=True)\n",
        "\n",
        "        # Visualisation des prédictions\n",
        "        visualize_spatialsense_predictions(best_model, test_dataset, DEVICE, num_samples=8)\n",
        "\n",
        "        # Analyse par relation avec correction\n",
        "        relation_accs, relation_counts = analyze_performance_per_relation_fixed(\n",
        "            best_model, test_loader, DEVICE\n",
        "        )\n",
        "\n",
        "        # Évaluation finale\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        test_loss, test_acc = evaluate(best_model, test_loader, criterion, DEVICE)\n",
        "\n",
        "        print(f\"\\n  Résultats finaux améliorés:\")\n",
        "        print(f\"  - Test Loss: {test_loss:.4f}\")\n",
        "        print(f\"  - Test Accuracy: {test_acc:.2f}%\")\n",
        "        print(f\"  - Amélioration vs article: {test_acc - 55.98:.2f}%\")\n",
        "\n",
        "        # Résumé par relation\n",
        "        print(\"\\n       Performance par relation:\")\n",
        "        sorted_relations = sorted(relation_accs.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "        print(\"\\nMeilleures performances:\")\n",
        "        for rel, acc in sorted_relations[:3]:\n",
        "            count = relation_counts[rel]\n",
        "            print(f\"  {rel}: {acc:.2f}% (n={count})\")\n",
        "\n",
        "        print(\"\\nPires performances:\")\n",
        "        for rel, acc in sorted_relations[-3:]:\n",
        "            count = relation_counts[rel]\n",
        "            print(f\"  {rel}: {acc:.2f}% (n={count})\")\n",
        "\n",
        "        overall_acc = sum([relation_counts[rel] * relation_accs[rel] for rel in SPATIAL_RELATIONS]) / sum(relation_counts.values())\n",
        "        print(f\"\\nAccuracy globale pondérée: {overall_acc:.2f}%\")\n",
        "\n",
        "    else:\n",
        "        print(\"Aucun dataset de test disponible\")\n",
        "\n",
        "    # Sauvegarde\n",
        "    torch.save(best_model.state_dict(), 'improved_spatialsense_final_model.pth')\n",
        "\n",
        "    # Comparaison finale\n",
        "    mean_acc = np.mean([r['best_val_acc'] for r in results])\n",
        "    std_acc = np.std([r['best_val_acc'] for r in results])\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"COMPARAISON FINALE AVEC HALDEKAR ET AL. 2017\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    print(f\"\\nArticle original:\")\n",
        "    print(f\"  - Dataset: SUN09 (3 relations)\")\n",
        "    print(f\"  - Architecture: VGGNet FC-7 + MLP\")\n",
        "    print(f\"  - Performance: 55.98% test accuracy\")\n",
        "\n",
        "    print(f\"\\nNotre implémentation améliorée:\")\n",
        "    print(f\"  - Dataset: SpatialSense+ ({len(SPATIAL_RELATIONS)} relations)\")\n",
        "    print(f\"  - Architecture: ViT-Base + MLP amélioré\")\n",
        "    print(f\"  - Performance: {mean_acc:.2f}% ± {std_acc:.2f}%\")\n",
        "\n",
        "    if 'test_acc' in locals():\n",
        "        print(f\"  - Test accuracy: {test_acc:.2f}%\")\n",
        "        if test_acc > 55.98:\n",
        "            print(f\"          Amélioration: +{test_acc - 55.98:.2f}%\")\n",
        "        else:\n",
        "            print(f\"        À améliorer: {test_acc - 55.98:.2f}% (tâche plus difficile)\")\n",
        "\n",
        "    print(f\"\\nTechniques modernes ajoutées:\")\n",
        "    print(f\"     Vision Transformer (2021) vs VGGNet (2014)\")\n",
        "    print(f\"     Fine-tuning partiel\")\n",
        "    print(f\"     BatchNormalization\")\n",
        "    print(f\"     Learning rate scheduling avec warmup\")\n",
        "    print(f\"     Mixed precision training\")\n",
        "    print(f\"     Label smoothing\")\n",
        "    print(f\"     Data augmentation renforcée\")\n",
        "    print(f\"     Gradient clipping\")\n",
        "\n",
        "    print(\"\\n   Expérience améliorée terminée avec succès!\")\n",
        "    print(f\"       {len(SPATIAL_RELATIONS)} relations vs 3 dans l'article original\")\n",
        "    print(f\"     Architecture moderne adaptée de Haldekar et al. 2017\")\n",
        "\n",
        "    return results\n",
        "\n",
        "# =============================================================================\n",
        "# POINT D'ENTRÉE\n",
        "# =============================================================================\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main_improved_spatialsense_experiment()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "yeB7rwarsOX0",
        "outputId": "0a266e16-12e4-496c-b0d7-55aab46274d1"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image, ImageDraw\n",
        "import numpy as np\n",
        "import json\n",
        "import os\n",
        "from sklearn.model_selection import KFold\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import cv2\n",
        "from collections import Counter\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# =============================================================================\n",
        "# CONFIGURATION GLOBALE\n",
        "# =============================================================================\n",
        "\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Device utilisé: {DEVICE}\")\n",
        "\n",
        "# Hyperparamètres\n",
        "BATCH_SIZE = 8  # Réduit car deux images en entrée\n",
        "LEARNING_RATE = 0.001\n",
        "EPOCHS = 15\n",
        "K_FOLDS = 5\n",
        "IMG_SIZE = 224\n",
        "DROPOUT_RATE = 0.4\n",
        "\n",
        "# Relations spatiales SpatialSense+\n",
        "SPATIAL_RELATIONS = [\n",
        "    'above', 'behind', 'in', 'in front of', 'next to',\n",
        "    'on', 'to the left of', 'to the right of', 'under'\n",
        "]\n",
        "\n",
        "print(f\"Architecture DUALE avec masquage bounding box:\")\n",
        "print(f\"  - Relations: {len(SPATIAL_RELATIONS)}\")\n",
        "print(f\"  - Entrée 1: Image originale\")\n",
        "print(f\"  - Entrée 2: Image masquée (seules les bounding boxes visibles)\")\n",
        "\n",
        "# =============================================================================\n",
        "# DATASET DUAL INPUT AVEC MASQUAGE BOUNDING BOX\n",
        "# =============================================================================\n",
        "\n",
        "class DualInputSpatialSenseDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Dataset SpatialSense+ avec architecture duale:\n",
        "    - Image originale\n",
        "    - Image masquée où seuls les contenus des bounding boxes sont visibles\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, data_dir, split='train', transform=None, mask_background_color=(128, 128, 128)):\n",
        "        self.data_dir = data_dir\n",
        "        self.split = split\n",
        "        self.transform = transform\n",
        "        self.mask_background_color = mask_background_color  # Couleur de masquage\n",
        "\n",
        "        self.relation_to_idx = {rel: idx for idx, rel in enumerate(SPATIAL_RELATIONS)}\n",
        "        self.idx_to_relation = {idx: rel for rel, idx in self.relation_to_idx.items()}\n",
        "\n",
        "        self.annotations = self._load_annotations()\n",
        "        self.data_samples = self._prepare_samples()\n",
        "\n",
        "        print(f\"Dataset DUAL SpatialSense+ {split}: {len(self.data_samples)} échantillons\")\n",
        "        if len(self.data_samples) > 0:\n",
        "            self._print_statistics()\n",
        "\n",
        "    def _load_annotations(self):\n",
        "        annotations_path = os.path.join(self.data_dir, 'annotations.json')\n",
        "        try:\n",
        "            with open(annotations_path, 'r') as f:\n",
        "                return json.load(f)\n",
        "        except FileNotFoundError:\n",
        "            print(f\"Erreur: {annotations_path} non trouvé!\")\n",
        "            return []\n",
        "\n",
        "    def _find_image_path(self, image_url):\n",
        "        base_dir = os.path.join(self.data_dir, \"images\", \"images\")\n",
        "        filename = os.path.basename(image_url)\n",
        "\n",
        "        if \"staticflickr\" in image_url or len(filename.split('_')) == 2:\n",
        "            return os.path.join(base_dir, \"flickr\", filename)\n",
        "        else:\n",
        "            return os.path.join(base_dir, \"nyu\", filename)\n",
        "\n",
        "    def _prepare_samples(self):\n",
        "        samples = []\n",
        "        images_not_found = 0\n",
        "\n",
        "        for img_data in self.annotations:\n",
        "            if img_data['split'] != self.split:\n",
        "                continue\n",
        "\n",
        "            img_path = self._find_image_path(img_data['url'])\n",
        "\n",
        "            if not os.path.exists(img_path):\n",
        "                images_not_found += 1\n",
        "                continue\n",
        "\n",
        "            for ann in img_data['annotations']:\n",
        "                if ann['label'] and ann['predicate'].lower().strip() in [rel.lower() for rel in SPATIAL_RELATIONS]:\n",
        "                    # Trouver la relation correspondante\n",
        "                    relation = None\n",
        "                    for rel in SPATIAL_RELATIONS:\n",
        "                        if rel.lower() == ann['predicate'].lower().strip():\n",
        "                            relation = rel\n",
        "                            break\n",
        "\n",
        "                    if relation and 'bbox' in ann['subject'] and 'bbox' in ann['object']:\n",
        "                        sample = {\n",
        "                            'image_path': img_path,\n",
        "                            'subject': ann['subject']['name'],\n",
        "                            'object': ann['object']['name'],\n",
        "                            'relation': relation,\n",
        "                            'original_relation': ann['predicate'],\n",
        "                            'subject_bbox': ann['subject']['bbox'],  # [y1, y2, x1, x2]\n",
        "                            'object_bbox': ann['object']['bbox'],   # [y1, y2, x1, x2]\n",
        "                            'image_width': img_data['width'],\n",
        "                            'image_height': img_data['height']\n",
        "                        }\n",
        "                        samples.append(sample)\n",
        "\n",
        "        if images_not_found > 0:\n",
        "            print(f\"Images non trouvées: {images_not_found}\")\n",
        "\n",
        "        return samples\n",
        "\n",
        "    def _create_masked_image(self, image, subject_bbox, object_bbox):\n",
        "        \"\"\"\n",
        "        Crée une image masquée où seuls les contenus des bounding boxes sont visibles\n",
        "        bbox format: [y1, y2, x1, x2]\n",
        "        \"\"\"\n",
        "        # Convertir en array numpy pour manipulation\n",
        "        img_array = np.array(image)\n",
        "\n",
        "        # Créer un masque de la même taille, rempli avec la couleur de fond\n",
        "        masked_array = np.full_like(img_array, self.mask_background_color)\n",
        "\n",
        "        # Fonction pour appliquer une bounding box\n",
        "        def apply_bbox(bbox):\n",
        "            try:\n",
        "                y1, y2, x1, x2 = bbox\n",
        "                # Clamp les coordonnées pour éviter les débordements\n",
        "                y1 = max(0, min(int(y1), img_array.shape[0]))\n",
        "                y2 = max(0, min(int(y2), img_array.shape[0]))\n",
        "                x1 = max(0, min(int(x1), img_array.shape[1]))\n",
        "                x2 = max(0, min(int(x2), img_array.shape[1]))\n",
        "\n",
        "                # Copier le contenu de la bounding box de l'image originale\n",
        "                if y2 > y1 and x2 > x1:\n",
        "                    masked_array[y1:y2, x1:x2] = img_array[y1:y2, x1:x2]\n",
        "            except Exception as e:\n",
        "                print(f\"Erreur application bbox {bbox}: {e}\")\n",
        "\n",
        "        # Appliquer les deux bounding boxes\n",
        "        apply_bbox(subject_bbox)\n",
        "        apply_bbox(object_bbox)\n",
        "\n",
        "        # Reconvertir en PIL Image\n",
        "        masked_image = Image.fromarray(masked_array)\n",
        "        return masked_image\n",
        "\n",
        "    def _print_statistics(self):\n",
        "        relation_counts = Counter([s['relation'] for s in self.data_samples])\n",
        "        print(f\"\\nDistribution DUAL SpatialSense+ dans {self.split}:\")\n",
        "        total = len(self.data_samples)\n",
        "\n",
        "        for relation in SPATIAL_RELATIONS:\n",
        "            count = relation_counts.get(relation, 0)\n",
        "            percentage = count / total * 100 if total > 0 else 0\n",
        "            print(f\"   {relation}: {count} ({percentage:.1f}%)\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data_samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sample = self.data_samples[idx]\n",
        "\n",
        "        try:\n",
        "            # Charger l'image originale\n",
        "            original_image = Image.open(sample['image_path']).convert('RGB')\n",
        "\n",
        "            # Créer l'image masquée avec les bounding boxes\n",
        "            masked_image = self._create_masked_image(\n",
        "                original_image,\n",
        "                sample['subject_bbox'],\n",
        "                sample['object_bbox']\n",
        "            )\n",
        "\n",
        "            # Appliquer les transformations\n",
        "            if self.transform:\n",
        "                original_image = self.transform(original_image)\n",
        "                masked_image = self.transform(masked_image)\n",
        "\n",
        "            label = self.relation_to_idx[sample['relation']]\n",
        "\n",
        "            metadata = {\n",
        "                'subject': sample['subject'],\n",
        "                'object': sample['object'],\n",
        "                'relation': sample['relation'],\n",
        "                'original_relation': sample['original_relation'],\n",
        "                'subject_bbox': sample['subject_bbox'],\n",
        "                'object_bbox': sample['object_bbox']\n",
        "            }\n",
        "\n",
        "            return original_image, masked_image, label, metadata\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Erreur chargement {sample['image_path']}: {e}\")\n",
        "            # Images par défaut en cas d'erreur\n",
        "            dummy_original = Image.new('RGB', (224, 224), color='gray')\n",
        "            dummy_masked = Image.new('RGB', (224, 224), color=self.mask_background_color)\n",
        "\n",
        "            if self.transform:\n",
        "                dummy_original = self.transform(dummy_original)\n",
        "                dummy_masked = self.transform(dummy_masked)\n",
        "            else:\n",
        "                dummy_original = torch.zeros(3, 224, 224)\n",
        "                dummy_masked = torch.zeros(3, 224, 224)\n",
        "\n",
        "            return dummy_original, dummy_masked, 0, {\n",
        "                'subject': 'error', 'object': 'error',\n",
        "                'relation': 'next to', 'original_relation': 'error',\n",
        "                'subject_bbox': [0, 0, 0, 0], 'object_bbox': [0, 0, 0, 0]\n",
        "            }\n",
        "\n",
        "# =============================================================================\n",
        "# ARCHITECTURE DUALE VGG\n",
        "# =============================================================================\n",
        "\n",
        "class DualVGGFeatureExtractor(nn.Module):\n",
        "    \"\"\"\n",
        "    Extracteur de features VGG dual:\n",
        "    - Branche 1: Image originale\n",
        "    - Branche 2: Image masquée (bounding boxes seulement)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, fusion_method='concat'):\n",
        "        super(DualVGGFeatureExtractor, self).__init__()\n",
        "\n",
        "        self.fusion_method = fusion_method  # 'concat', 'add', 'attention'\n",
        "\n",
        "        # VGG16 pré-entraîné partagé pour les deux branches\n",
        "        vgg16 = models.vgg16(pretrained=True)\n",
        "\n",
        "        # Extraction des couches\n",
        "        self.features = vgg16.features\n",
        "        self.avgpool = vgg16.avgpool\n",
        "\n",
        "        # FC-7 partagé\n",
        "        classifier_layers = list(vgg16.classifier.children())[:6]\n",
        "        self.fc7 = nn.Sequential(*classifier_layers)\n",
        "\n",
        "        # Gel des poids VGG\n",
        "        for param in self.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        # Couche de fusion des features\n",
        "        if fusion_method == 'concat':\n",
        "            self.fusion_dim = 4096 * 2  # Concaténation\n",
        "        elif fusion_method == 'add':\n",
        "            self.fusion_dim = 4096      # Addition\n",
        "        elif fusion_method == 'attention':\n",
        "            self.fusion_dim = 4096\n",
        "            # Mécanisme d'attention simple\n",
        "            self.attention = nn.Sequential(\n",
        "                nn.Linear(4096 * 2, 4096),\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(4096, 2),\n",
        "                nn.Softmax(dim=1)\n",
        "            )\n",
        "\n",
        "        print(f\"Dual VGG Feature Extractor:\")\n",
        "        print(f\"  - Méthode fusion: {fusion_method}\")\n",
        "        print(f\"  - Dimension sortie: {self.fusion_dim}\")\n",
        "        print(f\"  - Branche 1: Image originale\")\n",
        "        print(f\"  - Branche 2: Image masquée (bounding boxes)\")\n",
        "\n",
        "    def forward(self, original_img, masked_img):\n",
        "        # Extraction features pour image originale\n",
        "        features_orig = self._extract_features(original_img)\n",
        "\n",
        "        # Extraction features pour image masquée\n",
        "        features_masked = self._extract_features(masked_img)\n",
        "\n",
        "        # Fusion des features\n",
        "        if self.fusion_method == 'concat':\n",
        "            # Concaténation simple\n",
        "            fused_features = torch.cat([features_orig, features_masked], dim=1)\n",
        "        elif self.fusion_method == 'add':\n",
        "            # Addition pondérée\n",
        "            fused_features = features_orig + features_masked\n",
        "        elif self.fusion_method == 'attention':\n",
        "            # Mécanisme d'attention\n",
        "            combined = torch.cat([features_orig, features_masked], dim=1)\n",
        "            attention_weights = self.attention(combined)  # [batch, 2]\n",
        "\n",
        "            # Application des poids d'attention\n",
        "            weighted_orig = features_orig * attention_weights[:, 0:1]\n",
        "            weighted_masked = features_masked * attention_weights[:, 1:2]\n",
        "            fused_features = weighted_orig + weighted_masked\n",
        "\n",
        "        return fused_features\n",
        "\n",
        "    def _extract_features(self, x):\n",
        "        \"\"\"Extraction des features VGG FC-7\"\"\"\n",
        "        x = self.features(x)\n",
        "        x = self.avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc7(x)\n",
        "        return x\n",
        "\n",
        "class DualSpatialRelationMLP(nn.Module):\n",
        "    \"\"\"\n",
        "    MLP adapté pour les features duales\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_dim, hidden1_dim=512, hidden2_dim=256,\n",
        "                 num_relations=len(SPATIAL_RELATIONS), dropout_rate=0.4):\n",
        "        super(DualSpatialRelationMLP, self).__init__()\n",
        "\n",
        "        print(f\"Dual MLP Architecture:\")\n",
        "        print(f\"  - Input: {input_dim} (features fusionnées)\")\n",
        "        print(f\"  - Hidden 1: {hidden1_dim}\")\n",
        "        print(f\"  - Hidden 2: {hidden2_dim}\")\n",
        "        print(f\"  - Output: {num_relations}\")\n",
        "        print(f\"  - Dropout: {dropout_rate}\")\n",
        "\n",
        "        # Couches MLP\n",
        "        self.fc1 = nn.Linear(input_dim, hidden1_dim)\n",
        "        self.bn1 = nn.BatchNorm1d(hidden1_dim)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.dropout1 = nn.Dropout(dropout_rate)\n",
        "\n",
        "        self.fc2 = nn.Linear(hidden1_dim, hidden2_dim)\n",
        "        self.bn2 = nn.BatchNorm1d(hidden2_dim)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.dropout2 = nn.Dropout(dropout_rate)\n",
        "\n",
        "        self.fc3 = nn.Linear(hidden2_dim, num_relations)\n",
        "\n",
        "        # Initialisation\n",
        "        nn.init.xavier_uniform_(self.fc1.weight)\n",
        "        nn.init.xavier_uniform_(self.fc2.weight)\n",
        "        nn.init.xavier_uniform_(self.fc3.weight)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu1(x)\n",
        "        x = self.dropout1(x)\n",
        "\n",
        "        x = self.fc2(x)\n",
        "        x = self.bn2(x)\n",
        "        x = self.relu2(x)\n",
        "        x = self.dropout2(x)\n",
        "\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "class DualInputSpatialRelationModel(nn.Module):\n",
        "    \"\"\"\n",
        "    Modèle complet avec architecture duale\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_relations=len(SPATIAL_RELATIONS), fusion_method='concat'):\n",
        "        super(DualInputSpatialRelationModel, self).__init__()\n",
        "\n",
        "        print(f\"Modèle Dual Input Spatial Relation:\")\n",
        "        print(f\"  - Relations: {num_relations}\")\n",
        "        print(f\"  - Fusion: {fusion_method}\")\n",
        "\n",
        "        # Extracteur dual\n",
        "        self.feature_extractor = DualVGGFeatureExtractor(fusion_method=fusion_method)\n",
        "\n",
        "        # Classifieur MLP\n",
        "        self.classifier = DualSpatialRelationMLP(\n",
        "            input_dim=self.feature_extractor.fusion_dim,\n",
        "            num_relations=num_relations\n",
        "        )\n",
        "\n",
        "    def forward(self, original_img, masked_img):\n",
        "        # Extraction des features duales\n",
        "        fused_features = self.feature_extractor(original_img, masked_img)\n",
        "\n",
        "        # Classification\n",
        "        output = self.classifier(fused_features)\n",
        "        return output\n",
        "\n",
        "# =============================================================================\n",
        "# TRANSFORMATIONS\n",
        "# =============================================================================\n",
        "\n",
        "def create_dual_transforms():\n",
        "    \"\"\"Transformations pour les images duales\"\"\"\n",
        "    normalize = transforms.Normalize(\n",
        "        mean=[0.485, 0.456, 0.406],\n",
        "        std=[0.229, 0.224, 0.225]\n",
        "    )\n",
        "\n",
        "    train_transform = transforms.Compose([\n",
        "        transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "        transforms.RandomHorizontalFlip(p=0.5),\n",
        "        transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.05),\n",
        "        transforms.ToTensor(),\n",
        "        normalize\n",
        "    ])\n",
        "\n",
        "    val_transform = transforms.Compose([\n",
        "        transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "        transforms.ToTensor(),\n",
        "        normalize\n",
        "    ])\n",
        "\n",
        "    return train_transform, val_transform\n",
        "\n",
        "# =============================================================================\n",
        "# FONCTIONS D'ENTRAÎNEMENT DUALES\n",
        "# =============================================================================\n",
        "\n",
        "def train_dual_epoch(model, dataloader, criterion, optimizer, device):\n",
        "    \"\"\"Entraîne une époque avec architecture duale\"\"\"\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    progress_bar = tqdm(dataloader, desc='Training Dual')\n",
        "    for batch_idx, (original_imgs, masked_imgs, labels, _) in enumerate(progress_bar):\n",
        "        original_imgs = original_imgs.to(device)\n",
        "        masked_imgs = masked_imgs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(original_imgs, masked_imgs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "\n",
        "        # Gradient clipping\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "        progress_bar.set_postfix({\n",
        "            'loss': running_loss / (batch_idx + 1),\n",
        "            'acc': 100. * correct / total\n",
        "        })\n",
        "\n",
        "    return running_loss / len(dataloader), 100. * correct / total\n",
        "\n",
        "def evaluate_dual(model, dataloader, criterion, device):\n",
        "    \"\"\"Évalue le modèle dual\"\"\"\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for original_imgs, masked_imgs, labels, _ in tqdm(dataloader, desc='Evaluating Dual'):\n",
        "            original_imgs = original_imgs.to(device)\n",
        "            masked_imgs = masked_imgs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(original_imgs, masked_imgs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    return running_loss / len(dataloader), 100. * correct / total\n",
        "\n",
        "# =============================================================================\n",
        "# VISUALISATION DUAL INPUT\n",
        "# =============================================================================\n",
        "\n",
        "def visualize_dual_input_samples(dataset, num_samples=4):\n",
        "    \"\"\"Visualise des échantillons avec les deux types d'images\"\"\"\n",
        "    fig, axes = plt.subplots(num_samples, 3, figsize=(15, 4*num_samples))\n",
        "    if num_samples == 1:\n",
        "        axes = axes.reshape(1, -1)\n",
        "\n",
        "    for i in range(min(num_samples, len(dataset))):\n",
        "        original_img, masked_img, label, metadata = dataset[i]\n",
        "\n",
        "        # Dénormaliser pour affichage\n",
        "        def denormalize(tensor):\n",
        "            mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
        "            std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n",
        "            return (tensor * std + mean).clamp(0, 1).permute(1, 2, 0)\n",
        "\n",
        "        original_display = denormalize(original_img)\n",
        "        masked_display = denormalize(masked_img)\n",
        "\n",
        "        # Image originale\n",
        "        axes[i, 0].imshow(original_display)\n",
        "        axes[i, 0].set_title(f\"Original\\n{metadata['subject']} - {metadata['object']}\")\n",
        "        axes[i, 0].axis('off')\n",
        "\n",
        "        # Image masquée\n",
        "        axes[i, 1].imshow(masked_display)\n",
        "        axes[i, 1].set_title(f\"Masquée (BBox seulement)\\nRelation: {metadata['relation']}\")\n",
        "        axes[i, 1].axis('off')\n",
        "\n",
        "        # Informations\n",
        "        axes[i, 2].axis('off')\n",
        "        info_text = f\"\"\"\n",
        "        Échantillon {i+1}:\n",
        "\n",
        "        Sujet: {metadata['subject']}\n",
        "        Objet: {metadata['object']}\n",
        "        Relation: {metadata['relation']}\n",
        "        Original: {metadata['original_relation']}\n",
        "\n",
        "        Subject BBox: {metadata['subject_bbox']}\n",
        "        Object BBox: {metadata['object_bbox']}\n",
        "\n",
        "        Architecture:\n",
        "        • Image complète\n",
        "        • Image masquée (objets seulement)\n",
        "        • Fusion des features VGG\n",
        "        \"\"\"\n",
        "        axes[i, 2].text(0.1, 0.5, info_text, transform=axes[i, 2].transAxes,\n",
        "                       fontsize=10, verticalalignment='center',\n",
        "                       bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.8))\n",
        "\n",
        "    plt.suptitle('Architecture Duale: Image Originale + Image Masquée (BBox)',\n",
        "                 fontsize=16, fontweight='bold')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# =============================================================================\n",
        "# ENTRAÎNEMENT DUAL K-FOLD\n",
        "# =============================================================================\n",
        "\n",
        "def train_dual_kfold(data_dir, k_folds=5, epochs=15, fusion_method='concat'):\n",
        "    \"\"\"Entraînement K-fold avec architecture duale\"\"\"\n",
        "\n",
        "    print(\"=\"*70)\n",
        "    print(\"ENTRAÎNEMENT DUAL INPUT SPATIALSENSE+\")\n",
        "    print(\"=\"*70)\n",
        "    print(f\"Architecture: VGG Dual + MLP\")\n",
        "    print(f\"Fusion: {fusion_method}\")\n",
        "    print(f\"Entrée 1: Image originale\")\n",
        "    print(f\"Entrée 2: Image masquée (bounding boxes)\")\n",
        "\n",
        "    # Transformations\n",
        "    train_transform, val_transform = create_dual_transforms()\n",
        "\n",
        "    # Dataset dual\n",
        "    full_dataset = DualInputSpatialSenseDataset(\n",
        "        data_dir=data_dir,\n",
        "        split='train',\n",
        "        transform=train_transform,\n",
        "        mask_background_color=(128, 128, 128)  # Gris\n",
        "    )\n",
        "\n",
        "    if len(full_dataset) == 0:\n",
        "        print(\"Dataset vide!\")\n",
        "        return []\n",
        "\n",
        "    # Visualisation d'échantillons\n",
        "    print(\"\\nVisu échantillons dual input:\")\n",
        "    visualize_dual_input_samples(full_dataset, num_samples=3)\n",
        "\n",
        "    # K-fold\n",
        "    kfold = KFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
        "    fold_results = []\n",
        "\n",
        "    for fold, (train_indices, val_indices) in enumerate(kfold.split(full_dataset)):\n",
        "        print(f\"\\n{'='*50}\")\n",
        "        print(f\"FOLD {fold + 1}/{k_folds} - DUAL INPUT\")\n",
        "        print(f\"{'='*50}\")\n",
        "\n",
        "        # Sous-ensembles\n",
        "        train_subset = torch.utils.data.Subset(full_dataset, train_indices)\n",
        "        val_subset = torch.utils.data.Subset(full_dataset, val_indices)\n",
        "\n",
        "        # DataLoaders\n",
        "        train_loader = DataLoader(\n",
        "            train_subset, batch_size=BATCH_SIZE, shuffle=True,\n",
        "            num_workers=2, pin_memory=True\n",
        "        )\n",
        "\n",
        "        val_loader = DataLoader(\n",
        "            val_subset, batch_size=BATCH_SIZE, shuffle=False,\n",
        "            num_workers=2, pin_memory=True\n",
        "        )\n",
        "\n",
        "        # Modèle dual\n",
        "        model = DualInputSpatialRelationModel(\n",
        "            num_relations=len(SPATIAL_RELATIONS),\n",
        "            fusion_method=fusion_method\n",
        "        )\n",
        "        model = model.to(DEVICE)\n",
        "\n",
        "        # Optimiseur (seulement MLP entraînable)\n",
        "        optimizer = optim.AdamW(model.classifier.parameters(),\n",
        "                               lr=LEARNING_RATE, weight_decay=0.01)\n",
        "        criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
        "\n",
        "        # Scheduler\n",
        "        scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
        "\n",
        "        # Entraînement\n",
        "        train_losses, val_losses = [], []\n",
        "        train_accs, val_accs = [], []\n",
        "        best_val_acc = 0.0\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            print(f\"\\nEpoch {epoch+1}/{epochs}\")\n",
        "\n",
        "            # Training\n",
        "            train_loss, train_acc = train_dual_epoch(\n",
        "                model, train_loader, criterion, optimizer, DEVICE\n",
        "            )\n",
        "            train_losses.append(train_loss)\n",
        "            train_accs.append(train_acc)\n",
        "\n",
        "            # Validation\n",
        "            val_loss, val_acc = evaluate_dual(model, val_loader, criterion, DEVICE)\n",
        "            val_losses.append(val_loss)\n",
        "            val_accs.append(val_acc)\n",
        "\n",
        "            scheduler.step()\n",
        "\n",
        "            print(f\"Train: {train_loss:.4f} / {train_acc:.2f}%\")\n",
        "            print(f\"Val: {val_loss:.4f} / {val_acc:.2f}%\")\n",
        "\n",
        "            if val_acc > best_val_acc:\n",
        "                best_val_acc = val_acc\n",
        "                torch.save(model.state_dict(),\n",
        "                          f'best_dual_model_fold_{fold+1}_{fusion_method}.pth')\n",
        "\n",
        "        # Résultats fold\n",
        "        fold_results.append({\n",
        "            'fold': fold + 1,\n",
        "            'train_losses': train_losses,\n",
        "            'val_losses': val_losses,\n",
        "            'train_accs': train_accs,\n",
        "            'val_accs': val_accs,\n",
        "            'best_val_acc': best_val_acc,\n",
        "            'model': model\n",
        "        })\n",
        "\n",
        "        print(f\"Fold {fold + 1} - Meilleur: {best_val_acc:.2f}%\")\n",
        "\n",
        "    return fold_results\n",
        "\n",
        "# =============================================================================\n",
        "# FONCTION PRINCIPALE DUAL\n",
        "# =============================================================================\n",
        "\n",
        "def main_dual_experiment():\n",
        "    \"\"\"Expérience principale avec architecture duale\"\"\"\n",
        "\n",
        "    DATA_DIR = \"data/spatialsense\"\n",
        "\n",
        "    print(\"=\"*80)\n",
        "    print(\"ARCHITECTURE DUALE - HALDEKAR + BOUNDING BOX MASKING\")\n",
        "    print(\"=\"*80)\n",
        "    print(\"Innovation:\")\n",
        "    print(\"     Entrée 1: Image complète (comme Haldekar)\")\n",
        "    print(\"     Entrée 2: Image masquée (seules les bounding boxes)\")\n",
        "    print(\"     Fusion des features VGG\")\n",
        "    print(\"     Focus sur les objets pertinents\")\n",
        "\n",
        "    if not os.path.exists(DATA_DIR):\n",
        "        print(f\"\\nERREUR: {DATA_DIR} n'existe pas!\")\n",
        "        return\n",
        "\n",
        "    # Test différentes méthodes de fusion\n",
        "    fusion_methods = ['concat', 'add', 'attention']\n",
        "\n",
        "    for fusion_method in fusion_methods:\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"TEST FUSION: {fusion_method.upper()}\")\n",
        "        print(f\"{'='*60}\")\n",
        "\n",
        "        results = train_dual_kfold(\n",
        "            data_dir=DATA_DIR,\n",
        "            k_folds=3,  # Réduit pour tester rapidement\n",
        "            epochs=10,\n",
        "            fusion_method=fusion_method\n",
        "        )\n",
        "\n",
        "        if results:\n",
        "            mean_acc = np.mean([r['best_val_acc'] for r in results])\n",
        "            print(f\"\\n  {fusion_method}: {mean_acc:.2f}% (moyenne)\")\n",
        "\n",
        "    print(\"\\n   Expérience architecture duale terminée!\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main_dual_experiment()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "8uYQczHQrDnd",
        "outputId": "f6eb318e-c1a5-4caf-b553-c1b51097593f"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image, ImageDraw\n",
        "import numpy as np\n",
        "import json\n",
        "import os\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tqdm import tqdm\n",
        "import cv2\n",
        "from collections import Counter\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# =============================================================================\n",
        "# CONFIGURATION GLOBALE\n",
        "# =============================================================================\n",
        "\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Device utilisé: {DEVICE}\")\n",
        "\n",
        "# Hyperparamètres\n",
        "BATCH_SIZE = 8  # Réduit car deux images en entrée\n",
        "LEARNING_RATE = 0.001\n",
        "EPOCHS = 15\n",
        "K_FOLDS = 5\n",
        "IMG_SIZE = 224\n",
        "DROPOUT_RATE = 0.4\n",
        "\n",
        "# Relations spatiales SpatialSense+\n",
        "SPATIAL_RELATIONS = [\n",
        "    'above', 'behind', 'in', 'in front of', 'next to',\n",
        "    'on', 'to the left of', 'to the right of', 'under'\n",
        "]\n",
        "\n",
        "print(f\"Architecture DUALE avec masque binaire bounding box (CONCAT SEULEMENT):\")\n",
        "print(f\"  - Relations: {len(SPATIAL_RELATIONS)}\")\n",
        "print(f\"  - Entrée 1: Image originale\")\n",
        "print(f\"  - Entrée 2: Masque binaire des bounding boxes\")\n",
        "print(f\"  - Fusion: Concaténation uniquement\")\n",
        "\n",
        "# =============================================================================\n",
        "# DATASET DUAL INPUT AVEC MASQUE BINAIRE\n",
        "# =============================================================================\n",
        "\n",
        "class DualInputBinaryMaskDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Dataset SpatialSense+ avec architecture duale:\n",
        "    - Image originale\n",
        "    - Masque binaire des bounding boxes (blanc=objet, noir=fond)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, data_dir, split='train', transform=None):\n",
        "        self.data_dir = data_dir\n",
        "        self.split = split\n",
        "        self.transform = transform\n",
        "\n",
        "        self.relation_to_idx = {rel: idx for idx, rel in enumerate(SPATIAL_RELATIONS)}\n",
        "        self.idx_to_relation = {idx: rel for rel, idx in self.relation_to_idx.items()}\n",
        "\n",
        "        self.annotations = self._load_annotations()\n",
        "        self.data_samples = self._prepare_samples()\n",
        "\n",
        "        print(f\"Dataset DUAL Binary Mask SpatialSense+ {split}: {len(self.data_samples)} échantillons\")\n",
        "        if len(self.data_samples) > 0:\n",
        "            self._print_statistics()\n",
        "\n",
        "    def _load_annotations(self):\n",
        "        annotations_path = os.path.join(self.data_dir, 'annotations.json')\n",
        "        try:\n",
        "            with open(annotations_path, 'r') as f:\n",
        "                return json.load(f)\n",
        "        except FileNotFoundError:\n",
        "            print(f\"Erreur: {annotations_path} non trouvé!\")\n",
        "            return []\n",
        "\n",
        "    def _find_image_path(self, image_url):\n",
        "        base_dir = os.path.join(self.data_dir, \"images\", \"images\")\n",
        "        filename = os.path.basename(image_url)\n",
        "\n",
        "        if \"staticflickr\" in image_url or len(filename.split('_')) == 2:\n",
        "            return os.path.join(base_dir, \"flickr\", filename)\n",
        "        else:\n",
        "            return os.path.join(base_dir, \"nyu\", filename)\n",
        "\n",
        "    def _prepare_samples(self):\n",
        "        samples = []\n",
        "        images_not_found = 0\n",
        "\n",
        "        for img_data in self.annotations:\n",
        "            if img_data['split'] != self.split:\n",
        "                continue\n",
        "\n",
        "            img_path = self._find_image_path(img_data['url'])\n",
        "\n",
        "            if not os.path.exists(img_path):\n",
        "                images_not_found += 1\n",
        "                continue\n",
        "\n",
        "            for ann in img_data['annotations']:\n",
        "                if ann['label'] and ann['predicate'].lower().strip() in [rel.lower() for rel in SPATIAL_RELATIONS]:\n",
        "                    # Trouver la relation correspondante\n",
        "                    relation = None\n",
        "                    for rel in SPATIAL_RELATIONS:\n",
        "                        if rel.lower() == ann['predicate'].lower().strip():\n",
        "                            relation = rel\n",
        "                            break\n",
        "\n",
        "                    if relation and 'bbox' in ann['subject'] and 'bbox' in ann['object']:\n",
        "                        sample = {\n",
        "                            'image_path': img_path,\n",
        "                            'subject': ann['subject']['name'],\n",
        "                            'object': ann['object']['name'],\n",
        "                            'relation': relation,\n",
        "                            'original_relation': ann['predicate'],\n",
        "                            'subject_bbox': ann['subject']['bbox'],  # [y1, y2, x1, x2]\n",
        "                            'object_bbox': ann['object']['bbox'],   # [y1, y2, x1, x2]\n",
        "                            'image_width': img_data['width'],\n",
        "                            'image_height': img_data['height']\n",
        "                        }\n",
        "                        samples.append(sample)\n",
        "\n",
        "        if images_not_found > 0:\n",
        "            print(f\"Images non trouvées: {images_not_found}\")\n",
        "\n",
        "        return samples\n",
        "\n",
        "    def _create_binary_mask(self, image_size, subject_bbox, object_bbox):\n",
        "        \"\"\"\n",
        "        Crée un masque binaire où les bounding boxes sont en blanc (255) et le fond en noir (0)\n",
        "        bbox format: [y1, y2, x1, x2]\n",
        "        \"\"\"\n",
        "        # Créer un masque noir\n",
        "        mask = np.zeros((image_size[1], image_size[0]), dtype=np.uint8)  # (height, width)\n",
        "\n",
        "        # Fonction pour appliquer une bounding box\n",
        "        def apply_bbox(bbox):\n",
        "            try:\n",
        "                y1, y2, x1, x2 = bbox\n",
        "                # Clamp les coordonnées pour éviter les débordements\n",
        "                y1 = max(0, min(int(y1), mask.shape[0]))\n",
        "                y2 = max(0, min(int(y2), mask.shape[0]))\n",
        "                x1 = max(0, min(int(x1), mask.shape[1]))\n",
        "                x2 = max(0, min(int(x2), mask.shape[1]))\n",
        "\n",
        "                # Remplir la bounding box en blanc (255)\n",
        "                if y2 > y1 and x2 > x1:\n",
        "                    mask[y1:y2, x1:x2] = 255\n",
        "            except Exception as e:\n",
        "                print(f\"Erreur application bbox {bbox}: {e}\")\n",
        "\n",
        "        # Appliquer les deux bounding boxes\n",
        "        apply_bbox(subject_bbox)\n",
        "        apply_bbox(object_bbox)\n",
        "\n",
        "        # Convertir en PIL Image et dupliquer pour avoir 3 canaux (RGB)\n",
        "        mask_pil = Image.fromarray(mask, mode='L')\n",
        "        mask_rgb = Image.merge('RGB', (mask_pil, mask_pil, mask_pil))\n",
        "\n",
        "        return mask_rgb\n",
        "\n",
        "    def _print_statistics(self):\n",
        "        relation_counts = Counter([s['relation'] for s in self.data_samples])\n",
        "        print(f\"\\nDistribution DUAL Binary Mask dans {self.split}:\")\n",
        "        total = len(self.data_samples)\n",
        "\n",
        "        for relation in SPATIAL_RELATIONS:\n",
        "            count = relation_counts.get(relation, 0)\n",
        "            percentage = count / total * 100 if total > 0 else 0\n",
        "            print(f\"   {relation}: {count} ({percentage:.1f}%)\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data_samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sample = self.data_samples[idx]\n",
        "\n",
        "        try:\n",
        "            # Charger l'image originale\n",
        "            original_image = Image.open(sample['image_path']).convert('RGB')\n",
        "\n",
        "            # Créer le masque binaire des bounding boxes\n",
        "            binary_mask = self._create_binary_mask(\n",
        "                original_image.size,  # (width, height)\n",
        "                sample['subject_bbox'],\n",
        "                sample['object_bbox']\n",
        "            )\n",
        "\n",
        "            # Appliquer les transformations\n",
        "            if self.transform:\n",
        "                original_image = self.transform(original_image)\n",
        "                binary_mask = self.transform(binary_mask)\n",
        "\n",
        "            label = self.relation_to_idx[sample['relation']]\n",
        "\n",
        "            metadata = {\n",
        "                'subject': sample['subject'],\n",
        "                'object': sample['object'],\n",
        "                'relation': sample['relation'],\n",
        "                'original_relation': sample['original_relation'],\n",
        "                'subject_bbox': sample['subject_bbox'],\n",
        "                'object_bbox': sample['object_bbox']\n",
        "            }\n",
        "\n",
        "            return original_image, binary_mask, label, metadata\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Erreur chargement {sample['image_path']}: {e}\")\n",
        "            # Images par défaut en cas d'erreur\n",
        "            dummy_original = Image.new('RGB', (224, 224), color='gray')\n",
        "            dummy_mask = Image.new('RGB', (224, 224), color='black')\n",
        "\n",
        "            if self.transform:\n",
        "                dummy_original = self.transform(dummy_original)\n",
        "                dummy_mask = self.transform(dummy_mask)\n",
        "            else:\n",
        "                dummy_original = torch.zeros(3, 224, 224)\n",
        "                dummy_mask = torch.zeros(3, 224, 224)\n",
        "\n",
        "            return dummy_original, dummy_mask, 0, {\n",
        "                'subject': 'error', 'object': 'error',\n",
        "                'relation': 'next to', 'original_relation': 'error',\n",
        "                'subject_bbox': [0, 0, 0, 0], 'object_bbox': [0, 0, 0, 0]\n",
        "            }\n",
        "\n",
        "# =============================================================================\n",
        "# ARCHITECTURE DUALE VGG (CONCAT SEULEMENT)\n",
        "# =============================================================================\n",
        "\n",
        "class DualVGGFeatureExtractor(nn.Module):\n",
        "    \"\"\"\n",
        "    Extracteur de features VGG dual avec concaténation uniquement:\n",
        "    - Branche 1: Image originale\n",
        "    - Branche 2: Masque binaire des bounding boxes\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super(DualVGGFeatureExtractor, self).__init__()\n",
        "\n",
        "        # VGG16 pré-entraîné partagé pour les deux branches\n",
        "        vgg16 = models.vgg16(pretrained=True)\n",
        "\n",
        "        # Extraction des couches\n",
        "        self.features = vgg16.features\n",
        "        self.avgpool = vgg16.avgpool\n",
        "\n",
        "        # FC-7 partagé\n",
        "        classifier_layers = list(vgg16.classifier.children())[:6]\n",
        "        self.fc7 = nn.Sequential(*classifier_layers)\n",
        "\n",
        "        # Gel des poids VGG\n",
        "        for param in self.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        # Dimension de sortie après concaténation\n",
        "        self.fusion_dim = 4096 * 2  # Concaténation des deux branches\n",
        "\n",
        "        print(f\"Dual VGG Feature Extractor (CONCAT uniquement):\")\n",
        "        print(f\"  - Dimension sortie: {self.fusion_dim}\")\n",
        "        print(f\"  - Branche 1: Image originale\")\n",
        "        print(f\"  - Branche 2: Masque binaire (bounding boxes)\")\n",
        "\n",
        "    def forward(self, original_img, binary_mask):\n",
        "        # Extraction features pour image originale\n",
        "        features_orig = self._extract_features(original_img)\n",
        "\n",
        "        # Extraction features pour masque binaire\n",
        "        features_mask = self._extract_features(binary_mask)\n",
        "\n",
        "        # Concaténation simple\n",
        "        fused_features = torch.cat([features_orig, features_mask], dim=1)\n",
        "\n",
        "        return fused_features\n",
        "\n",
        "    def _extract_features(self, x):\n",
        "        \"\"\"Extraction des features VGG FC-7\"\"\"\n",
        "        x = self.features(x)\n",
        "        x = self.avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc7(x)\n",
        "        return x\n",
        "\n",
        "class DualSpatialRelationMLP(nn.Module):\n",
        "    \"\"\"\n",
        "    MLP adapté pour les features duales concaténées\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_dim, hidden1_dim=512, hidden2_dim=256,\n",
        "                 num_relations=len(SPATIAL_RELATIONS), dropout_rate=0.4):\n",
        "        super(DualSpatialRelationMLP, self).__init__()\n",
        "\n",
        "        print(f\"Dual MLP Architecture:\")\n",
        "        print(f\"  - Input: {input_dim} (features concaténées)\")\n",
        "        print(f\"  - Hidden 1: {hidden1_dim}\")\n",
        "        print(f\"  - Hidden 2: {hidden2_dim}\")\n",
        "        print(f\"  - Output: {num_relations}\")\n",
        "        print(f\"  - Dropout: {dropout_rate}\")\n",
        "\n",
        "        # Couches MLP\n",
        "        self.fc1 = nn.Linear(input_dim, hidden1_dim)\n",
        "        self.bn1 = nn.BatchNorm1d(hidden1_dim)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.dropout1 = nn.Dropout(dropout_rate)\n",
        "\n",
        "        self.fc2 = nn.Linear(hidden1_dim, hidden2_dim)\n",
        "        self.bn2 = nn.BatchNorm1d(hidden2_dim)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.dropout2 = nn.Dropout(dropout_rate)\n",
        "\n",
        "        self.fc3 = nn.Linear(hidden2_dim, num_relations)\n",
        "\n",
        "        # Initialisation\n",
        "        nn.init.xavier_uniform_(self.fc1.weight)\n",
        "        nn.init.xavier_uniform_(self.fc2.weight)\n",
        "        nn.init.xavier_uniform_(self.fc3.weight)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu1(x)\n",
        "        x = self.dropout1(x)\n",
        "\n",
        "        x = self.fc2(x)\n",
        "        x = self.bn2(x)\n",
        "        x = self.relu2(x)\n",
        "        x = self.dropout2(x)\n",
        "\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "class DualInputSpatialRelationModel(nn.Module):\n",
        "    \"\"\"\n",
        "    Modèle complet avec architecture duale (concaténation)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_relations=len(SPATIAL_RELATIONS)):\n",
        "        super(DualInputSpatialRelationModel, self).__init__()\n",
        "\n",
        "        print(f\"Modèle Dual Input Spatial Relation (CONCAT):\")\n",
        "        print(f\"  - Relations: {num_relations}\")\n",
        "        print(f\"  - Fusion: Concaténation\")\n",
        "\n",
        "        # Extracteur dual\n",
        "        self.feature_extractor = DualVGGFeatureExtractor()\n",
        "\n",
        "        # Classifieur MLP\n",
        "        self.classifier = DualSpatialRelationMLP(\n",
        "            input_dim=self.feature_extractor.fusion_dim,\n",
        "            num_relations=num_relations\n",
        "        )\n",
        "\n",
        "    def forward(self, original_img, binary_mask):\n",
        "        # Extraction des features duales\n",
        "        fused_features = self.feature_extractor(original_img, binary_mask)\n",
        "\n",
        "        # Classification\n",
        "        output = self.classifier(fused_features)\n",
        "        return output\n",
        "\n",
        "# =============================================================================\n",
        "# FONCTIONS D'ÉVALUATION AVEC MATRICE DE CONFUSION\n",
        "# =============================================================================\n",
        "\n",
        "def evaluate_dual_with_confusion_matrix(model, dataloader, criterion, device, class_names=None):\n",
        "    \"\"\"Évalue le modèle dual et retourne les métriques + matrice de confusion\"\"\"\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    all_predictions = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for original_imgs, binary_masks, labels, _ in tqdm(dataloader, desc='Evaluating Dual'):\n",
        "            original_imgs = original_imgs.to(device)\n",
        "            binary_masks = binary_masks.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(original_imgs, binary_masks)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "            all_predictions.extend(predicted.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    # Calculs des métriques\n",
        "    accuracy = 100. * np.mean(np.array(all_predictions) == np.array(all_labels))\n",
        "    avg_loss = running_loss / len(dataloader)\n",
        "\n",
        "    # Matrice de confusion\n",
        "    cm = confusion_matrix(all_labels, all_predictions)\n",
        "\n",
        "    # Rapport de classification\n",
        "    if class_names is None:\n",
        "        class_names = SPATIAL_RELATIONS\n",
        "\n",
        "    report = classification_report(\n",
        "        all_labels, all_predictions,\n",
        "        target_names=class_names,\n",
        "        output_dict=True,\n",
        "        zero_division=0\n",
        "    )\n",
        "\n",
        "    return avg_loss, accuracy, cm, report, all_predictions, all_labels\n",
        "\n",
        "def plot_confusion_matrix(cm, class_names, title=\"Matrice de Confusion\", normalize=False):\n",
        "    \"\"\"Affiche la matrice de confusion avec style amélioré\"\"\"\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "        fmt = '.2f'\n",
        "        title += \" (Normalisée)\"\n",
        "    else:\n",
        "        fmt = 'd'\n",
        "\n",
        "    plt.figure(figsize=(12, 10))\n",
        "    sns.heatmap(cm,\n",
        "                annot=True,\n",
        "                fmt=fmt,\n",
        "                cmap='Blues',\n",
        "                xticklabels=class_names,\n",
        "                yticklabels=class_names,\n",
        "                cbar_kws={'label': 'Proportion' if normalize else 'Nombre de prédictions'})\n",
        "\n",
        "    plt.title(title, fontsize=16, fontweight='bold', pad=20)\n",
        "    plt.xlabel('Prédictions', fontsize=14, fontweight='bold')\n",
        "    plt.ylabel('Vraies étiquettes', fontsize=14, fontweight='bold')\n",
        "    plt.xticks(rotation=45, ha='right')\n",
        "    plt.yticks(rotation=0)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def display_classification_metrics(report, title=\"Métriques de Classification\"):\n",
        "    \"\"\"Affiche les métriques de classification sous forme de tableau\"\"\"\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"{title}\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    # Métriques par classe\n",
        "    print(f\"{'Classe':<15} {'Précision':<10} {'Rappel':<10} {'F1-Score':<10} {'Support':<10}\")\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "    for class_name in SPATIAL_RELATIONS:\n",
        "        if class_name in report:\n",
        "            metrics = report[class_name]\n",
        "            print(f\"{class_name:<15} {metrics['precision']:<10.3f} {metrics['recall']:<10.3f} \"\n",
        "                  f\"{metrics['f1-score']:<10.3f} {metrics['support']:<10.0f}\")\n",
        "\n",
        "    # Métriques globales\n",
        "    print(\"-\" * 60)\n",
        "    print(f\"{'Accuracy':<15} {'':<10} {'':<10} {report['accuracy']:<10.3f} {report['macro avg']['support']:<10.0f}\")\n",
        "    print(f\"{'Macro avg':<15} {report['macro avg']['precision']:<10.3f} {report['macro avg']['recall']:<10.3f} \"\n",
        "          f\"{report['macro avg']['f1-score']:<10.3f} {report['macro avg']['support']:<10.0f}\")\n",
        "    print(f\"{'Weighted avg':<15} {report['weighted avg']['precision']:<10.3f} {report['weighted avg']['recall']:<10.3f} \"\n",
        "          f\"{report['weighted avg']['f1-score']:<10.3f} {report['weighted avg']['support']:<10.0f}\")\n",
        "\n",
        "# =============================================================================\n",
        "# VISUALISATION DUAL INPUT AVEC MASQUE BINAIRE\n",
        "# =============================================================================\n",
        "\n",
        "def visualize_dual_binary_samples(dataset, num_samples=4):\n",
        "    \"\"\"Visualise des échantillons avec image originale et masque binaire\"\"\"\n",
        "    fig, axes = plt.subplots(num_samples, 3, figsize=(15, 4*num_samples))\n",
        "    if num_samples == 1:\n",
        "        axes = axes.reshape(1, -1)\n",
        "\n",
        "    for i in range(min(num_samples, len(dataset))):\n",
        "        original_img, binary_mask, label, metadata = dataset[i]\n",
        "\n",
        "        # Dénormaliser pour affichage\n",
        "        def denormalize(tensor):\n",
        "            mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
        "            std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n",
        "            return (tensor * std + mean).clamp(0, 1).permute(1, 2, 0)\n",
        "\n",
        "        original_display = denormalize(original_img)\n",
        "        mask_display = denormalize(binary_mask)\n",
        "\n",
        "        # Image originale\n",
        "        axes[i, 0].imshow(original_display)\n",
        "        axes[i, 0].set_title(f\"Original\\n{metadata['subject']} - {metadata['object']}\")\n",
        "        axes[i, 0].axis('off')\n",
        "\n",
        "        # Masque binaire\n",
        "        axes[i, 1].imshow(mask_display, cmap='gray')\n",
        "        axes[i, 1].set_title(f\"Masque Binaire\\nRelation: {metadata['relation']}\")\n",
        "        axes[i, 1].axis('off')\n",
        "\n",
        "        # Informations\n",
        "        axes[i, 2].axis('off')\n",
        "        info_text = f\"\"\"\n",
        "        Échantillon {i+1}:\n",
        "\n",
        "        Sujet: {metadata['subject']}\n",
        "        Objet: {metadata['object']}\n",
        "        Relation: {metadata['relation']}\n",
        "        Original: {metadata['original_relation']}\n",
        "\n",
        "        Subject BBox: {metadata['subject_bbox']}\n",
        "        Object BBox: {metadata['object_bbox']}\n",
        "\n",
        "        Architecture:\n",
        "        • Image complète (RGB)\n",
        "        • Masque binaire (Blanc=objets, Noir=fond)\n",
        "        • Concaténation des features VGG\n",
        "        \"\"\"\n",
        "        axes[i, 2].text(0.1, 0.5, info_text, transform=axes[i, 2].transAxes,\n",
        "                       fontsize=10, verticalalignment='center',\n",
        "                       bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.8))\n",
        "\n",
        "    plt.suptitle('Architecture Duale: Image Originale + Masque Binaire (CONCAT)',\n",
        "                 fontsize=16, fontweight='bold')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# =============================================================================\n",
        "# TRANSFORMATIONS\n",
        "# =============================================================================\n",
        "\n",
        "def create_dual_transforms():\n",
        "    \"\"\"Transformations pour les images duales\"\"\"\n",
        "    normalize = transforms.Normalize(\n",
        "        mean=[0.485, 0.456, 0.406],\n",
        "        std=[0.229, 0.224, 0.225]\n",
        "    )\n",
        "\n",
        "    train_transform = transforms.Compose([\n",
        "        transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "        transforms.RandomHorizontalFlip(p=0.5),\n",
        "        transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.05),\n",
        "        transforms.ToTensor(),\n",
        "        normalize\n",
        "    ])\n",
        "\n",
        "    val_transform = transforms.Compose([\n",
        "        transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "        transforms.ToTensor(),\n",
        "        normalize\n",
        "    ])\n",
        "\n",
        "    return train_transform, val_transform\n",
        "\n",
        "# =============================================================================\n",
        "# FONCTIONS D'ENTRAÎNEMENT DUALES\n",
        "# =============================================================================\n",
        "\n",
        "def train_dual_epoch(model, dataloader, criterion, optimizer, device):\n",
        "    \"\"\"Entraîne une époque avec architecture duale\"\"\"\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    progress_bar = tqdm(dataloader, desc='Training Dual')\n",
        "    for batch_idx, (original_imgs, binary_masks, labels, _) in enumerate(progress_bar):\n",
        "        original_imgs = original_imgs.to(device)\n",
        "        binary_masks = binary_masks.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(original_imgs, binary_masks)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "\n",
        "        # Gradient clipping\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "        progress_bar.set_postfix({\n",
        "            'loss': running_loss / (batch_idx + 1),\n",
        "            'acc': 100. * correct / total\n",
        "        })\n",
        "\n",
        "    return running_loss / len(dataloader), 100. * correct / total\n",
        "\n",
        "# =============================================================================\n",
        "# ENTRAÎNEMENT DUAL K-FOLD AVEC MATRICE DE CONFUSION (CONCAT SEULEMENT)\n",
        "# =============================================================================\n",
        "\n",
        "def train_dual_kfold_with_confusion_matrix(data_dir, k_folds=5, epochs=15):\n",
        "    \"\"\"Entraînement K-fold avec architecture duale et concaténation uniquement\"\"\"\n",
        "\n",
        "    print(\"=\"*70)\n",
        "    print(\"ENTRAÎNEMENT DUAL INPUT AVEC MASQUE BINAIRE (CONCAT UNIQUEMENT)\")\n",
        "    print(\"=\"*70)\n",
        "    print(f\"Architecture: VGG Dual + MLP\")\n",
        "    print(f\"Fusion: Concaténation des features\")\n",
        "    print(f\"Entrée 1: Image originale\")\n",
        "    print(f\"Entrée 2: Masque binaire des bounding boxes\")\n",
        "\n",
        "    # Transformations\n",
        "    train_transform, val_transform = create_dual_transforms()\n",
        "\n",
        "    # Dataset dual avec masque binaire\n",
        "    full_dataset = DualInputBinaryMaskDataset(\n",
        "        data_dir=data_dir,\n",
        "        split='train',\n",
        "        transform=train_transform\n",
        "    )\n",
        "\n",
        "    if len(full_dataset) == 0:\n",
        "        print(\"Dataset vide!\")\n",
        "        return []\n",
        "\n",
        "    # Visualisation d'échantillons\n",
        "    print(\"\\nVisu échantillons dual input avec masque binaire:\")\n",
        "    visualize_dual_binary_samples(full_dataset, num_samples=3)\n",
        "\n",
        "    # K-fold\n",
        "    kfold = KFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
        "    fold_results = []\n",
        "\n",
        "    for fold, (train_indices, val_indices) in enumerate(kfold.split(full_dataset)):\n",
        "        print(f\"\\n{'='*50}\")\n",
        "        print(f\"FOLD {fold + 1}/{k_folds} - DUAL INPUT BINARY MASK (CONCAT)\")\n",
        "        print(f\"{'='*50}\")\n",
        "\n",
        "        # Sous-ensembles\n",
        "        train_subset = torch.utils.data.Subset(full_dataset, train_indices)\n",
        "        val_subset = torch.utils.data.Subset(full_dataset, val_indices)\n",
        "\n",
        "        # DataLoaders\n",
        "        train_loader = DataLoader(\n",
        "            train_subset, batch_size=BATCH_SIZE, shuffle=True,\n",
        "            num_workers=2, pin_memory=True\n",
        "        )\n",
        "\n",
        "        val_loader = DataLoader(\n",
        "            val_subset, batch_size=BATCH_SIZE, shuffle=False,\n",
        "            num_workers=2, pin_memory=True\n",
        "        )\n",
        "\n",
        "        # Modèle dual avec concaténation\n",
        "        model = DualInputSpatialRelationModel(num_relations=len(SPATIAL_RELATIONS))\n",
        "        model = model.to(DEVICE)\n",
        "\n",
        "        # Optimiseur (seulement MLP entraînable)\n",
        "        optimizer = optim.AdamW(model.classifier.parameters(),\n",
        "                               lr=LEARNING_RATE, weight_decay=0.01)\n",
        "        criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
        "\n",
        "        # Scheduler\n",
        "        scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
        "\n",
        "        # Entraînement\n",
        "        train_losses, val_losses = [], []\n",
        "        train_accs, val_accs = [], []\n",
        "        best_val_acc = 0.0\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            print(f\"\\nEpoch {epoch+1}/{epochs}\")\n",
        "\n",
        "            # Training\n",
        "            train_loss, train_acc = train_dual_epoch(\n",
        "                model, train_loader, criterion, optimizer, DEVICE\n",
        "            )\n",
        "            train_losses.append(train_loss)\n",
        "            train_accs.append(train_acc)\n",
        "\n",
        "            # Validation avec matrice de confusion\n",
        "            val_loss, val_acc, cm, report, predictions, true_labels = evaluate_dual_with_confusion_matrix(\n",
        "                model, val_loader, criterion, DEVICE, SPATIAL_RELATIONS\n",
        "            )\n",
        "            val_losses.append(val_loss)\n",
        "            val_accs.append(val_acc)\n",
        "\n",
        "            scheduler.step()\n",
        "\n",
        "            print(f\"Train: {train_loss:.4f} / {train_acc:.2f}%\")\n",
        "            print(f\"Val: {val_loss:.4f} / {val_acc:.2f}%\")\n",
        "\n",
        "            if val_acc > best_val_acc:\n",
        "                best_val_acc = val_acc\n",
        "                torch.save(model.state_dict(),\n",
        "                          f'best_dual_binary_mask_model_fold_{fold+1}_concat.pth')\n",
        "\n",
        "        # Évaluation finale avec matrice de confusion\n",
        "        print(f\"\\n{'='*40}\")\n",
        "        print(f\"ÉVALUATION FINALE FOLD {fold + 1}\")\n",
        "        print(f\"{'='*40}\")\n",
        "\n",
        "        final_val_loss, final_val_acc, final_cm, final_report, final_predictions, final_true_labels = evaluate_dual_with_confusion_matrix(\n",
        "            model, val_loader, criterion, DEVICE, SPATIAL_RELATIONS\n",
        "        )\n",
        "\n",
        "        # Affichage des métriques\n",
        "        display_classification_metrics(final_report, f\"Métriques Fold {fold + 1}\")\n",
        "\n",
        "        # Affichage matrice de confusion\n",
        "        plot_confusion_matrix(final_cm, SPATIAL_RELATIONS,\n",
        "                            f\"Matrice de Confusion - Fold {fold + 1}\", normalize=False)\n",
        "        plot_confusion_matrix(final_cm, SPATIAL_RELATIONS,\n",
        "                            f\"Matrice de Confusion Normalisée - Fold {fold + 1}\", normalize=True)\n",
        "\n",
        "        # Résultats fold\n",
        "        fold_results.append({\n",
        "            'fold': fold + 1,\n",
        "            'train_losses': train_losses,\n",
        "            'val_losses': val_losses,\n",
        "            'train_accs': train_accs,\n",
        "            'val_accs': val_accs,\n",
        "            'best_val_acc': best_val_acc,\n",
        "            'final_confusion_matrix': final_cm,\n",
        "            'final_report': final_report,\n",
        "            'final_predictions': final_predictions,\n",
        "            'final_true_labels': final_true_labels,\n",
        "            'model': model\n",
        "        })\n",
        "\n",
        "        print(f\"Fold {fold + 1} - Meilleur: {best_val_acc:.2f}%\")\n",
        "\n",
        "    # Analyse globale des résultats\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(\"ANALYSE GLOBALE DES RÉSULTATS\")\n",
        "    print(f\"{'='*70}\")\n",
        "\n",
        "    # Moyennes des métriques\n",
        "    mean_acc = np.mean([r['best_val_acc'] for r in fold_results])\n",
        "    std_acc = np.std([r['best_val_acc'] for r in fold_results])\n",
        "    print(f\"Accuracy moyenne: {mean_acc:.2f}% ± {std_acc:.2f}%\")\n",
        "\n",
        "    # Matrice de confusion globale\n",
        "    global_cm = np.sum([r['final_confusion_matrix'] for r in fold_results], axis=0)\n",
        "    global_predictions = np.concatenate([r['final_predictions'] for r in fold_results])\n",
        "    global_true_labels = np.concatenate([r['final_true_labels'] for r in fold_results])\n",
        "\n",
        "    # Rapport global\n",
        "    global_report = classification_report(\n",
        "        global_true_labels, global_predictions,\n",
        "        target_names=SPATIAL_RELATIONS,\n",
        "        output_dict=True,\n",
        "        zero_division=0\n",
        "    )\n",
        "\n",
        "    # Affichage final\n",
        "    display_classification_metrics(global_report, \"Métriques Globales (tous les folds)\")\n",
        "    plot_confusion_matrix(global_cm, SPATIAL_RELATIONS,\n",
        "                        \"Matrice de Confusion Globale\", normalize=False)\n",
        "    plot_confusion_matrix(global_cm, SPATIAL_RELATIONS,\n",
        "                        \"Matrice de Confusion Globale Normalisée\", normalize=True)\n",
        "\n",
        "    return fold_results\n",
        "\n",
        "# =============================================================================\n",
        "# ANALYSE DES ERREURS\n",
        "# =============================================================================\n",
        "\n",
        "def analyze_prediction_errors(fold_results, top_k=5):\n",
        "    \"\"\"Analyse des erreurs de prédiction les plus fréquentes\"\"\"\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(\"ANALYSE DES ERREURS DE PRÉDICTION\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    # Combiner toutes les prédictions\n",
        "    all_predictions = np.concatenate([r['final_predictions'] for r in fold_results])\n",
        "    all_true_labels = np.concatenate([r['final_true_labels'] for r in fold_results])\n",
        "\n",
        "    # Identifier les erreurs\n",
        "    errors = []\n",
        "    for true_idx, pred_idx in zip(all_true_labels, all_predictions):\n",
        "        if true_idx != pred_idx:\n",
        "            true_relation = SPATIAL_RELATIONS[true_idx]\n",
        "            pred_relation = SPATIAL_RELATIONS[pred_idx]\n",
        "            errors.append((true_relation, pred_relation))\n",
        "\n",
        "    # Compter les erreurs les plus fréquentes\n",
        "    from collections import Counter\n",
        "    error_counts = Counter(errors)\n",
        "\n",
        "    print(f\"Nombre total d'erreurs: {len(errors)}\")\n",
        "    print(f\"Accuracy globale: {100 * (1 - len(errors) / len(all_predictions)):.2f}%\")\n",
        "    print(f\"\\nTop {top_k} erreurs les plus fréquentes:\")\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "    for i, ((true_rel, pred_rel), count) in enumerate(error_counts.most_common(top_k)):\n",
        "        percentage = 100 * count / len(errors)\n",
        "        print(f\"{i+1:2d}. {true_rel:>15} → {pred_rel:<15} : {count:3d} ({percentage:5.1f}%)\")\n",
        "\n",
        "def plot_learning_curves(fold_results):\n",
        "    \"\"\"Affiche les courbes d'apprentissage pour tous les folds\"\"\"\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "\n",
        "    # Loss curves\n",
        "    axes[0, 0].set_title('Courbes de Loss - Training', fontweight='bold')\n",
        "    axes[0, 1].set_title('Courbes de Loss - Validation', fontweight='bold')\n",
        "    axes[1, 0].set_title('Courbes d\\'Accuracy - Training', fontweight='bold')\n",
        "    axes[1, 1].set_title('Courbes d\\'Accuracy - Validation', fontweight='bold')\n",
        "\n",
        "    colors = ['blue', 'red', 'green', 'orange', 'purple']\n",
        "\n",
        "    for i, result in enumerate(fold_results):\n",
        "        epochs = range(1, len(result['train_losses']) + 1)\n",
        "        color = colors[i % len(colors)]\n",
        "\n",
        "        # Training loss\n",
        "        axes[0, 0].plot(epochs, result['train_losses'],\n",
        "                       color=color, label=f'Fold {result[\"fold\"]}', alpha=0.7)\n",
        "\n",
        "        # Validation loss\n",
        "        axes[0, 1].plot(epochs, result['val_losses'],\n",
        "                       color=color, label=f'Fold {result[\"fold\"]}', alpha=0.7)\n",
        "\n",
        "        # Training accuracy\n",
        "        axes[1, 0].plot(epochs, result['train_accs'],\n",
        "                       color=color, label=f'Fold {result[\"fold\"]}', alpha=0.7)\n",
        "\n",
        "        # Validation accuracy\n",
        "        axes[1, 1].plot(epochs, result['val_accs'],\n",
        "                       color=color, label=f'Fold {result[\"fold\"]}', alpha=0.7)\n",
        "\n",
        "    # Configuration des axes\n",
        "    for ax in axes.flat:\n",
        "        ax.set_xlabel('Époque')\n",
        "        ax.legend()\n",
        "        ax.grid(True, alpha=0.3)\n",
        "\n",
        "    axes[0, 0].set_ylabel('Loss')\n",
        "    axes[0, 1].set_ylabel('Loss')\n",
        "    axes[1, 0].set_ylabel('Accuracy (%)')\n",
        "    axes[1, 1].set_ylabel('Accuracy (%)')\n",
        "\n",
        "    plt.suptitle('Courbes d\\'Apprentissage - Architecture Duale avec Concaténation',\n",
        "                 fontsize=16, fontweight='bold')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# =============================================================================\n",
        "# FONCTION PRINCIPALE DUAL AVEC CONCAT SEULEMENT\n",
        "# =============================================================================\n",
        "\n",
        "def main_dual_binary_mask_concat_only():\n",
        "    \"\"\"Expérience principale avec architecture duale et concaténation uniquement\"\"\"\n",
        "\n",
        "    DATA_DIR = \"data/spatialsense\"\n",
        "\n",
        "    print(\"=\"*80)\n",
        "    print(\"ARCHITECTURE DUALE - MASQUE BINAIRE (CONCATÉNATION UNIQUEMENT)\")\n",
        "    print(\"=\"*80)\n",
        "    print(\"Configuration:\")\n",
        "    print(\"     Entrée 1: Image complète (RGB)\")\n",
        "    print(\"     Entrée 2: Masque binaire des bounding boxes (Blanc=objets, Noir=fond)\")\n",
        "    print(\"     Fusion: Concaténation des features VGG (4096 + 4096 = 8192)\")\n",
        "    print(\"     Matrice de confusion détaillée\")\n",
        "    print(\"     Analyse des erreurs de prédiction\")\n",
        "    print(\"     Courbes d'apprentissage\")\n",
        "\n",
        "    if not os.path.exists(DATA_DIR):\n",
        "        print(f\"\\nERREUR: {DATA_DIR} n'existe pas!\")\n",
        "        return\n",
        "\n",
        "    # Entraînement avec concaténation uniquement\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(\"ENTRAÎNEMENT AVEC CONCATÉNATION\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    results = train_dual_kfold_with_confusion_matrix(\n",
        "        data_dir=DATA_DIR,\n",
        "        k_folds=K_FOLDS,\n",
        "        epochs=EPOCHS\n",
        "    )\n",
        "\n",
        "    if results:\n",
        "        mean_acc = np.mean([r['best_val_acc'] for r in results])\n",
        "        std_acc = np.std([r['best_val_acc'] for r in results])\n",
        "\n",
        "        print(f\"\\n  Résultats finaux avec concaténation:\")\n",
        "        print(f\"   Accuracy moyenne: {mean_acc:.2f}% ± {std_acc:.2f}%\")\n",
        "        print(f\"   Meilleurs résultats par fold:\")\n",
        "\n",
        "        for r in results:\n",
        "            print(f\"     Fold {r['fold']}: {r['best_val_acc']:.2f}%\")\n",
        "\n",
        "        # Analyse des erreurs\n",
        "        analyze_prediction_errors(results)\n",
        "\n",
        "        # Courbes d'apprentissage\n",
        "        plot_learning_curves(results)\n",
        "\n",
        "        # Statistiques détaillées par relation\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(\"PERFORMANCE PAR RELATION SPATIALE\")\n",
        "        print(f\"{'='*60}\")\n",
        "\n",
        "        # Calculer les métriques globales\n",
        "        global_predictions = np.concatenate([r['final_predictions'] for r in results])\n",
        "        global_true_labels = np.concatenate([r['final_true_labels'] for r in results])\n",
        "\n",
        "        # Précision par classe\n",
        "        for i, relation in enumerate(SPATIAL_RELATIONS):\n",
        "            mask = global_true_labels == i\n",
        "            if np.sum(mask) > 0:\n",
        "                class_acc = 100 * np.mean(global_predictions[mask] == global_true_labels[mask])\n",
        "                support = np.sum(mask)\n",
        "                print(f\"  {relation:<15}: {class_acc:6.2f}% ({support:3d} échantillons)\")\n",
        "\n",
        "        print(f\"\\n   Expérience architecture duale avec concaténation terminée!\")\n",
        "        print(f\"    Performance globale: {mean_acc:.2f}% ± {std_acc:.2f}%\")\n",
        "\n",
        "        return results\n",
        "    else:\n",
        "        print(\"    Aucun résultat obtenu!\")\n",
        "        return None\n",
        "\n",
        "# =============================================================================\n",
        "# FONCTION DE TEST RAPIDE\n",
        "# =============================================================================\n",
        "\n",
        "def quick_test_concat():\n",
        "    \"\"\"Test rapide avec moins d'époques et de folds pour validation\"\"\"\n",
        "\n",
        "    DATA_DIR = \"data/spatialsense\"\n",
        "\n",
        "    print(\"=\"*60)\n",
        "    print(\"TEST RAPIDE - ARCHITECTURE DUALE CONCAT\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    if not os.path.exists(DATA_DIR):\n",
        "        print(f\"ERREUR: {DATA_DIR} n'existe pas!\")\n",
        "        return\n",
        "\n",
        "    # Test avec paramètres réduits\n",
        "    results = train_dual_kfold_with_confusion_matrix(\n",
        "        data_dir=DATA_DIR,\n",
        "        k_folds=3,  # Moins de folds\n",
        "        epochs=5    # Moins d'époques\n",
        "    )\n",
        "\n",
        "    if results:\n",
        "        mean_acc = np.mean([r['best_val_acc'] for r in results])\n",
        "        print(f\"\\n     Test rapide terminé!\")\n",
        "        print(f\"   Accuracy moyenne: {mean_acc:.2f}%\")\n",
        "\n",
        "        return results\n",
        "    else:\n",
        "        print(\"    Échec du test rapide!\")\n",
        "        return None\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Choix entre test rapide ou expérience complète\n",
        "    import sys\n",
        "\n",
        "    if len(sys.argv) > 1 and sys.argv[1] == \"quick\":\n",
        "        print(\"Mode test rapide activé...\")\n",
        "        quick_test_concat()\n",
        "    else:\n",
        "        print(\"Mode expérience complète activé...\")\n",
        "        main_dual_binary_mask_concat_only()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "RDXzYPUjNM1l",
        "outputId": "1f6703e6-e5ca-4a6c-80cd-d9272f802eaa"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import json\n",
        "import os\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tqdm import tqdm\n",
        "from collections import Counter\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# =============================================================================\n",
        "# CONFIGURATION GLOBALE\n",
        "# =============================================================================\n",
        "\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Device utilisé: {DEVICE}\")\n",
        "\n",
        "# Hyperparamètres\n",
        "BATCH_SIZE = 16  # Augmenté car une seule image en entrée\n",
        "LEARNING_RATE = 0.001\n",
        "EPOCHS = 15\n",
        "K_FOLDS = 5\n",
        "IMG_SIZE = 224\n",
        "DROPOUT_RATE = 0.4\n",
        "\n",
        "# Relations spatiales SpatialSense+\n",
        "SPATIAL_RELATIONS = [\n",
        "    'above', 'behind', 'in', 'in front of', 'next to',\n",
        "    'on', 'to the left of', 'to the right of', 'under'\n",
        "]\n",
        "\n",
        "print(f\"Architecture IMAGE + BOUNDING BOX COORDINATES:\")\n",
        "print(f\"  - Relations: {len(SPATIAL_RELATIONS)}\")\n",
        "print(f\"  - Entrée: Image originale + coordonnées des bounding boxes\")\n",
        "print(f\"  - Features VGG + features géométriques\")\n",
        "\n",
        "# =============================================================================\n",
        "# DATASET IMAGE + BOUNDING BOX COORDINATES\n",
        "# =============================================================================\n",
        "\n",
        "class ImageBBoxDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Dataset SpatialSense+ avec:\n",
        "    - Image originale uniquement\n",
        "    - Coordonnées des bounding boxes comme features supplémentaires\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, data_dir, split='train', transform=None):\n",
        "        self.data_dir = data_dir\n",
        "        self.split = split\n",
        "        self.transform = transform\n",
        "\n",
        "        self.relation_to_idx = {rel: idx for idx, rel in enumerate(SPATIAL_RELATIONS)}\n",
        "        self.idx_to_relation = {idx: rel for rel, idx in self.relation_to_idx.items()}\n",
        "\n",
        "        self.annotations = self._load_annotations()\n",
        "        self.data_samples = self._prepare_samples()\n",
        "\n",
        "        print(f\"Dataset Image + BBox Coordinates SpatialSense+ {split}: {len(self.data_samples)} échantillons\")\n",
        "        if len(self.data_samples) > 0:\n",
        "            self._print_statistics()\n",
        "\n",
        "    def _load_annotations(self):\n",
        "        annotations_path = os.path.join(self.data_dir, 'annotations.json')\n",
        "        try:\n",
        "            with open(annotations_path, 'r') as f:\n",
        "                return json.load(f)\n",
        "        except FileNotFoundError:\n",
        "            print(f\"Erreur: {annotations_path} non trouvé!\")\n",
        "            return []\n",
        "\n",
        "    def _find_image_path(self, image_url):\n",
        "        base_dir = os.path.join(self.data_dir, \"images\", \"images\")\n",
        "        filename = os.path.basename(image_url)\n",
        "\n",
        "        if \"staticflickr\" in image_url or len(filename.split('_')) == 2:\n",
        "            return os.path.join(base_dir, \"flickr\", filename)\n",
        "        else:\n",
        "            return os.path.join(base_dir, \"nyu\", filename)\n",
        "\n",
        "    def _prepare_samples(self):\n",
        "        samples = []\n",
        "        images_not_found = 0\n",
        "\n",
        "        for img_data in self.annotations:\n",
        "            if img_data['split'] != self.split:\n",
        "                continue\n",
        "\n",
        "            img_path = self._find_image_path(img_data['url'])\n",
        "\n",
        "            if not os.path.exists(img_path):\n",
        "                images_not_found += 1\n",
        "                continue\n",
        "\n",
        "            for ann in img_data['annotations']:\n",
        "                if ann['label'] and ann['predicate'].lower().strip() in [rel.lower() for rel in SPATIAL_RELATIONS]:\n",
        "                    # Trouver la relation correspondante\n",
        "                    relation = None\n",
        "                    for rel in SPATIAL_RELATIONS:\n",
        "                        if rel.lower() == ann['predicate'].lower().strip():\n",
        "                            relation = rel\n",
        "                            break\n",
        "\n",
        "                    if relation and 'bbox' in ann['subject'] and 'bbox' in ann['object']:\n",
        "                        sample = {\n",
        "                            'image_path': img_path,\n",
        "                            'subject': ann['subject']['name'],\n",
        "                            'object': ann['object']['name'],\n",
        "                            'relation': relation,\n",
        "                            'original_relation': ann['predicate'],\n",
        "                            'subject_bbox': ann['subject']['bbox'],  # [y1, y2, x1, x2]\n",
        "                            'object_bbox': ann['object']['bbox'],   # [y1, y2, x1, x2]\n",
        "                            'image_width': img_data['width'],\n",
        "                            'image_height': img_data['height']\n",
        "                        }\n",
        "                        samples.append(sample)\n",
        "\n",
        "        if images_not_found > 0:\n",
        "            print(f\"Images non trouvées: {images_not_found}\")\n",
        "\n",
        "        return samples\n",
        "\n",
        "    def _normalize_bbox(self, bbox, img_width, img_height):\n",
        "        \"\"\"\n",
        "        Normalise les coordonnées de bounding box entre 0 et 1\n",
        "        bbox format: [y1, y2, x1, x2]\n",
        "        \"\"\"\n",
        "        y1, y2, x1, x2 = bbox\n",
        "\n",
        "        # Normalisation\n",
        "        norm_y1 = y1 / img_height\n",
        "        norm_y2 = y2 / img_height\n",
        "        norm_x1 = x1 / img_width\n",
        "        norm_x2 = x2 / img_width\n",
        "\n",
        "        # Calcul des features géométriques\n",
        "        width = abs(norm_x2 - norm_x1)\n",
        "        height = abs(norm_y2 - norm_y1)\n",
        "        area = width * height\n",
        "        center_x = (norm_x1 + norm_x2) / 2\n",
        "        center_y = (norm_y1 + norm_y2) / 2\n",
        "        aspect_ratio = width / (height + 1e-8)  # Éviter division par zéro\n",
        "\n",
        "        return [norm_x1, norm_y1, norm_x2, norm_y2, width, height, area, center_x, center_y, aspect_ratio]\n",
        "\n",
        "    def _compute_spatial_features(self, subject_bbox, object_bbox, img_width, img_height):\n",
        "        \"\"\"\n",
        "        Calcule des features spatiales entre les deux bounding boxes\n",
        "        \"\"\"\n",
        "        # Normaliser les bounding boxes\n",
        "        subj_features = self._normalize_bbox(subject_bbox, img_width, img_height)\n",
        "        obj_features = self._normalize_bbox(object_bbox, img_width, img_height)\n",
        "\n",
        "        # Features individuelles (20 features: 10 + 10)\n",
        "        individual_features = subj_features + obj_features\n",
        "\n",
        "        # Features relationnelles\n",
        "        subj_center_x, subj_center_y = subj_features[7], subj_features[8]\n",
        "        obj_center_x, obj_center_y = obj_features[7], obj_features[8]\n",
        "\n",
        "        # Distance entre centres\n",
        "        distance = np.sqrt((subj_center_x - obj_center_x)**2 + (subj_center_y - obj_center_y)**2)\n",
        "\n",
        "        # Direction relative (angle)\n",
        "        angle = np.arctan2(obj_center_y - subj_center_y, obj_center_x - subj_center_x)\n",
        "\n",
        "        # Différences de taille\n",
        "        area_ratio = (subj_features[6] + 1e-8) / (obj_features[6] + 1e-8)\n",
        "\n",
        "        # Chevauchement (IoU approximatif)\n",
        "        subj_x1, subj_y1, subj_x2, subj_y2 = subj_features[:4]\n",
        "        obj_x1, obj_y1, obj_x2, obj_y2 = obj_features[:4]\n",
        "\n",
        "        # Intersection\n",
        "        inter_x1 = max(subj_x1, obj_x1)\n",
        "        inter_y1 = max(subj_y1, obj_y1)\n",
        "        inter_x2 = min(subj_x2, obj_x2)\n",
        "        inter_y2 = min(subj_y2, obj_y2)\n",
        "\n",
        "        if inter_x2 > inter_x1 and inter_y2 > inter_y1:\n",
        "            intersection = (inter_x2 - inter_x1) * (inter_y2 - inter_y1)\n",
        "            union = subj_features[6] + obj_features[6] - intersection\n",
        "            iou = intersection / (union + 1e-8)\n",
        "        else:\n",
        "            iou = 0.0\n",
        "            intersection = 0.0\n",
        "\n",
        "        # Position relative\n",
        "        relative_x = obj_center_x - subj_center_x\n",
        "        relative_y = obj_center_y - subj_center_y\n",
        "\n",
        "        # Features relationnelles (8 features)\n",
        "        relational_features = [\n",
        "            distance, angle, area_ratio, iou,\n",
        "            intersection, relative_x, relative_y,\n",
        "            1.0 if distance < 0.1 else 0.0  # Très proche\n",
        "        ]\n",
        "\n",
        "        # Total: 28 features (20 individuelles + 8 relationnelles)\n",
        "        return individual_features + relational_features\n",
        "\n",
        "    def _print_statistics(self):\n",
        "        relation_counts = Counter([s['relation'] for s in self.data_samples])\n",
        "        print(f\"\\nDistribution Image + BBox dans {self.split}:\")\n",
        "        total = len(self.data_samples)\n",
        "\n",
        "        for relation in SPATIAL_RELATIONS:\n",
        "            count = relation_counts.get(relation, 0)\n",
        "            percentage = count / total * 100 if total > 0 else 0\n",
        "            print(f\"   {relation}: {count} ({percentage:.1f}%)\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data_samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sample = self.data_samples[idx]\n",
        "\n",
        "        try:\n",
        "            # Charger l'image originale uniquement\n",
        "            image = Image.open(sample['image_path']).convert('RGB')\n",
        "\n",
        "            # Appliquer les transformations\n",
        "            if self.transform:\n",
        "                image = self.transform(image)\n",
        "\n",
        "            # Calculer les features spatiales des bounding boxes\n",
        "            spatial_features = self._compute_spatial_features(\n",
        "                sample['subject_bbox'],\n",
        "                sample['object_bbox'],\n",
        "                sample['image_width'],\n",
        "                sample['image_height']\n",
        "            )\n",
        "\n",
        "            # Convertir en tensor\n",
        "            spatial_features = torch.tensor(spatial_features, dtype=torch.float32)\n",
        "\n",
        "            label = self.relation_to_idx[sample['relation']]\n",
        "\n",
        "            metadata = {\n",
        "                'subject': sample['subject'],\n",
        "                'object': sample['object'],\n",
        "                'relation': sample['relation'],\n",
        "                'original_relation': sample['original_relation'],\n",
        "                'subject_bbox': sample['subject_bbox'],\n",
        "                'object_bbox': sample['object_bbox'],\n",
        "                'spatial_features_count': len(spatial_features)\n",
        "            }\n",
        "\n",
        "            return image, spatial_features, label, metadata\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Erreur chargement {sample['image_path']}: {e}\")\n",
        "            # Image et features par défaut en cas d'erreur\n",
        "            dummy_image = Image.new('RGB', (224, 224), color='gray')\n",
        "\n",
        "            if self.transform:\n",
        "                dummy_image = self.transform(dummy_image)\n",
        "            else:\n",
        "                dummy_image = torch.zeros(3, 224, 224)\n",
        "\n",
        "            dummy_features = torch.zeros(28, dtype=torch.float32)  # 28 features spatiales\n",
        "\n",
        "            return dummy_image, dummy_features, 0, {\n",
        "                'subject': 'error', 'object': 'error',\n",
        "                'relation': 'next to', 'original_relation': 'error',\n",
        "                'subject_bbox': [0, 0, 0, 0], 'object_bbox': [0, 0, 0, 0],\n",
        "                'spatial_features_count': 28\n",
        "            }\n",
        "\n",
        "# =============================================================================\n",
        "# ARCHITECTURE VGG + BOUNDING BOX FEATURES\n",
        "# =============================================================================\n",
        "\n",
        "class VGGBBoxFeatureExtractor(nn.Module):\n",
        "    \"\"\"\n",
        "    Extracteur de features combinant:\n",
        "    - Features VGG de l'image\n",
        "    - Features géométriques des bounding boxes\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, spatial_features_dim=28):\n",
        "        super(VGGBBoxFeatureExtractor, self).__init__()\n",
        "\n",
        "        self.spatial_features_dim = spatial_features_dim\n",
        "\n",
        "        # VGG16 pré-entraîné pour l'image\n",
        "        vgg16 = models.vgg16(pretrained=True)\n",
        "\n",
        "        # Extraction des couches\n",
        "        self.features = vgg16.features\n",
        "        self.avgpool = vgg16.avgpool\n",
        "\n",
        "        # FC-7 de VGG\n",
        "        classifier_layers = list(vgg16.classifier.children())[:6]\n",
        "        self.fc7 = nn.Sequential(*classifier_layers)\n",
        "\n",
        "        # Gel des poids VGG\n",
        "        for param in self.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        # Projection des features spatiales\n",
        "        self.spatial_projection = nn.Sequential(\n",
        "            nn.Linear(spatial_features_dim, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(256, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3)\n",
        "        )\n",
        "\n",
        "        # Dimension de sortie après concaténation\n",
        "        self.fusion_dim = 4096 + 512  # VGG FC-7 + projected spatial features\n",
        "\n",
        "        print(f\"VGG + BBox Feature Extractor:\")\n",
        "        print(f\"  - VGG FC-7: 4096 features\")\n",
        "        print(f\"  - Spatial features: {spatial_features_dim} → 512 (projetées)\")\n",
        "        print(f\"  - Dimension sortie: {self.fusion_dim}\")\n",
        "\n",
        "    def forward(self, image, spatial_features):\n",
        "        # Extraction features VGG de l'image\n",
        "        vgg_features = self._extract_vgg_features(image)\n",
        "\n",
        "        # Projection des features spatiales\n",
        "        projected_spatial = self.spatial_projection(spatial_features)\n",
        "\n",
        "        # Concaténation\n",
        "        fused_features = torch.cat([vgg_features, projected_spatial], dim=1)\n",
        "\n",
        "        return fused_features\n",
        "\n",
        "    def _extract_vgg_features(self, x):\n",
        "        \"\"\"Extraction des features VGG FC-7\"\"\"\n",
        "        x = self.features(x)\n",
        "        x = self.avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc7(x)\n",
        "        return x\n",
        "\n",
        "class SpatialRelationMLP(nn.Module):\n",
        "    \"\"\"\n",
        "    MLP pour la classification des relations spatiales\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_dim, hidden1_dim=512, hidden2_dim=256,\n",
        "                 num_relations=len(SPATIAL_RELATIONS), dropout_rate=0.4):\n",
        "        super(SpatialRelationMLP, self).__init__()\n",
        "\n",
        "        print(f\"Spatial Relation MLP:\")\n",
        "        print(f\"  - Input: {input_dim}\")\n",
        "        print(f\"  - Hidden 1: {hidden1_dim}\")\n",
        "        print(f\"  - Hidden 2: {hidden2_dim}\")\n",
        "        print(f\"  - Output: {num_relations}\")\n",
        "        print(f\"  - Dropout: {dropout_rate}\")\n",
        "\n",
        "        # Couches MLP\n",
        "        self.fc1 = nn.Linear(input_dim, hidden1_dim)\n",
        "        self.bn1 = nn.BatchNorm1d(hidden1_dim)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.dropout1 = nn.Dropout(dropout_rate)\n",
        "\n",
        "        self.fc2 = nn.Linear(hidden1_dim, hidden2_dim)\n",
        "        self.bn2 = nn.BatchNorm1d(hidden2_dim)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.dropout2 = nn.Dropout(dropout_rate)\n",
        "\n",
        "        self.fc3 = nn.Linear(hidden2_dim, num_relations)\n",
        "\n",
        "        # Initialisation\n",
        "        nn.init.xavier_uniform_(self.fc1.weight)\n",
        "        nn.init.xavier_uniform_(self.fc2.weight)\n",
        "        nn.init.xavier_uniform_(self.fc3.weight)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu1(x)\n",
        "        x = self.dropout1(x)\n",
        "\n",
        "        x = self.fc2(x)\n",
        "        x = self.bn2(x)\n",
        "        x = self.relu2(x)\n",
        "        x = self.dropout2(x)\n",
        "\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "class ImageBBoxSpatialRelationModel(nn.Module):\n",
        "    \"\"\"\n",
        "    Modèle complet avec image + coordonnées bounding boxes\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_relations=len(SPATIAL_RELATIONS), spatial_features_dim=28):\n",
        "        super(ImageBBoxSpatialRelationModel, self).__init__()\n",
        "\n",
        "        print(f\"Modèle Image + BBox Spatial Relation:\")\n",
        "        print(f\"  - Relations: {num_relations}\")\n",
        "        print(f\"  - Features spatiales: {spatial_features_dim}\")\n",
        "\n",
        "        # Extracteur de features\n",
        "        self.feature_extractor = VGGBBoxFeatureExtractor(spatial_features_dim=spatial_features_dim)\n",
        "\n",
        "        # Classifieur MLP\n",
        "        self.classifier = SpatialRelationMLP(\n",
        "            input_dim=self.feature_extractor.fusion_dim,\n",
        "            num_relations=num_relations\n",
        "        )\n",
        "\n",
        "    def forward(self, image, spatial_features):\n",
        "        # Extraction et fusion des features\n",
        "        fused_features = self.feature_extractor(image, spatial_features)\n",
        "\n",
        "        # Classification\n",
        "        output = self.classifier(fused_features)\n",
        "        return output\n",
        "\n",
        "# =============================================================================\n",
        "# EARLY STOPPING CLASS\n",
        "# =============================================================================\n",
        "\n",
        "class EarlyStopping:\n",
        "    \"\"\"Early Stopping pour éviter l'overfitting\"\"\"\n",
        "\n",
        "    def __init__(self, patience=5, min_delta=0.001, restore_best_weights=True, verbose=True):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            patience (int): Nombre d'époques à attendre sans amélioration (défaut: 5)\n",
        "            min_delta (float): Amélioration minimale considérée comme significative\n",
        "            restore_best_weights (bool): Restaurer les meilleurs poids à la fin\n",
        "            verbose (bool): Afficher les messages\n",
        "        \"\"\"\n",
        "        self.patience = patience\n",
        "        self.min_delta = min_delta\n",
        "        self.restore_best_weights = restore_best_weights\n",
        "        self.verbose = verbose\n",
        "\n",
        "        self.best_score = None\n",
        "        self.counter = 0\n",
        "        self.best_weights = None\n",
        "        self.early_stop = False\n",
        "\n",
        "    def __call__(self, val_score, model):\n",
        "        \"\"\"\n",
        "        Vérifie si l'entraînement doit s'arrêter\n",
        "\n",
        "        Args:\n",
        "            val_score (float): Score de validation (accuracy %)\n",
        "            model: Modèle PyTorch\n",
        "        \"\"\"\n",
        "        if self.best_score is None:\n",
        "            self.best_score = val_score\n",
        "            self.save_checkpoint(model)\n",
        "            if self.verbose:\n",
        "                print(f\"   Early Stopping: Score initial = {val_score:.3f}%\")\n",
        "\n",
        "        elif val_score < self.best_score + self.min_delta:\n",
        "            self.counter += 1\n",
        "            if self.verbose:\n",
        "                print(f\"     Early Stopping: {self.counter}/{self.patience} (Best: {self.best_score:.3f}%)\")\n",
        "\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "                if self.verbose:\n",
        "                    print(f\"🛑 Early Stopping déclenché! Restauration du meilleur modèle (Accuracy: {self.best_score:.3f}%)\")\n",
        "\n",
        "        else:\n",
        "            improvement = val_score - self.best_score\n",
        "            if self.verbose:\n",
        "                print(f\"    Amélioration: {improvement:.3f}% (Nouveau best: {val_score:.3f}%)\")\n",
        "\n",
        "            self.best_score = val_score\n",
        "            self.save_checkpoint(model)\n",
        "            self.counter = 0\n",
        "\n",
        "    def save_checkpoint(self, model):\n",
        "        \"\"\"Sauvegarde les poids du modèle\"\"\"\n",
        "        if self.restore_best_weights:\n",
        "            self.best_weights = {key: value.cpu().clone() for key, value in model.state_dict().items()}\n",
        "\n",
        "    def restore_best_weights_to_model(self, model):\n",
        "        \"\"\"Restaure les meilleurs poids dans le modèle\"\"\"\n",
        "        if self.best_weights is not None:\n",
        "            # Restaurer les poids sur le bon device\n",
        "            device = next(model.parameters()).device\n",
        "            best_weights_on_device = {key: value.to(device) for key, value in self.best_weights.items()}\n",
        "            model.load_state_dict(best_weights_on_device)\n",
        "\n",
        "# =============================================================================\n",
        "# FONCTIONS D'ÉVALUATION AVEC MATRICE DE CONFUSION\n",
        "# =============================================================================\n",
        "\n",
        "def evaluate_with_confusion_matrix(model, dataloader, criterion, device, class_names=None):\n",
        "    \"\"\"Évalue le modèle et retourne les métriques + matrice de confusion\"\"\"\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    all_predictions = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, spatial_features, labels, _ in tqdm(dataloader, desc='Evaluating'):\n",
        "            images = images.to(device)\n",
        "            spatial_features = spatial_features.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(images, spatial_features)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "            all_predictions.extend(predicted.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    # Calculs des métriques\n",
        "    accuracy = 100. * np.mean(np.array(all_predictions) == np.array(all_labels))\n",
        "    avg_loss = running_loss / len(dataloader)\n",
        "\n",
        "    # Matrice de confusion\n",
        "    cm = confusion_matrix(all_labels, all_predictions)\n",
        "\n",
        "    # Rapport de classification\n",
        "    if class_names is None:\n",
        "        class_names = SPATIAL_RELATIONS\n",
        "\n",
        "    report = classification_report(\n",
        "        all_labels, all_predictions,\n",
        "        target_names=class_names,\n",
        "        output_dict=True,\n",
        "        zero_division=0\n",
        "    )\n",
        "\n",
        "    return avg_loss, accuracy, cm, report, all_predictions, all_labels\n",
        "\n",
        "def plot_confusion_matrix(cm, class_names, title=\"Matrice de Confusion\", normalize=False):\n",
        "    \"\"\"Affiche la matrice de confusion avec style amélioré\"\"\"\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "        fmt = '.2f'\n",
        "        title += \" (Normalisée)\"\n",
        "    else:\n",
        "        fmt = 'd'\n",
        "\n",
        "    plt.figure(figsize=(12, 10))\n",
        "    sns.heatmap(cm,\n",
        "                annot=True,\n",
        "                fmt=fmt,\n",
        "                cmap='Blues',\n",
        "                xticklabels=class_names,\n",
        "                yticklabels=class_names,\n",
        "                cbar_kws={'label': 'Proportion' if normalize else 'Nombre de prédictions'})\n",
        "\n",
        "    plt.title(title, fontsize=16, fontweight='bold', pad=20)\n",
        "    plt.xlabel('Prédictions', fontsize=14, fontweight='bold')\n",
        "    plt.ylabel('Vraies étiquettes', fontsize=14, fontweight='bold')\n",
        "    plt.xticks(rotation=45, ha='right')\n",
        "    plt.yticks(rotation=0)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def display_classification_metrics(report, title=\"Métriques de Classification\"):\n",
        "    \"\"\"Affiche les métriques de classification sous forme de tableau\"\"\"\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"{title}\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    # Métriques par classe\n",
        "    print(f\"{'Classe':<15} {'Précision':<10} {'Rappel':<10} {'F1-Score':<10} {'Support':<10}\")\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "    for class_name in SPATIAL_RELATIONS:\n",
        "        if class_name in report:\n",
        "            metrics = report[class_name]\n",
        "            print(f\"{class_name:<15} {metrics['precision']:<10.3f} {metrics['recall']:<10.3f} \"\n",
        "                  f\"{metrics['f1-score']:<10.3f} {metrics['support']:<10.0f}\")\n",
        "\n",
        "    # Métriques globales\n",
        "    print(\"-\" * 60)\n",
        "    print(f\"{'Accuracy':<15} {'':<10} {'':<10} {report['accuracy']:<10.3f} {report['macro avg']['support']:<10.0f}\")\n",
        "    print(f\"{'Macro avg':<15} {report['macro avg']['precision']:<10.3f} {report['macro avg']['recall']:<10.3f} \"\n",
        "          f\"{report['macro avg']['f1-score']:<10.3f} {report['macro avg']['support']:<10.0f}\")\n",
        "    print(f\"{'Weighted avg':<15} {report['weighted avg']['precision']:<10.3f} {report['weighted avg']['recall']:<10.3f} \"\n",
        "          f\"{report['weighted avg']['f1-score']:<10.3f} {report['weighted avg']['support']:<10.0f}\")\n",
        "\n",
        "# =============================================================================\n",
        "# VISUALISATION AVEC BOUNDING BOXES\n",
        "# =============================================================================\n",
        "\n",
        "def visualize_image_bbox_samples(dataset, num_samples=4):\n",
        "    \"\"\"Visualise des échantillons avec image et informations des bounding boxes\"\"\"\n",
        "    fig, axes = plt.subplots(num_samples, 2, figsize=(12, 4*num_samples))\n",
        "    if num_samples == 1:\n",
        "        axes = axes.reshape(1, -1)\n",
        "\n",
        "    for i in range(min(num_samples, len(dataset))):\n",
        "        image, spatial_features, label, metadata = dataset[i]\n",
        "\n",
        "        # Dénormaliser pour affichage\n",
        "        def denormalize(tensor):\n",
        "            mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
        "            std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n",
        "            return (tensor * std + mean).clamp(0, 1).permute(1, 2, 0)\n",
        "\n",
        "        image_display = denormalize(image)\n",
        "\n",
        "        # Image originale\n",
        "        axes[i, 0].imshow(image_display)\n",
        "        axes[i, 0].set_title(f\"Image Originale\\n{metadata['subject']} {metadata['relation']} {metadata['object']}\")\n",
        "        axes[i, 0].axis('off')\n",
        "\n",
        "        # Informations détaillées\n",
        "        axes[i, 1].axis('off')\n",
        "        info_text = f\"\"\"\n",
        "        Échantillon {i+1}:\n",
        "\n",
        "        Sujet: {metadata['subject']}\n",
        "        Objet: {metadata['object']}\n",
        "        Relation: {metadata['relation']}\n",
        "        Original: {metadata['original_relation']}\n",
        "\n",
        "        Subject BBox: {metadata['subject_bbox']}\n",
        "        Object BBox: {metadata['object_bbox']}\n",
        "\n",
        "        Features Spatiales ({metadata['spatial_features_count']}):\n",
        "        • Coordinates normalisées (8)\n",
        "        • Dimensions et aires (6)\n",
        "        • Centres et ratios (6)\n",
        "        • Features relationnelles (8)\n",
        "\n",
        "        Architecture:\n",
        "        • VGG features (4096)\n",
        "        • Spatial features projetées (512)\n",
        "        • Total: 4608 features\n",
        "        \"\"\"\n",
        "        axes[i, 1].text(0.1, 0.5, info_text, transform=axes[i, 1].transAxes,\n",
        "                       fontsize=9, verticalalignment='center',\n",
        "                       bbox=dict(boxstyle='round', facecolor='lightgreen', alpha=0.8))\n",
        "\n",
        "    plt.suptitle('Architecture: Image + Coordonnées Bounding Boxes',\n",
        "                 fontsize=16, fontweight='bold')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# =============================================================================\n",
        "# TRANSFORMATIONS\n",
        "# =============================================================================\n",
        "\n",
        "def create_transforms():\n",
        "    \"\"\"Transformations pour les images\"\"\"\n",
        "    normalize = transforms.Normalize(\n",
        "        mean=[0.485, 0.456, 0.406],\n",
        "        std=[0.229, 0.224, 0.225]\n",
        "    )\n",
        "\n",
        "    train_transform = transforms.Compose([\n",
        "        transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "        transforms.RandomHorizontalFlip(p=0.5),\n",
        "        transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.05),\n",
        "        transforms.ToTensor(),\n",
        "        normalize\n",
        "    ])\n",
        "\n",
        "    val_transform = transforms.Compose([\n",
        "        transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "        transforms.ToTensor(),\n",
        "        normalize\n",
        "    ])\n",
        "\n",
        "    return train_transform, val_transform\n",
        "\n",
        "# =============================================================================\n",
        "# FONCTIONS D'ENTRAÎNEMENT\n",
        "# =============================================================================\n",
        "\n",
        "def train_epoch(model, dataloader, criterion, optimizer, device):\n",
        "    \"\"\"Entraîne une époque\"\"\"\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    progress_bar = tqdm(dataloader, desc='Training')\n",
        "    for batch_idx, (images, spatial_features, labels, _) in enumerate(progress_bar):\n",
        "        images = images.to(device)\n",
        "        spatial_features = spatial_features.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images, spatial_features)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "\n",
        "        # Gradient clipping\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "        progress_bar.set_postfix({\n",
        "            'loss': running_loss / (batch_idx + 1),\n",
        "            'acc': 100. * correct / total\n",
        "        })\n",
        "\n",
        "    return running_loss / len(dataloader), 100. * correct / total\n",
        "\n",
        "# =============================================================================\n",
        "# ENTRAÎNEMENT K-FOLD AVEC MATRICE DE CONFUSION ET EARLY STOPPING\n",
        "# =============================================================================\n",
        "\n",
        "def train_kfold_with_confusion_matrix(data_dir, k_folds=5, epochs=15,\n",
        "                                     early_stopping_patience=5, min_delta=0.001):\n",
        "    \"\"\"Entraînement K-fold avec image + bounding box coordinates et Early Stopping\"\"\"\n",
        "\n",
        "    print(\"=\"*70)\n",
        "    print(\"ENTRAÎNEMENT IMAGE + BOUNDING BOX COORDINATES + EARLY STOPPING\")\n",
        "    print(\"=\"*70)\n",
        "    print(f\"Architecture: VGG + Spatial Features MLP\")\n",
        "    print(f\"Entrée: Image originale + 28 features spatiales des bounding boxes\")\n",
        "    print(f\"Early Stopping: Patience={early_stopping_patience}, Min Delta={min_delta}\")\n",
        "\n",
        "    # Transformations\n",
        "    train_transform, val_transform = create_transforms()\n",
        "\n",
        "    # Dataset\n",
        "    full_dataset = ImageBBoxDataset(\n",
        "        data_dir=data_dir,\n",
        "        split='train',\n",
        "        transform=train_transform\n",
        "    )\n",
        "\n",
        "    if len(full_dataset) == 0:\n",
        "        print(\"Dataset vide!\")\n",
        "        return []\n",
        "\n",
        "    # Visualisation d'échantillons\n",
        "    print(\"\\nVisu échantillons image + bounding box coordinates:\")\n",
        "    visualize_image_bbox_samples(full_dataset, num_samples=3)\n",
        "\n",
        "    # K-fold\n",
        "    kfold = KFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
        "    fold_results = []\n",
        "\n",
        "    for fold, (train_indices, val_indices) in enumerate(kfold.split(full_dataset)):\n",
        "        print(f\"\\n{'='*50}\")\n",
        "        print(f\"FOLD {fold + 1}/{k_folds} - IMAGE + BBOX COORDINATES + EARLY STOPPING\")\n",
        "        print(f\"{'='*50}\")\n",
        "\n",
        "        # Sous-ensembles\n",
        "        train_subset = torch.utils.data.Subset(full_dataset, train_indices)\n",
        "        val_subset = torch.utils.data.Subset(full_dataset, val_indices)\n",
        "\n",
        "        # DataLoaders\n",
        "        train_loader = DataLoader(\n",
        "            train_subset, batch_size=BATCH_SIZE, shuffle=True,\n",
        "            num_workers=2, pin_memory=True\n",
        "        )\n",
        "\n",
        "        val_loader = DataLoader(\n",
        "            val_subset, batch_size=BATCH_SIZE, shuffle=False,\n",
        "            num_workers=2, pin_memory=True\n",
        "        )\n",
        "\n",
        "        # Modèle\n",
        "        model = ImageBBoxSpatialRelationModel(\n",
        "            num_relations=len(SPATIAL_RELATIONS),\n",
        "            spatial_features_dim=28\n",
        "        )\n",
        "        model = model.to(DEVICE)\n",
        "\n",
        "        # Optimiseur (entraîner seulement les couches non-VGG)\n",
        "        trainable_params = []\n",
        "        trainable_params.extend(model.feature_extractor.spatial_projection.parameters())\n",
        "        trainable_params.extend(model.classifier.parameters())\n",
        "\n",
        "        optimizer = optim.AdamW(trainable_params, lr=LEARNING_RATE, weight_decay=0.01)\n",
        "        criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
        "\n",
        "        # Scheduler\n",
        "        scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
        "\n",
        "        # Early Stopping\n",
        "        early_stopping = EarlyStopping(\n",
        "            patience=early_stopping_patience,\n",
        "            min_delta=min_delta,\n",
        "            restore_best_weights=True,\n",
        "            verbose=True\n",
        "        )\n",
        "\n",
        "        # Entraînement\n",
        "        train_losses, val_losses = [], []\n",
        "        train_accs, val_accs = [], []\n",
        "        best_val_acc = 0.0\n",
        "        actual_epochs = 0\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            print(f\"\\n   Epoch {epoch+1}/{epochs}\")\n",
        "\n",
        "            # Training\n",
        "            train_loss, train_acc = train_epoch(\n",
        "                model, train_loader, criterion, optimizer, DEVICE\n",
        "            )\n",
        "            train_losses.append(train_loss)\n",
        "            train_accs.append(train_acc)\n",
        "\n",
        "            # Validation avec matrice de confusion\n",
        "            val_loss, val_acc, cm, report, predictions, true_labels = evaluate_with_confusion_matrix(\n",
        "                model, val_loader, criterion, DEVICE, SPATIAL_RELATIONS\n",
        "            )\n",
        "            val_losses.append(val_loss)\n",
        "            val_accs.append(val_acc)\n",
        "\n",
        "            scheduler.step()\n",
        "            actual_epochs = epoch + 1\n",
        "\n",
        "            print(f\"       Train: {train_loss:.4f} / {train_acc:.2f}%\")\n",
        "            print(f\"   Val: {val_loss:.4f} / {val_acc:.2f}%\")\n",
        "\n",
        "            if val_acc > best_val_acc:\n",
        "                best_val_acc = val_acc\n",
        "                torch.save(model.state_dict(),\n",
        "                          f'best_image_bbox_model_fold_{fold+1}.pth')\n",
        "\n",
        "            # Vérification Early Stopping\n",
        "            early_stopping(val_acc, model)\n",
        "\n",
        "            if early_stopping.early_stop:\n",
        "                print(f\"🛑 Arrêt anticipé à l'époque {epoch+1}\")\n",
        "                # Restaurer les meilleurs poids\n",
        "                early_stopping.restore_best_weights_to_model(model)\n",
        "                break\n",
        "\n",
        "        # Si pas d'arrêt anticipé, restaurer quand même les meilleurs poids\n",
        "        if not early_stopping.early_stop:\n",
        "            early_stopping.restore_best_weights_to_model(model)\n",
        "            print(f\"   Entraînement terminé ({epochs} époques). Meilleurs poids restaurés.\")\n",
        "\n",
        "        # Évaluation finale avec matrice de confusion\n",
        "        print(f\"\\n{'='*40}\")\n",
        "        print(f\"ÉVALUATION FINALE FOLD {fold + 1}\")\n",
        "        print(f\"{'='*40}\")\n",
        "\n",
        "        final_val_loss, final_val_acc, final_cm, final_report, final_predictions, final_true_labels = evaluate_with_confusion_matrix(\n",
        "            model, val_loader, criterion, DEVICE, SPATIAL_RELATIONS\n",
        "        )\n",
        "\n",
        "        # Affichage des métriques\n",
        "        display_classification_metrics(final_report, f\"Métriques Fold {fold + 1}\")\n",
        "\n",
        "        # Affichage matrice de confusion\n",
        "        plot_confusion_matrix(final_cm, SPATIAL_RELATIONS,\n",
        "                            f\"Matrice de Confusion - Fold {fold + 1}\", normalize=False)\n",
        "        plot_confusion_matrix(final_cm, SPATIAL_RELATIONS,\n",
        "                            f\"Matrice de Confusion Normalisée - Fold {fold + 1}\", normalize=True)\n",
        "\n",
        "        # Résultats fold\n",
        "        fold_results.append({\n",
        "            'fold': fold + 1,\n",
        "            'train_losses': train_losses,\n",
        "            'val_losses': val_losses,\n",
        "            'train_accs': train_accs,\n",
        "            'val_accs': val_accs,\n",
        "            'best_val_acc': early_stopping.best_score,  # Utiliser le meilleur score d'early stopping\n",
        "            'final_confusion_matrix': final_cm,\n",
        "            'final_report': final_report,\n",
        "            'final_predictions': final_predictions,\n",
        "            'final_true_labels': final_true_labels,\n",
        "            'model': model,\n",
        "            'actual_epochs': actual_epochs,\n",
        "            'early_stopped': early_stopping.early_stop\n",
        "        })\n",
        "\n",
        "        print(f\"   Fold {fold + 1} - Meilleur: {early_stopping.best_score:.2f}% (après {actual_epochs} époques)\")\n",
        "        if early_stopping.early_stop:\n",
        "            print(f\"   🛑 Arrêt anticipé activé\")\n",
        "        else:\n",
        "            print(f\"      Entraînement complet\")\n",
        "\n",
        "    # Analyse globale des résultats\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(\"ANALYSE GLOBALE DES RÉSULTATS AVEC EARLY STOPPING\")\n",
        "    print(f\"{'='*70}\")\n",
        "\n",
        "    # Moyennes des métriques\n",
        "    mean_acc = np.mean([r['best_val_acc'] for r in fold_results])\n",
        "    std_acc = np.std([r['best_val_acc'] for r in fold_results])\n",
        "    print(f\"Accuracy moyenne: {mean_acc:.2f}% ± {std_acc:.2f}%\")\n",
        "\n",
        "    # Statistiques Early Stopping\n",
        "    early_stopped_folds = [r for r in fold_results if r['early_stopped']]\n",
        "    mean_epochs = np.mean([r['actual_epochs'] for r in fold_results])\n",
        "\n",
        "    print(f\"   Statistiques Early Stopping:\")\n",
        "    print(f\"   Folds avec arrêt anticipé: {len(early_stopped_folds)}/{k_folds}\")\n",
        "    print(f\"   Époques moyennes: {mean_epochs:.1f}/{epochs}\")\n",
        "\n",
        "    for r in fold_results:\n",
        "        status = \"🛑 Arrêté\" if r['early_stopped'] else \"   Complet\"\n",
        "        print(f\"   Fold {r['fold']}: {r['actual_epochs']:2d} époques - {status}\")\n",
        "\n",
        "    # Matrice de confusion globale\n",
        "    global_cm = np.sum([r['final_confusion_matrix'] for r in fold_results], axis=0)\n",
        "    global_predictions = np.concatenate([r['final_predictions'] for r in fold_results])\n",
        "    global_true_labels = np.concatenate([r['final_true_labels'] for r in fold_results])\n",
        "\n",
        "    # Rapport global\n",
        "    global_report = classification_report(\n",
        "        global_true_labels, global_predictions,\n",
        "        target_names=SPATIAL_RELATIONS,\n",
        "        output_dict=True,\n",
        "        zero_division=0\n",
        "    )\n",
        "\n",
        "    # Affichage final\n",
        "    display_classification_metrics(global_report, \"Métriques Globales (tous les folds)\")\n",
        "    plot_confusion_matrix(global_cm, SPATIAL_RELATIONS,\n",
        "                        \"Matrice de Confusion Globale\", normalize=False)\n",
        "    plot_confusion_matrix(global_cm, SPATIAL_RELATIONS,\n",
        "                        \"Matrice de Confusion Globale Normalisée\", normalize=True)\n",
        "\n",
        "    return fold_results\n",
        "\n",
        "# =============================================================================\n",
        "# ANALYSE DES ERREURS\n",
        "# =============================================================================\n",
        "\n",
        "def analyze_prediction_errors(fold_results, top_k=5):\n",
        "    \"\"\"Analyse des erreurs de prédiction les plus fréquentes\"\"\"\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(\"ANALYSE DES ERREURS DE PRÉDICTION\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    # Combiner toutes les prédictions\n",
        "    all_predictions = np.concatenate([r['final_predictions'] for r in fold_results])\n",
        "    all_true_labels = np.concatenate([r['final_true_labels'] for r in fold_results])\n",
        "\n",
        "    # Identifier les erreurs\n",
        "    errors = []\n",
        "    for true_idx, pred_idx in zip(all_true_labels, all_predictions):\n",
        "        if true_idx != pred_idx:\n",
        "            true_relation = SPATIAL_RELATIONS[true_idx]\n",
        "            pred_relation = SPATIAL_RELATIONS[pred_idx]\n",
        "            errors.append((true_relation, pred_relation))\n",
        "\n",
        "    # Compter les erreurs les plus fréquentes\n",
        "    error_counts = Counter(errors)\n",
        "\n",
        "    print(f\"Nombre total d'erreurs: {len(errors)}\")\n",
        "    print(f\"Accuracy globale: {100 * (1 - len(errors) / len(all_predictions)):.2f}%\")\n",
        "    print(f\"\\nTop {top_k} erreurs les plus fréquentes:\")\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "    for i, ((true_rel, pred_rel), count) in enumerate(error_counts.most_common(top_k)):\n",
        "        percentage = 100 * count / len(errors)\n",
        "        print(f\"{i+1:2d}. {true_rel:>15} → {pred_rel:<15} : {count:3d} ({percentage:5.1f}%)\")\n",
        "\n",
        "def plot_learning_curves_with_early_stopping(fold_results):\n",
        "    \"\"\"Affiche les courbes d'apprentissage avec indication de l'Early Stopping\"\"\"\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "\n",
        "    # Loss curves\n",
        "    axes[0, 0].set_title('Courbes de Loss - Training', fontweight='bold')\n",
        "    axes[0, 1].set_title('Courbes de Loss - Validation', fontweight='bold')\n",
        "    axes[1, 0].set_title('Courbes d\\'Accuracy - Training', fontweight='bold')\n",
        "    axes[1, 1].set_title('Courbes d\\'Accuracy - Validation', fontweight='bold')\n",
        "\n",
        "    colors = ['blue', 'red', 'green', 'orange', 'purple']\n",
        "\n",
        "    for i, result in enumerate(fold_results):\n",
        "        epochs = range(1, len(result['train_losses']) + 1)\n",
        "        color = colors[i % len(colors)]\n",
        "\n",
        "        # Style de ligne selon early stopping\n",
        "        linestyle = '--' if result['early_stopped'] else '-'\n",
        "        alpha = 0.8 if result['early_stopped'] else 0.7\n",
        "\n",
        "        label = f'Fold {result[\"fold\"]}'\n",
        "        if result['early_stopped']:\n",
        "            label += f' (ES@{result[\"actual_epochs\"]})'\n",
        "\n",
        "        # Training loss\n",
        "        axes[0, 0].plot(epochs, result['train_losses'],\n",
        "                       color=color, label=label, alpha=alpha, linestyle=linestyle)\n",
        "\n",
        "        # Validation loss\n",
        "        axes[0, 1].plot(epochs, result['val_losses'],\n",
        "                       color=color, label=label, alpha=alpha, linestyle=linestyle)\n",
        "\n",
        "        # Training accuracy\n",
        "        axes[1, 0].plot(epochs, result['train_accs'],\n",
        "                       color=color, label=label, alpha=alpha, linestyle=linestyle)\n",
        "\n",
        "        # Validation accuracy\n",
        "        axes[1, 1].plot(epochs, result['val_accs'],\n",
        "                       color=color, label=label, alpha=alpha, linestyle=linestyle)\n",
        "\n",
        "        # Marquer le point d'arrêt si early stopping\n",
        "        if result['early_stopped']:\n",
        "            stop_epoch = result['actual_epochs']\n",
        "            # Marquer sur validation accuracy\n",
        "            axes[1, 1].scatter(stop_epoch, result['val_accs'][stop_epoch-1],\n",
        "                             color=color, s=100, marker='X', zorder=5)\n",
        "\n",
        "    # Configuration des axes\n",
        "    for ax in axes.flat:\n",
        "        ax.set_xlabel('Époque')\n",
        "        ax.legend()\n",
        "        ax.grid(True, alpha=0.3)\n",
        "\n",
        "    axes[0, 0].set_ylabel('Loss')\n",
        "    axes[0, 1].set_ylabel('Loss')\n",
        "    axes[1, 0].set_ylabel('Accuracy (%)')\n",
        "    axes[1, 1].set_ylabel('Accuracy (%)')\n",
        "\n",
        "    plt.suptitle('Courbes d\\'Apprentissage avec Early Stopping\\n(X = Arrêt anticipé)',\n",
        "                 fontsize=16, fontweight='bold')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def analyze_spatial_features_importance(fold_results):\n",
        "    \"\"\"Analyse l'importance des features spatiales\"\"\"\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(\"ANALYSE DES FEATURES SPATIALES\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    feature_names = [\n",
        "        # Features Subject (10)\n",
        "        'subj_x1', 'subj_y1', 'subj_x2', 'subj_y2', 'subj_width', 'subj_height',\n",
        "        'subj_area', 'subj_center_x', 'subj_center_y', 'subj_aspect_ratio',\n",
        "\n",
        "        # Features Object (10)\n",
        "        'obj_x1', 'obj_y1', 'obj_x2', 'obj_y2', 'obj_width', 'obj_height',\n",
        "        'obj_area', 'obj_center_x', 'obj_center_y', 'obj_aspect_ratio',\n",
        "\n",
        "        # Features Relationnelles (8)\n",
        "        'distance', 'angle', 'area_ratio', 'iou', 'intersection',\n",
        "        'relative_x', 'relative_y', 'very_close'\n",
        "    ]\n",
        "\n",
        "    print(\"Features Spatiales Calculées (28 au total):\")\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "    categories = {\n",
        "        'Sujet (10)': feature_names[:10],\n",
        "        'Objet (10)': feature_names[10:20],\n",
        "        'Relationnelles (8)': feature_names[20:]\n",
        "    }\n",
        "\n",
        "    for category, features in categories.items():\n",
        "        print(f\"\\n{category}:\")\n",
        "        for i, feature in enumerate(features):\n",
        "            print(f\"  {i+1:2d}. {feature}\")\n",
        "\n",
        "    print(f\"\\nCes features sont projetées de 28 → 512 dimensions avant fusion avec VGG.\")\n",
        "\n",
        "# =============================================================================\n",
        "# FONCTION PRINCIPALE\n",
        "# =============================================================================\n",
        "\n",
        "def main_image_bbox_experiment():\n",
        "    \"\"\"Expérience principale avec image + coordonnées bounding boxes et Early Stopping\"\"\"\n",
        "\n",
        "    DATA_DIR = \"data/spatialsense\"\n",
        "\n",
        "\n",
        "    if not os.path.exists(DATA_DIR):\n",
        "        print(f\"\\nERREUR: {DATA_DIR} n'existe pas!\")\n",
        "        return\n",
        "\n",
        "    # Entraînement avec Early Stopping\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(\"ENTRAÎNEMENT AVEC EARLY STOPPING\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    results = train_kfold_with_confusion_matrix(\n",
        "        data_dir=DATA_DIR,\n",
        "        k_folds=K_FOLDS,\n",
        "        epochs=EPOCHS,\n",
        "        early_stopping_patience=5,  # Patience pour early stopping (réduite à 5)\n",
        "        min_delta=0.001             # Amélioration minimale\n",
        "    )\n",
        "\n",
        "    if results:\n",
        "        mean_acc = np.mean([r['best_val_acc'] for r in results])\n",
        "        std_acc = np.std([r['best_val_acc'] for r in results])\n",
        "\n",
        "        print(f\"\\n  Résultats finaux avec Early Stopping:\")\n",
        "        print(f\"   Accuracy moyenne: {mean_acc:.2f}% ± {std_acc:.2f}%\")\n",
        "        print(f\"   Détails par fold:\")\n",
        "\n",
        "        total_epochs_saved = 0\n",
        "        for r in results:\n",
        "            epochs_saved = EPOCHS - r['actual_epochs']\n",
        "            total_epochs_saved += epochs_saved\n",
        "            status = \"🛑\" if r['early_stopped'] else \"  \"\n",
        "            print(f\"     Fold {r['fold']}: {r['best_val_acc']:.2f}% ({r['actual_epochs']}/{EPOCHS} époques) {status}\")\n",
        "\n",
        "        efficiency = 100 * total_epochs_saved / (K_FOLDS * EPOCHS)\n",
        "        print(f\"\\n     Efficacité Early Stopping:\")\n",
        "        print(f\"   Époques économisées: {total_epochs_saved}/{K_FOLDS * EPOCHS} ({efficiency:.1f}%)\")\n",
        "\n",
        "        # Analyse des erreurs\n",
        "        analyze_prediction_errors(results)\n",
        "\n",
        "        # Courbes d'apprentissage (avec Early Stopping visible)\n",
        "        plot_learning_curves_with_early_stopping(results)\n",
        "\n",
        "        # Analyse des features spatiales\n",
        "        analyze_spatial_features_importance(results)\n",
        "\n",
        "        # Statistiques détaillées par relation\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(\"PERFORMANCE PAR RELATION SPATIALE\")\n",
        "        print(f\"{'='*60}\")\n",
        "\n",
        "        # Calculer les métriques globales\n",
        "        global_predictions = np.concatenate([r['final_predictions'] for r in results])\n",
        "        global_true_labels = np.concatenate([r['final_true_labels'] for r in results])\n",
        "\n",
        "        # Précision par classe\n",
        "        for i, relation in enumerate(SPATIAL_RELATIONS):\n",
        "            mask = global_true_labels == i\n",
        "            if np.sum(mask) > 0:\n",
        "                class_acc = 100 * np.mean(global_predictions[mask] == global_true_labels[mask])\n",
        "                support = np.sum(mask)\n",
        "                print(f\"  {relation:<15}: {class_acc:6.2f}% ({support:3d} échantillons)\")\n",
        "\n",
        "        print(f\"\\n   Expérience avec Early Stopping terminée!\")\n",
        "        print(f\"    Performance globale: {mean_acc:.2f}% ± {std_acc:.2f}%\")\n",
        "        print(f\"     Efficacité: {efficiency:.1f}% d'époques économisées\")\n",
        "\n",
        "        # Avantages de cette approche\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(\"AVANTAGES DE L'EARLY STOPPING\")\n",
        "        print(f\"{'='*60}\")\n",
        "        print(\"   Prévention de l'overfitting\")\n",
        "        print(\"   Réduction du temps d'entraînement\")\n",
        "        print(\"   Automatisation de l'arrêt optimal\")\n",
        "        print(\"   Restauration des meilleurs poids\")\n",
        "        print(\"   Meilleure généralisation\")\n",
        "\n",
        "        return results\n",
        "    else:\n",
        "        print(\"    Aucun résultat obtenu!\")\n",
        "        return None\n",
        "\n",
        "# =============================================================================\n",
        "# FONCTION DE TEST RAPIDE\n",
        "# =============================================================================\n",
        "\n",
        "def quick_test_image_bbox():\n",
        "    \"\"\"Test rapide avec Early Stopping pour validation\"\"\"\n",
        "\n",
        "    DATA_DIR = \"data/spatialsense\"\n",
        "\n",
        "    print(\"=\"*60)\n",
        "    print(\"TEST RAPIDE - IMAGE + BBOX COORDINATES + EARLY STOPPING\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    if not os.path.exists(DATA_DIR):\n",
        "        print(f\"ERREUR: {DATA_DIR} n'existe pas!\")\n",
        "        return\n",
        "\n",
        "    # Test avec paramètres réduits\n",
        "    results = train_kfold_with_confusion_matrix(\n",
        "        data_dir=DATA_DIR,\n",
        "        k_folds=3,               # Moins de folds\n",
        "        epochs=15,               # Époques pour tester l'early stopping\n",
        "        early_stopping_patience=3,  # Patience très réduite pour test rapide\n",
        "        min_delta=0.001\n",
        "    )\n",
        "\n",
        "    if results:\n",
        "        mean_acc = np.mean([r['best_val_acc'] for r in results])\n",
        "\n",
        "        print(f\"\\n     Test rapide terminé!\")\n",
        "        print(f\"   Accuracy moyenne: {mean_acc:.2f}%\")\n",
        "\n",
        "        # Statistiques Early Stopping\n",
        "        early_stopped_count = sum(1 for r in results if r['early_stopped'])\n",
        "        mean_epochs = np.mean([r['actual_epochs'] for r in results])\n",
        "\n",
        "        print(f\"\\n   Efficacité Early Stopping:\")\n",
        "        print(f\"   Folds avec arrêt anticipé: {early_stopped_count}/{len(results)}\")\n",
        "        print(f\"   Époques moyennes: {mean_epochs:.1f}/15\")\n",
        "\n",
        "        for r in results:\n",
        "            status = \"🛑 Arrêté\" if r['early_stopped'] else \"   Complet\"\n",
        "            print(f\"   Fold {r['fold']}: {r['actual_epochs']:2d} époques - {status}\")\n",
        "\n",
        "        # Vérification des features spatiales\n",
        "        print(f\"\\n   Vérification des features spatiales:\")\n",
        "        sample_dataset = ImageBBoxDataset(\n",
        "            data_dir=DATA_DIR,\n",
        "            split='train',\n",
        "            transform=None\n",
        "        )\n",
        "\n",
        "        if len(sample_dataset) > 0:\n",
        "            _, spatial_features, _, metadata = sample_dataset[0]\n",
        "            print(f\"   Nombre de features: {len(spatial_features)}\")\n",
        "            print(f\"   Features min: {spatial_features.min():.3f}\")\n",
        "            print(f\"   Features max: {spatial_features.max():.3f}\")\n",
        "            print(f\"   Features moyennes: {spatial_features.mean():.3f}\")\n",
        "\n",
        "        return results\n",
        "    else:\n",
        "        print(\"    Échec du test rapide!\")\n",
        "        return None\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Choix entre test rapide ou expérience complète\n",
        "    import sys\n",
        "\n",
        "    if len(sys.argv) > 1 and sys.argv[1] == \"quick\":\n",
        "        print(\"Mode test rapide avec Early Stopping activé...\")\n",
        "        quick_test_image_bbox()\n",
        "    else:\n",
        "        print(\"Mode expérience complète avec Early Stopping activé...\")\n",
        "        main_image_bbox_experiment()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "da6a9908b71a458bb32d27e7d1520a49",
            "15f61f2613eb46dbb4522491221dd773",
            "004a86d6b43148d6a8c592e8bdc92be8",
            "4a85fc626c3e415c96677bea65aa2c95",
            "60f6cf62f801402c91f46fca685486b5",
            "2fb95c983aec4139b37fb4a59c66555e",
            "9027b1f0fb874e56ba41cc10daea86a5",
            "0da71258f30647c29eee263864e924ab",
            "ca8e38ef94f6484d9c31fc2256baaa5c",
            "c52d270a9d4b4644bf5407f3f0f0cbd1",
            "330099d4952c4684a7624b91bb0359ac",
            "06b03b2212004abab376eda1f823a07a",
            "423dfb24ea344c04bf319bad059e85d4",
            "17311b765f81451dbc0df40221624d7b",
            "8c990038c4494629b48dcc294e2cde02",
            "dd9fa1211e9e46f090af03840778fc2e",
            "347eec727e4644cea85c2bdee820336b",
            "84d6d82cfcdc486baa98c3ccf7367351",
            "af497b30896e4710a4649b897d31e3a4",
            "9062e64f47744a9baa53a7a5a49c85bd",
            "875f60ff8b174e69bfdf18548a436a27",
            "17b64c6ddcc44effa96179e806a32b0f",
            "dd872bdfd00e471a8814f04e76575aa9",
            "beff29b49889472a8c33fb9020b0c49d",
            "f7d170c5e3ff470b88d8256c00c5a61c",
            "1d25eb82f76842f6a9cada56261d572d",
            "4fb01993c94d4b16b80c7fd621836c81",
            "0b64e5c2547a4478a770fa1a023a0417",
            "3407a8ebbd164e8a81bd91960116a361",
            "e394d322766a4241a5b9f64d79eb8c69",
            "c882f56940584f20ba5f05ff16471c04",
            "0e75dd1997804bc4bcceb080fa1715d3",
            "ecb1f0d35c794ac4a6a06a0bcb03c502",
            "8464e04ddcbe4d53b23dff39a5c3de2c",
            "b18e7ce391724c199a0bedf7b295a428",
            "e35173f87e23415fb0440149063f6dc9",
            "0f5fed8fd9944af18a895a32d63a40da",
            "d629e31d85394526be77754949084f07",
            "764faf3783b0434fac2df2d99e1fad67",
            "cc514d10e5c140a09c8a6969d7eddb32",
            "8680dbd9f37a48029cc34127f4b5a137",
            "1ac2e3a6980447559b58996269f9d2bf",
            "aba7e5f0fcbf485393178700f7462db4",
            "5280273be01f42238dd1e3f375e729cc",
            "8c5186525b9746c28283afacf781d27e",
            "bc90cd1805564a83b1cd8688c4c9a363",
            "27ffacba4b134eb6b74c9a467e5801d1",
            "52437dc5345f4506a8196d0d9c6de2ea",
            "cafc296409d640c2bc08ff39a1602d87",
            "4f2b417b69104f3aaa1cfd71199fff7c",
            "e2c237e82d86435aa7d05144eba14711",
            "3539d46706244894823104ceb4813afd",
            "0fbc6fa2170d4ac9991b66671e1ad9b8",
            "0743396c1ca64bc6b40dd308ff726314",
            "199c8ee291534bb4b38cddda9d32e65d"
          ]
        },
        "id": "u6ucK5Qk5sIQ",
        "outputId": "526bb362-1358-42c0-842f-52c0a53e2edd"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import json\n",
        "import os\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tqdm import tqdm\n",
        "from collections import Counter\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# BERT imports\n",
        "from transformers import BertTokenizer, BertModel\n",
        "\n",
        "# =============================================================================\n",
        "# CONFIGURATION GLOBALE\n",
        "# =============================================================================\n",
        "\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Device utilisé: {DEVICE}\")\n",
        "\n",
        "# Hyperparamètres\n",
        "BATCH_SIZE = 8  # Réduit à cause de BERT\n",
        "LEARNING_RATE = 0.001\n",
        "EPOCHS = 15\n",
        "K_FOLDS = 5\n",
        "IMG_SIZE = 224\n",
        "DROPOUT_RATE = 0.4\n",
        "\n",
        "# Relations spatiales SpatialSense+\n",
        "SPATIAL_RELATIONS = [\n",
        "    'above', 'behind', 'in', 'in front of', 'next to',\n",
        "    'on', 'to the left of', 'to the right of', 'under'\n",
        "]\n",
        "\n",
        "print(f\"Architecture MULTIMODALE: IMAGE + BBOX + BERT TEXT:\")\n",
        "print(f\"  - Relations: {len(SPATIAL_RELATIONS)}\")\n",
        "print(f\"  - Modalité 1: Image originale → VGG16 FC-7 (4096)\")\n",
        "print(f\"  - Modalité 2: Coordonnées BBox → MLP (28 → 512)\")\n",
        "print(f\"  - Modalité 3: Texte 'subject object' → BERT (768 → 512)\")\n",
        "print(f\"  - Fusion: Concaténation triple (5120 features)\")\n",
        "\n",
        "# =============================================================================\n",
        "# BERT TEXT ENCODER\n",
        "# =============================================================================\n",
        "\n",
        "class BERTTextEncoder(nn.Module):\n",
        "    \"\"\"Encodeur BERT pour les textes 'subject object'\"\"\"\n",
        "\n",
        "    def __init__(self, bert_model_name='bert-base-uncased', freeze_bert=True):\n",
        "        super(BERTTextEncoder, self).__init__()\n",
        "\n",
        "        # Charger BERT pré-entraîné\n",
        "        self.tokenizer = BertTokenizer.from_pretrained(bert_model_name)\n",
        "        self.bert = BertModel.from_pretrained(bert_model_name)\n",
        "\n",
        "        # Gel des paramètres BERT si demandé\n",
        "        if freeze_bert:\n",
        "            for param in self.bert.parameters():\n",
        "                param.requires_grad = False\n",
        "\n",
        "        self.bert_dim = self.bert.config.hidden_size  # 768 pour bert-base\n",
        "\n",
        "        # Projection des features BERT\n",
        "        self.text_projection = nn.Sequential(\n",
        "            nn.Linear(self.bert_dim, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(256, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3)\n",
        "        )\n",
        "\n",
        "        print(f\"BERT Text Encoder:\")\n",
        "        print(f\"  - Modèle: {bert_model_name}\")\n",
        "        print(f\"  - BERT dim: {self.bert_dim}\")\n",
        "        print(f\"  - Projection: {self.bert_dim} → 512\")\n",
        "        print(f\"  - Paramètres gelés: {freeze_bert}\")\n",
        "\n",
        "    def encode_text(self, texts, max_length=64):\n",
        "        \"\"\"Encode une liste de textes avec BERT\"\"\"\n",
        "        # Tokenisation\n",
        "        encoded = self.tokenizer(\n",
        "            texts,\n",
        "            padding=True,\n",
        "            truncation=True,\n",
        "            max_length=max_length,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        # Déplacer sur le bon device\n",
        "        input_ids = encoded['input_ids'].to(self.bert.device)\n",
        "        attention_mask = encoded['attention_mask'].to(self.bert.device)\n",
        "\n",
        "        # Passage dans BERT\n",
        "        with torch.no_grad() if not any(p.requires_grad for p in self.bert.parameters()) else torch.enable_grad():\n",
        "            outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "\n",
        "        # Utiliser le token [CLS] pour la représentation globale\n",
        "        cls_embeddings = outputs.last_hidden_state[:, 0, :]  # [batch_size, bert_dim]\n",
        "\n",
        "        return cls_embeddings\n",
        "\n",
        "    def forward(self, texts):\n",
        "        \"\"\"Forward pass pour l'encodage de texte\"\"\"\n",
        "        # Encoder avec BERT\n",
        "        bert_features = self.encode_text(texts)\n",
        "\n",
        "        # Projeter les features\n",
        "        projected_features = self.text_projection(bert_features)\n",
        "\n",
        "        return projected_features\n",
        "\n",
        "# =============================================================================\n",
        "# DATASET MULTIMODAL: IMAGE + BBOX + TEXT\n",
        "# =============================================================================\n",
        "\n",
        "class MultiModalDataset(Dataset):\n",
        "    \"\"\"Dataset SpatialSense+ multimodal avec:\n",
        "    - Image originale\n",
        "    - Coordonnées des bounding boxes comme features supplémentaires\n",
        "    - Texte 'subject object' encodé par BERT\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, data_dir, split='train', transform=None):\n",
        "        self.data_dir = data_dir\n",
        "        self.split = split\n",
        "        self.transform = transform\n",
        "\n",
        "        self.relation_to_idx = {rel: idx for idx, rel in enumerate(SPATIAL_RELATIONS)}\n",
        "        self.idx_to_relation = {idx: rel for rel, idx in self.relation_to_idx.items()}\n",
        "\n",
        "        self.annotations = self._load_annotations()\n",
        "        self.data_samples = self._prepare_samples()\n",
        "\n",
        "        print(f\"Dataset MultiModal SpatialSense+ {split}: {len(self.data_samples)} échantillons\")\n",
        "        if len(self.data_samples) > 0:\n",
        "            self._print_statistics()\n",
        "\n",
        "    def _load_annotations(self):\n",
        "        annotations_path = os.path.join(self.data_dir, 'annotations.json')\n",
        "        try:\n",
        "            with open(annotations_path, 'r') as f:\n",
        "                return json.load(f)\n",
        "        except FileNotFoundError:\n",
        "            print(f\"Erreur: {annotations_path} non trouvé!\")\n",
        "            return []\n",
        "\n",
        "    def _find_image_path(self, image_url):\n",
        "        base_dir = os.path.join(self.data_dir, \"images\", \"images\")\n",
        "        filename = os.path.basename(image_url)\n",
        "\n",
        "        if \"staticflickr\" in image_url or len(filename.split('_')) == 2:\n",
        "            return os.path.join(base_dir, \"flickr\", filename)\n",
        "        else:\n",
        "            return os.path.join(base_dir, \"nyu\", filename)\n",
        "\n",
        "    def _prepare_samples(self):\n",
        "        samples = []\n",
        "        images_not_found = 0\n",
        "\n",
        "        for img_data in self.annotations:\n",
        "            if img_data['split'] != self.split:\n",
        "                continue\n",
        "\n",
        "            img_path = self._find_image_path(img_data['url'])\n",
        "\n",
        "            if not os.path.exists(img_path):\n",
        "                images_not_found += 1\n",
        "                continue\n",
        "\n",
        "            for ann in img_data['annotations']:\n",
        "                if ann['label'] and ann['predicate'].lower().strip() in [rel.lower() for rel in SPATIAL_RELATIONS]:\n",
        "                    # Trouver la relation correspondante\n",
        "                    relation = None\n",
        "                    for rel in SPATIAL_RELATIONS:\n",
        "                        if rel.lower() == ann['predicate'].lower().strip():\n",
        "                            relation = rel\n",
        "                            break\n",
        "\n",
        "                    if relation and 'bbox' in ann['subject'] and 'bbox' in ann['object']:\n",
        "                        sample = {\n",
        "                            'image_path': img_path,\n",
        "                            'subject': ann['subject']['name'],\n",
        "                            'object': ann['object']['name'],\n",
        "                            'relation': relation,\n",
        "                            'original_relation': ann['predicate'],\n",
        "                            'subject_bbox': ann['subject']['bbox'],  # [y1, y2, x1, x2]\n",
        "                            'object_bbox': ann['object']['bbox'],   # [y1, y2, x1, x2]\n",
        "                            'image_width': img_data['width'],\n",
        "                            'image_height': img_data['height']\n",
        "                        }\n",
        "                        samples.append(sample)\n",
        "\n",
        "        if images_not_found > 0:\n",
        "            print(f\"Images non trouvées: {images_not_found}\")\n",
        "\n",
        "        return samples\n",
        "\n",
        "    def _normalize_bbox(self, bbox, img_width, img_height):\n",
        "        \"\"\"Normalise les coordonnées de bounding box entre 0 et 1\n",
        "        bbox format: [y1, y2, x1, x2]\n",
        "        \"\"\"\n",
        "        y1, y2, x1, x2 = bbox\n",
        "\n",
        "        # Normalisation\n",
        "        norm_y1 = y1 / img_height\n",
        "        norm_y2 = y2 / img_height\n",
        "        norm_x1 = x1 / img_width\n",
        "        norm_x2 = x2 / img_width\n",
        "\n",
        "        # Calcul des features géométriques\n",
        "        width = abs(norm_x2 - norm_x1)\n",
        "        height = abs(norm_y2 - norm_y1)\n",
        "        area = width * height\n",
        "        center_x = (norm_x1 + norm_x2) / 2\n",
        "        center_y = (norm_y1 + norm_y2) / 2\n",
        "        aspect_ratio = width / (height + 1e-8)  # Éviter division par zéro\n",
        "\n",
        "        return [norm_x1, norm_y1, norm_x2, norm_y2, width, height, area, center_x, center_y, aspect_ratio]\n",
        "\n",
        "    def _compute_spatial_features(self, subject_bbox, object_bbox, img_width, img_height):\n",
        "        \"\"\"Calcule des features spatiales entre les deux bounding boxes\"\"\"\n",
        "        # Normaliser les bounding boxes\n",
        "        subj_features = self._normalize_bbox(subject_bbox, img_width, img_height)\n",
        "        obj_features = self._normalize_bbox(object_bbox, img_width, img_height)\n",
        "\n",
        "        # Features individuelles (20 features: 10 + 10)\n",
        "        individual_features = subj_features + obj_features\n",
        "\n",
        "        # Features relationnelles\n",
        "        subj_center_x, subj_center_y = subj_features[7], subj_features[8]\n",
        "        obj_center_x, obj_center_y = obj_features[7], obj_features[8]\n",
        "\n",
        "        # Distance entre centres\n",
        "        distance = np.sqrt((subj_center_x - obj_center_x)**2 + (subj_center_y - obj_center_y)**2)\n",
        "\n",
        "        # Direction relative (angle)\n",
        "        angle = np.arctan2(obj_center_y - subj_center_y, obj_center_x - subj_center_x)\n",
        "\n",
        "        # Différences de taille\n",
        "        area_ratio = (subj_features[6] + 1e-8) / (obj_features[6] + 1e-8)\n",
        "\n",
        "        # Chevauchement (IoU approximatif)\n",
        "        subj_x1, subj_y1, subj_x2, subj_y2 = subj_features[:4]\n",
        "        obj_x1, obj_y1, obj_x2, obj_y2 = obj_features[:4]\n",
        "\n",
        "        # Intersection\n",
        "        inter_x1 = max(subj_x1, obj_x1)\n",
        "        inter_y1 = max(subj_y1, obj_y1)\n",
        "        inter_x2 = min(subj_x2, obj_x2)\n",
        "        inter_y2 = min(subj_y2, obj_y2)\n",
        "\n",
        "        if inter_x2 > inter_x1 and inter_y2 > inter_y1:\n",
        "            intersection = (inter_x2 - inter_x1) * (inter_y2 - inter_y1)\n",
        "            union = subj_features[6] + obj_features[6] - intersection\n",
        "            iou = intersection / (union + 1e-8)\n",
        "        else:\n",
        "            iou = 0.0\n",
        "            intersection = 0.0\n",
        "\n",
        "        # Position relative\n",
        "        relative_x = obj_center_x - subj_center_x\n",
        "        relative_y = obj_center_y - subj_center_y\n",
        "\n",
        "        # Features relationnelles (8 features)\n",
        "        relational_features = [\n",
        "            distance, angle, area_ratio, iou,\n",
        "            intersection, relative_x, relative_y,\n",
        "            1.0 if distance < 0.1 else 0.0  # Très proche\n",
        "        ]\n",
        "\n",
        "        # Total: 28 features (20 individuelles + 8 relationnelles)\n",
        "        return individual_features + relational_features\n",
        "\n",
        "    def _print_statistics(self):\n",
        "        relation_counts = Counter([s['relation'] for s in self.data_samples])\n",
        "        print(f\"\\nDistribution MultiModal dans {self.split}:\")\n",
        "        total = len(self.data_samples)\n",
        "\n",
        "        for relation in SPATIAL_RELATIONS:\n",
        "            count = relation_counts.get(relation, 0)\n",
        "            percentage = count / total * 100 if total > 0 else 0\n",
        "            print(f\"   {relation}: {count} ({percentage:.1f}%)\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data_samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sample = self.data_samples[idx]\n",
        "\n",
        "        try:\n",
        "            # Charger l'image originale\n",
        "            image = Image.open(sample['image_path']).convert('RGB')\n",
        "\n",
        "            # Appliquer les transformations\n",
        "            if self.transform:\n",
        "                image = self.transform(image)\n",
        "\n",
        "            # Calculer les features spatiales des bounding boxes\n",
        "            spatial_features = self._compute_spatial_features(\n",
        "                sample['subject_bbox'],\n",
        "                sample['object_bbox'],\n",
        "                sample['image_width'],\n",
        "                sample['image_height']\n",
        "            )\n",
        "\n",
        "            # Convertir en tensor\n",
        "            spatial_features = torch.tensor(spatial_features, dtype=torch.float32)\n",
        "\n",
        "            # Créer le texte 'subject object'\n",
        "            text = f\"{sample['subject']} {sample['object']}\"\n",
        "\n",
        "            label = self.relation_to_idx[sample['relation']]\n",
        "\n",
        "            metadata = {\n",
        "                'subject': sample['subject'],\n",
        "                'object': sample['object'],\n",
        "                'relation': sample['relation'],\n",
        "                'original_relation': sample['original_relation'],\n",
        "                'subject_bbox': sample['subject_bbox'],\n",
        "                'object_bbox': sample['object_bbox'],\n",
        "                'spatial_features_count': len(spatial_features),\n",
        "                'text': text\n",
        "            }\n",
        "\n",
        "            return image, spatial_features, text, label, metadata\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Erreur chargement {sample['image_path']}: {e}\")\n",
        "            # Données par défaut en cas d'erreur\n",
        "            dummy_image = Image.new('RGB', (224, 224), color='gray')\n",
        "\n",
        "            if self.transform:\n",
        "                dummy_image = self.transform(dummy_image)\n",
        "            else:\n",
        "                dummy_image = torch.zeros(3, 224, 224)\n",
        "\n",
        "            dummy_features = torch.zeros(28, dtype=torch.float32)\n",
        "            dummy_text = \"error error\"\n",
        "\n",
        "            return dummy_image, dummy_features, dummy_text, 0, {\n",
        "                'subject': 'error', 'object': 'error',\n",
        "                'relation': 'next to', 'original_relation': 'error',\n",
        "                'subject_bbox': [0, 0, 0, 0], 'object_bbox': [0, 0, 0, 0],\n",
        "                'spatial_features_count': 28, 'text': dummy_text\n",
        "            }\n",
        "\n",
        "# =============================================================================\n",
        "# ARCHITECTURE MULTIMODALE: VGG + BBOX + BERT\n",
        "# =============================================================================\n",
        "\n",
        "class MultiModalFeatureExtractor(nn.Module):\n",
        "    \"\"\"Extracteur de features multimodal combinant:\n",
        "    - Features VGG de l'image (4096)\n",
        "    - Features géométriques des bounding boxes (28 → 512)\n",
        "    - Features textuelles BERT (768 → 512)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, spatial_features_dim=28):\n",
        "        super(MultiModalFeatureExtractor, self).__init__()\n",
        "\n",
        "        self.spatial_features_dim = spatial_features_dim\n",
        "\n",
        "        # 1. VGG16 pré-entraîné pour l'image\n",
        "        vgg16 = models.vgg16(pretrained=True)\n",
        "        self.vgg_features = vgg16.features\n",
        "        self.vgg_avgpool = vgg16.avgpool\n",
        "        classifier_layers = list(vgg16.classifier.children())[:6]\n",
        "        self.vgg_fc7 = nn.Sequential(*classifier_layers)\n",
        "\n",
        "        # Gel des poids VGG\n",
        "        for param in self.vgg_features.parameters():\n",
        "            param.requires_grad = False\n",
        "        for param in self.vgg_avgpool.parameters():\n",
        "            param.requires_grad = False\n",
        "        for param in self.vgg_fc7.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        # 2. Projection des features spatiales BBox\n",
        "        self.spatial_projection = nn.Sequential(\n",
        "            nn.Linear(spatial_features_dim, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(256, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3)\n",
        "        )\n",
        "\n",
        "        # 3. Encodeur BERT pour le texte\n",
        "        self.text_encoder = BERTTextEncoder(freeze_bert=True)\n",
        "\n",
        "        # Dimension de sortie après fusion triple\n",
        "        self.fusion_dim = 4096 + 512 + 512  # VGG + spatial + text = 5120\n",
        "\n",
        "        print(f\"MultiModal Feature Extractor:\")\n",
        "        print(f\"  - VGG FC-7: 4096 features (gelées)\")\n",
        "        print(f\"  - Spatial features: {spatial_features_dim} → 512 (entraînables)\")\n",
        "        print(f\"  - BERT text features: 768 → 512 (projection entraînable)\")\n",
        "        print(f\"  - Fusion dim: {self.fusion_dim}\")\n",
        "\n",
        "    def forward(self, image, spatial_features, texts):\n",
        "        # 1. Extraction features VGG de l'image\n",
        "        vgg_features = self._extract_vgg_features(image)\n",
        "\n",
        "        # 2. Projection des features spatiales\n",
        "        projected_spatial = self.spatial_projection(spatial_features)\n",
        "\n",
        "        # 3. Encoding des features textuelles BERT\n",
        "        text_features = self.text_encoder(texts)\n",
        "\n",
        "        # 4. Fusion par concaténation\n",
        "        fused_features = torch.cat([vgg_features, projected_spatial, text_features], dim=1)\n",
        "\n",
        "        return fused_features\n",
        "\n",
        "    def _extract_vgg_features(self, x):\n",
        "        \"\"\"Extraction des features VGG FC-7\"\"\"\n",
        "        x = self.vgg_features(x)\n",
        "        x = self.vgg_avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.vgg_fc7(x)\n",
        "        return x\n",
        "\n",
        "class MultiModalMLP(nn.Module):\n",
        "    \"\"\"MLP adapté pour la classification avec features multimodales\"\"\"\n",
        "\n",
        "    def __init__(self, input_dim, hidden1_dim=1024, hidden2_dim=512,\n",
        "                 num_relations=len(SPATIAL_RELATIONS), dropout_rate=0.4):\n",
        "        super(MultiModalMLP, self).__init__()\n",
        "\n",
        "        print(f\"MultiModal Classification MLP:\")\n",
        "        print(f\"  - Input: {input_dim}\")\n",
        "        print(f\"  - Hidden 1: {hidden1_dim}\")\n",
        "        print(f\"  - Hidden 2: {hidden2_dim}\")\n",
        "        print(f\"  - Output: {num_relations}\")\n",
        "        print(f\"  - Dropout: {dropout_rate}\")\n",
        "\n",
        "        # Architecture MLP adaptée aux features multimodales\n",
        "        self.fc1 = nn.Linear(input_dim, hidden1_dim)\n",
        "        self.bn1 = nn.BatchNorm1d(hidden1_dim)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.dropout1 = nn.Dropout(dropout_rate)\n",
        "\n",
        "        self.fc2 = nn.Linear(hidden1_dim, hidden2_dim)\n",
        "        self.bn2 = nn.BatchNorm1d(hidden2_dim)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.dropout2 = nn.Dropout(dropout_rate)\n",
        "\n",
        "        self.fc3 = nn.Linear(hidden2_dim, num_relations)\n",
        "\n",
        "        # Initialisation Xavier\n",
        "        nn.init.xavier_uniform_(self.fc1.weight)\n",
        "        nn.init.xavier_uniform_(self.fc2.weight)\n",
        "        nn.init.xavier_uniform_(self.fc3.weight)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu1(x)\n",
        "        x = self.dropout1(x)\n",
        "\n",
        "        x = self.fc2(x)\n",
        "        x = self.bn2(x)\n",
        "        x = self.relu2(x)\n",
        "        x = self.dropout2(x)\n",
        "\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "class MultiModalSpatialRelationModel(nn.Module):\n",
        "    \"\"\"Modèle complet multimodal pour la classification des relations spatiales\"\"\"\n",
        "\n",
        "    def __init__(self, num_relations=len(SPATIAL_RELATIONS), spatial_features_dim=28):\n",
        "        super(MultiModalSpatialRelationModel, self).__init__()\n",
        "\n",
        "\n",
        "        # Extracteur de features multimodal\n",
        "        self.feature_extractor = MultiModalFeatureExtractor(spatial_features_dim=spatial_features_dim)\n",
        "\n",
        "        # Classifieur MLP\n",
        "        self.classifier = MultiModalMLP(\n",
        "            input_dim=self.feature_extractor.fusion_dim,  # 5120\n",
        "            num_relations=num_relations\n",
        "        )\n",
        "\n",
        "    def forward(self, image, spatial_features, texts):\n",
        "        # Extraction et fusion des features multimodales\n",
        "        fused_features = self.feature_extractor(image, spatial_features, texts)\n",
        "\n",
        "        # Classification\n",
        "        output = self.classifier(fused_features)\n",
        "        return output\n",
        "\n",
        "# =============================================================================\n",
        "# EARLY STOPPING\n",
        "# =============================================================================\n",
        "\n",
        "class EarlyStopping:\n",
        "    \"\"\"Early Stopping pour éviter l'overfitting\"\"\"\n",
        "\n",
        "    def __init__(self, patience=5, min_delta=0.001, restore_best_weights=True, verbose=True):\n",
        "        self.patience = patience\n",
        "        self.min_delta = min_delta\n",
        "        self.restore_best_weights = restore_best_weights\n",
        "        self.verbose = verbose\n",
        "\n",
        "        self.best_score = None\n",
        "        self.counter = 0\n",
        "        self.best_weights = None\n",
        "        self.early_stop = False\n",
        "\n",
        "    def __call__(self, val_score, model):\n",
        "        if self.best_score is None:\n",
        "            self.best_score = val_score\n",
        "            self.save_checkpoint(model)\n",
        "            if self.verbose:\n",
        "                print(f\"   Early Stopping: Score initial = {val_score:.3f}%\")\n",
        "\n",
        "        elif val_score < self.best_score + self.min_delta:\n",
        "            self.counter += 1\n",
        "            if self.verbose:\n",
        "                print(f\"     Early Stopping: {self.counter}/{self.patience} (Best: {self.best_score:.3f}%)\")\n",
        "\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "                if self.verbose:\n",
        "                    print(f\"Early Stopping déclenché! Restauration du meilleur modèle (Accuracy: {self.best_score:.3f}%)\")\n",
        "\n",
        "        else:\n",
        "            improvement = val_score - self.best_score\n",
        "            if self.verbose:\n",
        "                print(f\"    Amélioration: {improvement:.3f}% (Nouveau best: {val_score:.3f}%)\")\n",
        "\n",
        "            self.best_score = val_score\n",
        "            self.save_checkpoint(model)\n",
        "            self.counter = 0\n",
        "\n",
        "    def save_checkpoint(self, model):\n",
        "        if self.restore_best_weights:\n",
        "            self.best_weights = {key: value.cpu().clone() for key, value in model.state_dict().items()}\n",
        "\n",
        "    def restore_best_weights_to_model(self, model):\n",
        "        if self.best_weights is not None:\n",
        "            device = next(model.parameters()).device\n",
        "            best_weights_on_device = {key: value.to(device) for key, value in self.best_weights.items()}\n",
        "            model.load_state_dict(best_weights_on_device)\n",
        "\n",
        "# =============================================================================\n",
        "# FONCTIONS D'ÉVALUATION\n",
        "# =============================================================================\n",
        "\n",
        "def evaluate_multimodal_with_confusion_matrix(model, dataloader, criterion, device, class_names=None):\n",
        "    \"\"\"Évalue le modèle multimodal\"\"\"\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    all_predictions = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, spatial_features, texts, labels, _ in tqdm(dataloader, desc='Evaluating MultiModal'):\n",
        "            images = images.to(device)\n",
        "            spatial_features = spatial_features.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(images, spatial_features, texts)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "            all_predictions.extend(predicted.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    # Calculs des métriques\n",
        "    accuracy = 100. * np.mean(np.array(all_predictions) == np.array(all_labels))\n",
        "    avg_loss = running_loss / len(dataloader)\n",
        "\n",
        "    # Matrice de confusion\n",
        "    cm = confusion_matrix(all_labels, all_predictions)\n",
        "\n",
        "    # Rapport de classification\n",
        "    if class_names is None:\n",
        "        class_names = SPATIAL_RELATIONS\n",
        "\n",
        "    report = classification_report(\n",
        "        all_labels, all_predictions,\n",
        "        target_names=class_names,\n",
        "        output_dict=True,\n",
        "        zero_division=0\n",
        "    )\n",
        "\n",
        "    return avg_loss, accuracy, cm, report, all_predictions, all_labels\n",
        "\n",
        "def plot_confusion_matrix(cm, class_names, title=\"Matrice de Confusion\", normalize=False):\n",
        "    \"\"\"Affiche la matrice de confusion\"\"\"\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "        fmt = '.2f'\n",
        "        title += \" (Normalisée)\"\n",
        "    else:\n",
        "        fmt = 'd'\n",
        "\n",
        "    plt.figure(figsize=(12, 10))\n",
        "    sns.heatmap(cm,\n",
        "                annot=True,\n",
        "                fmt=fmt,\n",
        "                cmap='Blues',\n",
        "                xticklabels=class_names,\n",
        "                yticklabels=class_names,\n",
        "                cbar_kws={'label': 'Proportion' if normalize else 'Nombre de prédictions'})\n",
        "\n",
        "    plt.title(title, fontsize=16, fontweight='bold', pad=20)\n",
        "    plt.xlabel('Prédictions', fontsize=14, fontweight='bold')\n",
        "    plt.ylabel('Vraies étiquettes', fontsize=14, fontweight='bold')\n",
        "    plt.xticks(rotation=45, ha='right')\n",
        "    plt.yticks(rotation=0)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def display_classification_metrics(report, title=\"Métriques de Classification\"):\n",
        "    \"\"\"Affiche les métriques de classification\"\"\"\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"{title}\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    # Métriques par classe\n",
        "    print(f\"{'Classe':<15} {'Précision':<10} {'Rappel':<10} {'F1-Score':<10} {'Support':<10}\")\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "    for class_name in SPATIAL_RELATIONS:\n",
        "        if class_name in report:\n",
        "            metrics = report[class_name]\n",
        "            print(f\"{class_name:<15} {metrics['precision']:<10.3f} {metrics['recall']:<10.3f} \"\n",
        "                  f\"{metrics['f1-score']:<10.3f} {metrics['support']:<10.0f}\")\n",
        "\n",
        "    # Métriques globales\n",
        "    print(\"-\" * 60)\n",
        "    print(f\"{'Accuracy':<15} {'':<10} {'':<10} {report['accuracy']:<10.3f} {report['macro avg']['support']:<10.0f}\")\n",
        "    print(f\"{'Macro avg':<15} {report['macro avg']['precision']:<10.3f} {report['macro avg']['recall']:<10.3f} \"\n",
        "          f\"{report['macro avg']['f1-score']:<10.3f} {report['macro avg']['support']:<10.0f}\")\n",
        "    print(f\"{'Weighted avg':<15} {report['weighted avg']['precision']:<10.3f} {report['weighted avg']['recall']:<10.3f} \"\n",
        "          f\"{report['weighted avg']['f1-score']:<10.3f} {report['weighted avg']['support']:<10.0f}\")\n",
        "\n",
        "# =============================================================================\n",
        "# VISUALISATION\n",
        "# =============================================================================\n",
        "\n",
        "def visualize_multimodal_samples(dataset, num_samples=3):\n",
        "    \"\"\"Visualise des échantillons multimodaux\"\"\"\n",
        "    fig, axes = plt.subplots(num_samples, 2, figsize=(14, 4*num_samples))\n",
        "    if num_samples == 1:\n",
        "        axes = axes.reshape(1, -1)\n",
        "\n",
        "    for i in range(min(num_samples, len(dataset))):\n",
        "        image, spatial_features, text, label, metadata = dataset[i]\n",
        "\n",
        "        # Dénormaliser pour affichage\n",
        "        def denormalize(tensor):\n",
        "            mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
        "            std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n",
        "            return (tensor * std + mean).clamp(0, 1).permute(1, 2, 0)\n",
        "\n",
        "        image_display = denormalize(image)\n",
        "\n",
        "        # Image + texte\n",
        "        axes[i, 0].imshow(image_display)\n",
        "        axes[i, 0].set_title(f\"Image + Texte BERT\\n'{text}'\\nRelation: {metadata['relation']}\")\n",
        "        axes[i, 0].axis('off')\n",
        "\n",
        "        # Informations détaillées\n",
        "        axes[i, 1].axis('off')\n",
        "        info_text = f\"\"\"\n",
        "        Échantillon {i+1} - MULTIMODAL:\n",
        "\n",
        "        Sujet: {metadata['subject']}\n",
        "        Objet: {metadata['object']}\n",
        "        Relation: {metadata['relation']}\n",
        "        Texte BERT: \"{text}\"\n",
        "\n",
        "        Subject BBox: {metadata['subject_bbox']}\n",
        "        Object BBox: {metadata['object_bbox']}\n",
        "\n",
        "        Features Multimodales (5120 total):\n",
        "        • VGG features: 4096 (gelées)\n",
        "        • Spatial features: 28 → 512 (entraînables)\n",
        "        • BERT features: 768 → 512 (projection entraînable)\n",
        "\n",
        "        Architecture:\n",
        "        1. Image → VGG16 FC-7\n",
        "        2. BBox coords → MLP projection\n",
        "        3. \"subject object\" → BERT encoding\n",
        "        4. Fusion → Concaténation (5120)\n",
        "        5. Classification → MLP (5120→1024→512→9)\n",
        "        \"\"\"\n",
        "        axes[i, 1].text(0.1, 0.5, info_text, transform=axes[i, 1].transAxes,\n",
        "                       fontsize=9, verticalalignment='center',\n",
        "                       bbox=dict(boxstyle='round', facecolor='lightcyan', alpha=0.8))\n",
        "\n",
        "    plt.suptitle('Architecture Multimodale: Image + BBox + BERT Text',\n",
        "                 fontsize=16, fontweight='bold')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# =============================================================================\n",
        "# TRANSFORMATIONS\n",
        "# =============================================================================\n",
        "\n",
        "def create_transforms():\n",
        "    \"\"\"Transformations pour les images\"\"\"\n",
        "    normalize = transforms.Normalize(\n",
        "        mean=[0.485, 0.456, 0.406],\n",
        "        std=[0.229, 0.224, 0.225]\n",
        "    )\n",
        "\n",
        "    train_transform = transforms.Compose([\n",
        "        transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "        transforms.RandomHorizontalFlip(p=0.5),\n",
        "        transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.05),\n",
        "        transforms.ToTensor(),\n",
        "        normalize\n",
        "    ])\n",
        "\n",
        "    val_transform = transforms.Compose([\n",
        "        transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "        transforms.ToTensor(),\n",
        "        normalize\n",
        "    ])\n",
        "\n",
        "    return train_transform, val_transform\n",
        "\n",
        "# =============================================================================\n",
        "# FONCTIONS D'ENTRAÎNEMENT\n",
        "# =============================================================================\n",
        "\n",
        "def train_multimodal_epoch(model, dataloader, criterion, optimizer, device):\n",
        "    \"\"\"Entraîne une époque avec architecture multimodale\"\"\"\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    progress_bar = tqdm(dataloader, desc='Training MultiModal')\n",
        "    for batch_idx, (images, spatial_features, texts, labels, _) in enumerate(progress_bar):\n",
        "        images = images.to(device)\n",
        "        spatial_features = spatial_features.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images, spatial_features, texts)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "\n",
        "        # Gradient clipping\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "        progress_bar.set_postfix({\n",
        "            'loss': running_loss / (batch_idx + 1),\n",
        "            'acc': 100. * correct / total\n",
        "        })\n",
        "\n",
        "    return running_loss / len(dataloader), 100. * correct / total\n",
        "\n",
        "# =============================================================================\n",
        "# ENTRAÎNEMENT K-FOLD MULTIMODAL\n",
        "# =============================================================================\n",
        "\n",
        "def train_multimodal_kfold_with_confusion_matrix(data_dir, k_folds=5, epochs=15,\n",
        "                                                early_stopping_patience=5, min_delta=0.001):\n",
        "    \"\"\"Entraînement K-fold multimodal avec Early Stopping\"\"\"\n",
        "\n",
        "\n",
        "    # Transformations\n",
        "    train_transform, val_transform = create_transforms()\n",
        "\n",
        "    # Dataset multimodal\n",
        "    full_dataset = MultiModalDataset(\n",
        "        data_dir=data_dir,\n",
        "        split='train',\n",
        "        transform=train_transform\n",
        "    )\n",
        "\n",
        "    if len(full_dataset) == 0:\n",
        "        print(\"Dataset vide!\")\n",
        "        return []\n",
        "\n",
        "    # Visualisation d'échantillons\n",
        "    print(\"\\nVisualisation échantillons multimodaux:\")\n",
        "    visualize_multimodal_samples(full_dataset, num_samples=3)\n",
        "\n",
        "    # K-fold cross-validation\n",
        "    kfold = KFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
        "    fold_results = []\n",
        "\n",
        "    for fold, (train_indices, val_indices) in enumerate(kfold.split(full_dataset)):\n",
        "        print(f\"\\n{'='*50}\")\n",
        "        print(f\"FOLD {fold + 1}/{k_folds} - MULTIMODAL + EARLY STOPPING\")\n",
        "        print(f\"{'='*50}\")\n",
        "\n",
        "        # Sous-ensembles\n",
        "        train_subset = torch.utils.data.Subset(full_dataset, train_indices)\n",
        "        val_subset = torch.utils.data.Subset(full_dataset, val_indices)\n",
        "\n",
        "        # DataLoaders\n",
        "        train_loader = DataLoader(\n",
        "            train_subset, batch_size=BATCH_SIZE, shuffle=True,\n",
        "            num_workers=2, pin_memory=True\n",
        "        )\n",
        "\n",
        "        val_loader = DataLoader(\n",
        "            val_subset, batch_size=BATCH_SIZE, shuffle=False,\n",
        "            num_workers=2, pin_memory=True\n",
        "        )\n",
        "\n",
        "        # Modèle multimodal\n",
        "        model = MultiModalSpatialRelationModel(\n",
        "            num_relations=len(SPATIAL_RELATIONS),\n",
        "            spatial_features_dim=28\n",
        "        )\n",
        "        model = model.to(DEVICE)\n",
        "\n",
        "        # Optimiseur (entraîner seulement les couches non-gelées)\n",
        "        trainable_params = []\n",
        "        # Spatial projection (entraînable)\n",
        "        trainable_params.extend(model.feature_extractor.spatial_projection.parameters())\n",
        "        # BERT text projection (entraînable, BERT gelé)\n",
        "        trainable_params.extend(model.feature_extractor.text_encoder.text_projection.parameters())\n",
        "        # Classifier final (entraînable)\n",
        "        trainable_params.extend(model.classifier.parameters())\n",
        "\n",
        "        optimizer = optim.AdamW(trainable_params, lr=LEARNING_RATE, weight_decay=0.01)\n",
        "        criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
        "\n",
        "        # Scheduler\n",
        "        scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
        "\n",
        "        # Early Stopping\n",
        "        early_stopping = EarlyStopping(\n",
        "            patience=early_stopping_patience,\n",
        "            min_delta=min_delta,\n",
        "            restore_best_weights=True,\n",
        "            verbose=True\n",
        "        )\n",
        "\n",
        "        # Variables d'entraînement\n",
        "        train_losses, val_losses = [], []\n",
        "        train_accs, val_accs = [], []\n",
        "        actual_epochs = 0\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            print(f\"\\n   Epoch {epoch+1}/{epochs}\")\n",
        "\n",
        "            # Training multimodal\n",
        "            train_loss, train_acc = train_multimodal_epoch(\n",
        "                model, train_loader, criterion, optimizer, DEVICE\n",
        "            )\n",
        "            train_losses.append(train_loss)\n",
        "            train_accs.append(train_acc)\n",
        "\n",
        "            # Validation multimodale\n",
        "            val_loss, val_acc, cm, report, predictions, true_labels = evaluate_multimodal_with_confusion_matrix(\n",
        "                model, val_loader, criterion, DEVICE, SPATIAL_RELATIONS\n",
        "            )\n",
        "            val_losses.append(val_loss)\n",
        "            val_accs.append(val_acc)\n",
        "\n",
        "            scheduler.step()\n",
        "            actual_epochs = epoch + 1\n",
        "\n",
        "            print(f\"       Train: {train_loss:.4f} / {train_acc:.2f}%\")\n",
        "            print(f\"   Val: {val_loss:.4f} / {val_acc:.2f}%\")\n",
        "\n",
        "            # Sauvegarder le meilleur modèle\n",
        "            if len(val_accs) == 1 or val_acc > max(val_accs[:-1]):\n",
        "                torch.save(model.state_dict(), f'best_multimodal_model_fold_{fold+1}.pth')\n",
        "\n",
        "            # Vérification Early Stopping\n",
        "            early_stopping(val_acc, model)\n",
        "\n",
        "            if early_stopping.early_stop:\n",
        "                print(f\"🛑 Arrêt anticipé à l'époque {epoch+1}\")\n",
        "                early_stopping.restore_best_weights_to_model(model)\n",
        "                break\n",
        "\n",
        "        # Restaurer les meilleurs poids si pas d'arrêt anticipé\n",
        "        if not early_stopping.early_stop:\n",
        "            early_stopping.restore_best_weights_to_model(model)\n",
        "            print(f\"   Entraînement terminé ({epochs} époques). Meilleurs poids restaurés.\")\n",
        "\n",
        "        # Évaluation finale\n",
        "        print(f\"\\n{'='*40}\")\n",
        "        print(f\"ÉVALUATION FINALE FOLD {fold + 1}\")\n",
        "        print(f\"{'='*40}\")\n",
        "\n",
        "        final_val_loss, final_val_acc, final_cm, final_report, final_predictions, final_true_labels = evaluate_multimodal_with_confusion_matrix(\n",
        "            model, val_loader, criterion, DEVICE, SPATIAL_RELATIONS\n",
        "        )\n",
        "\n",
        "        # Affichage des métriques\n",
        "        display_classification_metrics(final_report, f\"Métriques Multimodales Fold {fold + 1}\")\n",
        "\n",
        "        # Matrices de confusion\n",
        "        plot_confusion_matrix(final_cm, SPATIAL_RELATIONS,\n",
        "                            f\"Matrice de Confusion Multimodale - Fold {fold + 1}\", normalize=False)\n",
        "        plot_confusion_matrix(final_cm, SPATIAL_RELATIONS,\n",
        "                            f\"Matrice de Confusion Multimodale Normalisée - Fold {fold + 1}\", normalize=True)\n",
        "\n",
        "        # Stockage des résultats\n",
        "        fold_results.append({\n",
        "            'fold': fold + 1,\n",
        "            'train_losses': train_losses,\n",
        "            'val_losses': val_losses,\n",
        "            'train_accs': train_accs,\n",
        "            'val_accs': val_accs,\n",
        "            'best_val_acc': early_stopping.best_score,\n",
        "            'final_confusion_matrix': final_cm,\n",
        "            'final_report': final_report,\n",
        "            'final_predictions': final_predictions,\n",
        "            'final_true_labels': final_true_labels,\n",
        "            'model': model,\n",
        "            'actual_epochs': actual_epochs,\n",
        "            'early_stopped': early_stopping.early_stop\n",
        "        })\n",
        "\n",
        "        print(f\"   Fold {fold + 1} - Meilleur: {early_stopping.best_score:.2f}% (après {actual_epochs} époques)\")\n",
        "        if early_stopping.early_stop:\n",
        "            print(f\"   🛑 Arrêt anticipé activé\")\n",
        "        else:\n",
        "            print(f\"      Entraînement complet\")\n",
        "\n",
        "    # Analyse globale des résultats\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(\"ANALYSE GLOBALE DES RÉSULTATS MULTIMODAUX\")\n",
        "    print(f\"{'='*70}\")\n",
        "\n",
        "    # Statistiques générales\n",
        "    mean_acc = np.mean([r['best_val_acc'] for r in fold_results])\n",
        "    std_acc = np.std([r['best_val_acc'] for r in fold_results])\n",
        "    print(f\"Accuracy moyenne: {mean_acc:.2f}% ± {std_acc:.2f}%\")\n",
        "\n",
        "    # Statistiques Early Stopping\n",
        "    early_stopped_folds = [r for r in fold_results if r['early_stopped']]\n",
        "    mean_epochs = np.mean([r['actual_epochs'] for r in fold_results])\n",
        "\n",
        "    print(f\"\\n   Statistiques Early Stopping:\")\n",
        "    print(f\"   Folds avec arrêt anticipé: {len(early_stopped_folds)}/{k_folds}\")\n",
        "    print(f\"   Époques moyennes: {mean_epochs:.1f}/{epochs}\")\n",
        "\n",
        "    for r in fold_results:\n",
        "        status = \"🛑 Arrêté\" if r['early_stopped'] else \"   Complet\"\n",
        "        print(f\"   Fold {r['fold']}: {r['actual_epochs']:2d} époques - {status}\")\n",
        "\n",
        "    # Matrice de confusion globale\n",
        "    global_cm = np.sum([r['final_confusion_matrix'] for r in fold_results], axis=0)\n",
        "    global_predictions = np.concatenate([r['final_predictions'] for r in fold_results])\n",
        "    global_true_labels = np.concatenate([r['final_true_labels'] for r in fold_results])\n",
        "\n",
        "    # Rapport global\n",
        "    global_report = classification_report(\n",
        "        global_true_labels, global_predictions,\n",
        "        target_names=SPATIAL_RELATIONS,\n",
        "        output_dict=True,\n",
        "        zero_division=0\n",
        "    )\n",
        "\n",
        "    # Affichage final\n",
        "    display_classification_metrics(global_report, \"Métriques Globales Multimodales\")\n",
        "    plot_confusion_matrix(global_cm, SPATIAL_RELATIONS,\n",
        "                        \"Matrice de Confusion Globale Multimodale\", normalize=False)\n",
        "    plot_confusion_matrix(global_cm, SPATIAL_RELATIONS,\n",
        "                        \"Matrice de Confusion Globale Multimodale Normalisée\", normalize=True)\n",
        "\n",
        "    return fold_results\n",
        "\n",
        "# =============================================================================\n",
        "# ANALYSE DES ERREURS\n",
        "# =============================================================================\n",
        "\n",
        "def analyze_prediction_errors(fold_results, top_k=5):\n",
        "    \"\"\"Analyse des erreurs de prédiction\"\"\"\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(\"ANALYSE DES ERREURS DE PRÉDICTION MULTIMODALES\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    # Combiner toutes les prédictions\n",
        "    all_predictions = np.concatenate([r['final_predictions'] for r in fold_results])\n",
        "    all_true_labels = np.concatenate([r['final_true_labels'] for r in fold_results])\n",
        "\n",
        "    # Identifier les erreurs\n",
        "    errors = []\n",
        "    for true_idx, pred_idx in zip(all_true_labels, all_predictions):\n",
        "        if true_idx != pred_idx:\n",
        "            true_relation = SPATIAL_RELATIONS[true_idx]\n",
        "            pred_relation = SPATIAL_RELATIONS[pred_idx]\n",
        "            errors.append((true_relation, pred_relation))\n",
        "\n",
        "    # Compter les erreurs\n",
        "    error_counts = Counter(errors)\n",
        "\n",
        "    print(f\"Nombre total d'erreurs: {len(errors)}\")\n",
        "    print(f\"Accuracy globale: {100 * (1 - len(errors) / len(all_predictions)):.2f}%\")\n",
        "    print(f\"\\nTop {top_k} erreurs les plus fréquentes:\")\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "    for i, ((true_rel, pred_rel), count) in enumerate(error_counts.most_common(top_k)):\n",
        "        percentage = 100 * count / len(errors)\n",
        "        print(f\"{i+1:2d}. {true_rel:>15} → {pred_rel:<15} : {count:3d} ({percentage:5.1f}%)\")\n",
        "\n",
        "def plot_learning_curves_multimodal(fold_results):\n",
        "    \"\"\"Affiche les courbes d'apprentissage multimodales\"\"\"\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "\n",
        "    axes[0, 0].set_title('Courbes de Loss - Training', fontweight='bold')\n",
        "    axes[0, 1].set_title('Courbes de Loss - Validation', fontweight='bold')\n",
        "    axes[1, 0].set_title('Courbes d\\'Accuracy - Training', fontweight='bold')\n",
        "    axes[1, 1].set_title('Courbes d\\'Accuracy - Validation', fontweight='bold')\n",
        "\n",
        "    colors = ['blue', 'red', 'green', 'orange', 'purple']\n",
        "\n",
        "    for i, result in enumerate(fold_results):\n",
        "        epochs = range(1, len(result['train_losses']) + 1)\n",
        "        color = colors[i % len(colors)]\n",
        "\n",
        "        # Style selon early stopping\n",
        "        linestyle = '--' if result['early_stopped'] else '-'\n",
        "        alpha = 0.8 if result['early_stopped'] else 0.7\n",
        "\n",
        "        label = f'Fold {result[\"fold\"]}'\n",
        "        if result['early_stopped']:\n",
        "            label += f' (ES@{result[\"actual_epochs\"]})'\n",
        "\n",
        "        # Plots\n",
        "        axes[0, 0].plot(epochs, result['train_losses'], color=color, label=label, alpha=alpha, linestyle=linestyle)\n",
        "        axes[0, 1].plot(epochs, result['val_losses'], color=color, label=label, alpha=alpha, linestyle=linestyle)\n",
        "        axes[1, 0].plot(epochs, result['train_accs'], color=color, label=label, alpha=alpha, linestyle=linestyle)\n",
        "        axes[1, 1].plot(epochs, result['val_accs'], color=color, label=label, alpha=alpha, linestyle=linestyle)\n",
        "\n",
        "        # Marquer le point d'arrêt\n",
        "        if result['early_stopped']:\n",
        "            stop_epoch = result['actual_epochs']\n",
        "            axes[1, 1].scatter(stop_epoch, result['val_accs'][stop_epoch-1],\n",
        "                             color=color, s=100, marker='X', zorder=5)\n",
        "\n",
        "    # Configuration\n",
        "    for ax in axes.flat:\n",
        "        ax.set_xlabel('Époque')\n",
        "        ax.legend()\n",
        "        ax.grid(True, alpha=0.3)\n",
        "\n",
        "    axes[0, 0].set_ylabel('Loss')\n",
        "    axes[0, 1].set_ylabel('Loss')\n",
        "    axes[1, 0].set_ylabel('Accuracy (%)')\n",
        "    axes[1, 1].set_ylabel('Accuracy (%)')\n",
        "\n",
        "    plt.suptitle('Courbes d\\'Apprentissage Multimodales\\n(X = Arrêt anticipé)',\n",
        "                 fontsize=16, fontweight='bold')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        " \n",
        "\n",
        "# =============================================================================\n",
        "# FONCTION PRINCIPALE\n",
        "# =============================================================================\n",
        "\n",
        "def main_multimodal_experiment():\n",
        "    \"\"\"Expérience principale multimodale\"\"\"\n",
        "\n",
        "    DATA_DIR = \"data/spatialsense\"\n",
        "\n",
        "\n",
        "    if not os.path.exists(DATA_DIR):\n",
        "        print(f\"\\nERREUR: {DATA_DIR} n'existe pas!\")\n",
        "        return\n",
        "\n",
        "    # Entraînement multimodal\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(\"LANCEMENT ENTRAÎNEMENT MULTIMODAL\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    results = train_multimodal_kfold_with_confusion_matrix(\n",
        "        data_dir=DATA_DIR,\n",
        "        k_folds=K_FOLDS,\n",
        "        epochs=EPOCHS,\n",
        "        early_stopping_patience=5,\n",
        "        min_delta=0.001\n",
        "    )\n",
        "\n",
        "    if results:\n",
        "        mean_acc = np.mean([r['best_val_acc'] for r in results])\n",
        "        std_acc = np.std([r['best_val_acc'] for r in results])\n",
        "\n",
        "        print(f\"\\n  RÉSULTATS FINAUX MULTIMODAUX:\")\n",
        "        print(f\"   Accuracy moyenne: {mean_acc:.2f}% ± {std_acc:.2f}%\")\n",
        "        print(f\"   Détails par fold:\")\n",
        "\n",
        "        total_epochs_saved = 0\n",
        "        for r in results:\n",
        "            epochs_saved = EPOCHS - r['actual_epochs']\n",
        "            total_epochs_saved += epochs_saved\n",
        "            status = \"🛑\" if r['early_stopped'] else \"  \"\n",
        "            print(f\"     Fold {r['fold']}: {r['best_val_acc']:.2f}% ({r['actual_epochs']}/{EPOCHS} époques) {status}\")\n",
        "\n",
        "        efficiency = 100 * total_epochs_saved / (K_FOLDS * EPOCHS)\n",
        "        print(f\"\\n     Efficacité Early Stopping: {efficiency:.1f}% d'époques économisées\")\n",
        "\n",
        "        # Analyses\n",
        "        analyze_prediction_errors(results)\n",
        "        plot_learning_curves_multimodal(results)\n",
        "\n",
        "        # Performance par relation\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(\"PERFORMANCE PAR RELATION SPATIALE\")\n",
        "        print(f\"{'='*60}\")\n",
        "\n",
        "        global_predictions = np.concatenate([r['final_predictions'] for r in results])\n",
        "        global_true_labels = np.concatenate([r['final_true_labels'] for r in results])\n",
        "\n",
        "        for i, relation in enumerate(SPATIAL_RELATIONS):\n",
        "            mask = global_true_labels == i\n",
        "            if np.sum(mask) > 0:\n",
        "                class_acc = 100 * np.mean(global_predictions[mask] == global_true_labels[mask])\n",
        "                support = np.sum(mask)\n",
        "                print(f\"  {relation:<15}: {class_acc:6.2f}% ({support:3d} échantillons)\")\n",
        "\n",
        "        print(f\"\\n   Expérience Multimodale terminée!\")\n",
        "        print(f\"    Performance: {mean_acc:.2f}% ± {std_acc:.2f}%\")\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(\"RÉCAPITULATIF ARCHITECTURE MULTIMODALE\")\n",
        "        print(f\"{'='*60}\")\n",
        "        print(\"   3 modalités fusionnées intelligemment\")\n",
        "        print(\"   VGG: Représentation visuelle riche\")\n",
        "        print(\"   BBox: Information spatiale précise\")\n",
        "        print(\"   BERT: Compréhension sémantique\")\n",
        "        print(\"   Early Stopping: Optimisation automatique\")\n",
        "        print(\"   Évaluation complète avec matrices de confusion\")\n",
        "\n",
        "        return results\n",
        "    else:\n",
        "        print(\"    Aucun résultat obtenu!\")\n",
        "        return None\n",
        "\n",
        "# =============================================================================\n",
        "# TEST RAPIDE\n",
        "# =============================================================================\n",
        "\n",
        "def quick_test_multimodal():\n",
        "    \"\"\"Test rapide multimodal\"\"\"\n",
        "\n",
        "    DATA_DIR = \"data/spatialsense\"\n",
        "\n",
        "    print(\"=\"*60)\n",
        "    print(\"TEST RAPIDE MULTIMODAL\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    if not os.path.exists(DATA_DIR):\n",
        "        print(f\"ERREUR: {DATA_DIR} n'existe pas!\")\n",
        "        return\n",
        "\n",
        "    results = train_multimodal_kfold_with_confusion_matrix(\n",
        "        data_dir=DATA_DIR,\n",
        "        k_folds=3,\n",
        "        epochs=8,\n",
        "        early_stopping_patience=3,\n",
        "        min_delta=0.001\n",
        "    )\n",
        "\n",
        "    if results:\n",
        "        mean_acc = np.mean([r['best_val_acc'] for r in results])\n",
        "        early_stopped_count = sum(1 for r in results if r['early_stopped'])\n",
        "        mean_epochs = np.mean([r['actual_epochs'] for r in results])\n",
        "\n",
        "        print(f\"\\nTest rapide terminé!\")\n",
        "        print(f\"   Accuracy moyenne: {mean_acc:.2f}%\")\n",
        "        print(f\"   Folds avec arrêt anticipé: {early_stopped_count}/3\")\n",
        "        print(f\"   Époques moyennes: {mean_epochs:.1f}/8\")\n",
        "\n",
        "        # Vérification modalités\n",
        "        sample_dataset = MultiModalDataset(data_dir=DATA_DIR, split='train', transform=None)\n",
        "        if len(sample_dataset) > 0:\n",
        "            _, spatial_features, text, _, metadata = sample_dataset[0]\n",
        "            print(f\"\\nVérification modalités:\")\n",
        "            print(f\"   Spatial features: {len(spatial_features)} dimensions\")\n",
        "            print(f\"   Texte BERT: '{text}'\")\n",
        "            print(f\"   Relation: {metadata['relation']}\")\n",
        "            print(f\"   Subject: {metadata['subject']}\")\n",
        "            print(f\"   Object: {metadata['object']}\")\n",
        "\n",
        "        return results\n",
        "    else:\n",
        "        print(\"Échec du test rapide!\")\n",
        "        return None\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    import sys\n",
        "\n",
        "    if len(sys.argv) > 1 and sys.argv[1] == \"quick\":\n",
        "        print(\"Mode test rapide multimodal avec Early Stopping...\")\n",
        "        quick_test_multimodal()\n",
        "    else:\n",
        "        print(\"Mode expérience complète multimodale avec Early Stopping...\")\n",
        "        main_multimodal_experiment()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "004a86d6b43148d6a8c592e8bdc92be8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0da71258f30647c29eee263864e924ab",
            "max": 48,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ca8e38ef94f6484d9c31fc2256baaa5c",
            "value": 48
          }
        },
        "04061f1be1ac44d3b25dfcc1a1bba11b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "059ff8ae8c3e48278d5e22836bbbc3ba": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "06b03b2212004abab376eda1f823a07a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_423dfb24ea344c04bf319bad059e85d4",
              "IPY_MODEL_17311b765f81451dbc0df40221624d7b",
              "IPY_MODEL_8c990038c4494629b48dcc294e2cde02"
            ],
            "layout": "IPY_MODEL_dd9fa1211e9e46f090af03840778fc2e"
          }
        },
        "0743396c1ca64bc6b40dd308ff726314": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0b64e5c2547a4478a770fa1a023a0417": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0ca959a218bd449ea6fbaef3a2c09208": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0da71258f30647c29eee263864e924ab": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0e75dd1997804bc4bcceb080fa1715d3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0ec9353b0919430cb4de9afa5db759e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_39f31f09248f43e2b81d620f1d87194e",
            "placeholder": "​",
            "style": "IPY_MODEL_58e59b2ffda643718c504bba1eed6bde",
            "value": "config.json: 100%"
          }
        },
        "0f5fed8fd9944af18a895a32d63a40da": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_aba7e5f0fcbf485393178700f7462db4",
            "placeholder": "​",
            "style": "IPY_MODEL_5280273be01f42238dd1e3f375e729cc",
            "value": " 570/570 [00:00&lt;00:00, 51.3kB/s]"
          }
        },
        "0fbc6fa2170d4ac9991b66671e1ad9b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0fbf1fd8314c42ecab0eeae86302d7c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_928abcb61b124d3eb1c0aebfc363b2e3",
            "placeholder": "​",
            "style": "IPY_MODEL_1f3b3721861d46aeb10fa75ed4785f08",
            "value": "model.safetensors: 100%"
          }
        },
        "14f5763d449b476682eb012f4249b50d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "157b49dddff3488e8527d762b4b757fb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "15f61f2613eb46dbb4522491221dd773": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2fb95c983aec4139b37fb4a59c66555e",
            "placeholder": "​",
            "style": "IPY_MODEL_9027b1f0fb874e56ba41cc10daea86a5",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "169b948522f348f1bebd73a89017559a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fafec77a2db84a77ade453a0016fa69b",
            "max": 231508,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a7826185cb2f4a99ae0fb19455879114",
            "value": 231508
          }
        },
        "17311b765f81451dbc0df40221624d7b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_af497b30896e4710a4649b897d31e3a4",
            "max": 231508,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9062e64f47744a9baa53a7a5a49c85bd",
            "value": 231508
          }
        },
        "17b64c6ddcc44effa96179e806a32b0f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "199c8ee291534bb4b38cddda9d32e65d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1a851ed291684256b758eb406c286c32": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6752d61de50f4a8486004541c1fe0e33",
            "max": 69665,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_83029fc1e0e44556902f018a5ca0acdd",
            "value": 69665
          }
        },
        "1ac2e3a6980447559b58996269f9d2bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1af981fd170e43fc89aec44d6ebd700e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_887951614ed745d2a12c40cb166f8ae0",
            "max": 346293852,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c3de2f853cf2464a8475d663ed08caef",
            "value": 346293852
          }
        },
        "1d25eb82f76842f6a9cada56261d572d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0e75dd1997804bc4bcceb080fa1715d3",
            "placeholder": "​",
            "style": "IPY_MODEL_ecb1f0d35c794ac4a6a06a0bcb03c502",
            "value": " 466k/466k [00:00&lt;00:00, 2.12MB/s]"
          }
        },
        "1f3b3721861d46aeb10fa75ed4785f08": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "229f7855b7784f64b037e9ea3749b0b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "23c1698c71744636acda658eaa56e85a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c6306b57268a444991e261fb0f7a8438",
            "max": 285,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6fa164c9a35946a782a5bd0e722cd97a",
            "value": 285
          }
        },
        "27ffacba4b134eb6b74c9a467e5801d1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3539d46706244894823104ceb4813afd",
            "max": 440449768,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0fbc6fa2170d4ac9991b66671e1ad9b8",
            "value": 440449768
          }
        },
        "29d1a46de0bb433db358ff9d3a23e66c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_909cc227af554e28896b685171819c9e",
            "placeholder": "​",
            "style": "IPY_MODEL_69b7d93bb20446c19d673e6368f8ea2d",
            "value": " 346M/346M [00:05&lt;00:00, 50.7MB/s]"
          }
        },
        "2c02c1edefdb4f8180b7edbb6c61fb58": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2e34544b5514446e9b1944c40d4e771c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e4b05b0367e0474ca4a350b0e5152e0e",
            "placeholder": "​",
            "style": "IPY_MODEL_76cccf964fb9404386e05ade2507ee60",
            "value": "model.safetensors: 100%"
          }
        },
        "2fb95c983aec4139b37fb4a59c66555e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "327d71562b2943ee9e2b9c6c3d1bad54": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7a8239ec4b5d4e30b7fd25dc1f1434bc",
            "placeholder": "​",
            "style": "IPY_MODEL_9dd6c972cd6240d2b48b0d21715c45e0",
            "value": "pytorch_model.bin: 100%"
          }
        },
        "330099d4952c4684a7624b91bb0359ac": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3407a8ebbd164e8a81bd91960116a361": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "347eec727e4644cea85c2bdee820336b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3539d46706244894823104ceb4813afd": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "39f31f09248f43e2b81d620f1d87194e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "423dfb24ea344c04bf319bad059e85d4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_347eec727e4644cea85c2bdee820336b",
            "placeholder": "​",
            "style": "IPY_MODEL_84d6d82cfcdc486baa98c3ccf7367351",
            "value": "vocab.txt: 100%"
          }
        },
        "43a215b749394c3d8016899ae77ff67b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d4ccc9e6180549b2bec9f3591e3f8ccb",
              "IPY_MODEL_169b948522f348f1bebd73a89017559a",
              "IPY_MODEL_be29257f455a4958a17945be8782fecd"
            ],
            "layout": "IPY_MODEL_75485ab146b8405ba4bdca21b50edfc4"
          }
        },
        "44d2f1d6ccb64f8b8a1a70973242f2ed": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4735f545d6a64610a667ff5e97ed2926": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_49b5f04a51b54083a8e0b0154e816833",
            "placeholder": "​",
            "style": "IPY_MODEL_0ca959a218bd449ea6fbaef3a2c09208",
            "value": "config.json: 100%"
          }
        },
        "49b5f04a51b54083a8e0b0154e816833": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4a85fc626c3e415c96677bea65aa2c95": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c52d270a9d4b4644bf5407f3f0f0cbd1",
            "placeholder": "​",
            "style": "IPY_MODEL_330099d4952c4684a7624b91bb0359ac",
            "value": " 48.0/48.0 [00:00&lt;00:00, 2.34kB/s]"
          }
        },
        "4bba3dff9df84e44b543b3c456fca820": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4f2b417b69104f3aaa1cfd71199fff7c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4fb01993c94d4b16b80c7fd621836c81": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4fc028900f30408d959af35b17e1f49f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4735f545d6a64610a667ff5e97ed2926",
              "IPY_MODEL_1a851ed291684256b758eb406c286c32",
              "IPY_MODEL_61cea817de014e5db5007231d6f34347"
            ],
            "layout": "IPY_MODEL_afaa66bd1eac43d5bd8cf1d79b7b4829"
          }
        },
        "52437dc5345f4506a8196d0d9c6de2ea": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0743396c1ca64bc6b40dd308ff726314",
            "placeholder": "​",
            "style": "IPY_MODEL_199c8ee291534bb4b38cddda9d32e65d",
            "value": " 440M/440M [00:17&lt;00:00, 35.3MB/s]"
          }
        },
        "5280273be01f42238dd1e3f375e729cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "52d67314ce3e41df9d5588d817231b0d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_44d2f1d6ccb64f8b8a1a70973242f2ed",
            "max": 17756393,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_229f7855b7784f64b037e9ea3749b0b4",
            "value": 17756393
          }
        },
        "555694256dfd4bc1b8d13be5d02563e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "572821628c4a45f8bd7dfe6be077f33e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9488f4cd0d37407f8df1af036068105e",
            "placeholder": "​",
            "style": "IPY_MODEL_14f5763d449b476682eb012f4249b50d",
            "value": " 17.8M/17.8M [00:00&lt;00:00, 60.4MB/s]"
          }
        },
        "58e59b2ffda643718c504bba1eed6bde": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5d6f379dbcb045da821e6e5f802206a3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "60f6cf62f801402c91f46fca685486b5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "61cea817de014e5db5007231d6f34347": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ca4fdce253c74ffdafa3197789acfcc3",
            "placeholder": "​",
            "style": "IPY_MODEL_157b49dddff3488e8527d762b4b757fb",
            "value": " 69.7k/69.7k [00:00&lt;00:00, 3.91MB/s]"
          }
        },
        "6752d61de50f4a8486004541c1fe0e33": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "69b7d93bb20446c19d673e6368f8ea2d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6e1e153e2f99470da86c55bc3a4c43b1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2e34544b5514446e9b1944c40d4e771c",
              "IPY_MODEL_1af981fd170e43fc89aec44d6ebd700e",
              "IPY_MODEL_29d1a46de0bb433db358ff9d3a23e66c"
            ],
            "layout": "IPY_MODEL_9576ce5bf660433aaad0685f73dd826b"
          }
        },
        "6fa164c9a35946a782a5bd0e722cd97a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "725b0a7f5000496b8f4f9a27a0d6accd": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "75485ab146b8405ba4bdca21b50edfc4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "764faf3783b0434fac2df2d99e1fad67": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "76cccf964fb9404386e05ade2507ee60": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7a8239ec4b5d4e30b7fd25dc1f1434bc": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8236ddd6b5ea4afc94a6c67f5614aed8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "83029fc1e0e44556902f018a5ca0acdd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8464e04ddcbe4d53b23dff39a5c3de2c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b18e7ce391724c199a0bedf7b295a428",
              "IPY_MODEL_e35173f87e23415fb0440149063f6dc9",
              "IPY_MODEL_0f5fed8fd9944af18a895a32d63a40da"
            ],
            "layout": "IPY_MODEL_d629e31d85394526be77754949084f07"
          }
        },
        "84d6d82cfcdc486baa98c3ccf7367351": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8680dbd9f37a48029cc34127f4b5a137": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "875f60ff8b174e69bfdf18548a436a27": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "887951614ed745d2a12c40cb166f8ae0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8c5186525b9746c28283afacf781d27e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_bc90cd1805564a83b1cd8688c4c9a363",
              "IPY_MODEL_27ffacba4b134eb6b74c9a467e5801d1",
              "IPY_MODEL_52437dc5345f4506a8196d0d9c6de2ea"
            ],
            "layout": "IPY_MODEL_cafc296409d640c2bc08ff39a1602d87"
          }
        },
        "8c990038c4494629b48dcc294e2cde02": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_875f60ff8b174e69bfdf18548a436a27",
            "placeholder": "​",
            "style": "IPY_MODEL_17b64c6ddcc44effa96179e806a32b0f",
            "value": " 232k/232k [00:00&lt;00:00, 11.7MB/s]"
          }
        },
        "9027b1f0fb874e56ba41cc10daea86a5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9062e64f47744a9baa53a7a5a49c85bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "909cc227af554e28896b685171819c9e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "928abcb61b124d3eb1c0aebfc363b2e3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9488f4cd0d37407f8df1af036068105e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "94ed908a0d2542319dff0f28e75d9050": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_327d71562b2943ee9e2b9c6c3d1bad54",
              "IPY_MODEL_52d67314ce3e41df9d5588d817231b0d",
              "IPY_MODEL_572821628c4a45f8bd7dfe6be077f33e"
            ],
            "layout": "IPY_MODEL_059ff8ae8c3e48278d5e22836bbbc3ba"
          }
        },
        "9576ce5bf660433aaad0685f73dd826b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9dd6c972cd6240d2b48b0d21715c45e0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9ee1b496d072430588cd11405940d395": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c51c1ebd52c14cb1ace379c241e4b9db",
            "placeholder": "​",
            "style": "IPY_MODEL_c8c4a5fbc0304a01adcf846f223c6276",
            "value": " 17.7M/17.7M [00:00&lt;00:00, 138MB/s]"
          }
        },
        "a163ba94f6ac44d491cbb287ef9c2bf8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a7826185cb2f4a99ae0fb19455879114": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a9f15fa581f3459b9e11500a26122d65": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aba7e5f0fcbf485393178700f7462db4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "adbbaa541c674ab6a6959218af3e26e8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2c02c1edefdb4f8180b7edbb6c61fb58",
            "max": 17743328,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5d6f379dbcb045da821e6e5f802206a3",
            "value": 17743328
          }
        },
        "af497b30896e4710a4649b897d31e3a4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "afaa66bd1eac43d5bd8cf1d79b7b4829": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b18e7ce391724c199a0bedf7b295a428": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_764faf3783b0434fac2df2d99e1fad67",
            "placeholder": "​",
            "style": "IPY_MODEL_cc514d10e5c140a09c8a6969d7eddb32",
            "value": "config.json: 100%"
          }
        },
        "bc90cd1805564a83b1cd8688c4c9a363": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4f2b417b69104f3aaa1cfd71199fff7c",
            "placeholder": "​",
            "style": "IPY_MODEL_e2c237e82d86435aa7d05144eba14711",
            "value": "model.safetensors: 100%"
          }
        },
        "be29257f455a4958a17945be8782fecd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a9f15fa581f3459b9e11500a26122d65",
            "placeholder": "​",
            "style": "IPY_MODEL_c01bb7e706854954b1cf7f6f4fa0dee8",
            "value": " 232k/232k [00:00&lt;00:00, 3.91MB/s]"
          }
        },
        "beff29b49889472a8c33fb9020b0c49d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0b64e5c2547a4478a770fa1a023a0417",
            "placeholder": "​",
            "style": "IPY_MODEL_3407a8ebbd164e8a81bd91960116a361",
            "value": "tokenizer.json: 100%"
          }
        },
        "c01bb7e706854954b1cf7f6f4fa0dee8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c3de2f853cf2464a8475d663ed08caef": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c51c1ebd52c14cb1ace379c241e4b9db": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c52d270a9d4b4644bf5407f3f0f0cbd1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c6306b57268a444991e261fb0f7a8438": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c6fc66c13740433fbc15c5be68f4e3d6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8236ddd6b5ea4afc94a6c67f5614aed8",
            "placeholder": "​",
            "style": "IPY_MODEL_04061f1be1ac44d3b25dfcc1a1bba11b",
            "value": " 285/285 [00:00&lt;00:00, 22.4kB/s]"
          }
        },
        "c882f56940584f20ba5f05ff16471c04": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c8c4a5fbc0304a01adcf846f223c6276": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ca4fdce253c74ffdafa3197789acfcc3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ca8e38ef94f6484d9c31fc2256baaa5c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "cafc296409d640c2bc08ff39a1602d87": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cc514d10e5c140a09c8a6969d7eddb32": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d1fadde324f34279995d85b85f83a3fd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0ec9353b0919430cb4de9afa5db759e6",
              "IPY_MODEL_23c1698c71744636acda658eaa56e85a",
              "IPY_MODEL_c6fc66c13740433fbc15c5be68f4e3d6"
            ],
            "layout": "IPY_MODEL_a163ba94f6ac44d491cbb287ef9c2bf8"
          }
        },
        "d4ccc9e6180549b2bec9f3591e3f8ccb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4bba3dff9df84e44b543b3c456fca820",
            "placeholder": "​",
            "style": "IPY_MODEL_555694256dfd4bc1b8d13be5d02563e6",
            "value": "vocab.txt: 100%"
          }
        },
        "d629e31d85394526be77754949084f07": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "da6a9908b71a458bb32d27e7d1520a49": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_15f61f2613eb46dbb4522491221dd773",
              "IPY_MODEL_004a86d6b43148d6a8c592e8bdc92be8",
              "IPY_MODEL_4a85fc626c3e415c96677bea65aa2c95"
            ],
            "layout": "IPY_MODEL_60f6cf62f801402c91f46fca685486b5"
          }
        },
        "dd872bdfd00e471a8814f04e76575aa9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_beff29b49889472a8c33fb9020b0c49d",
              "IPY_MODEL_f7d170c5e3ff470b88d8256c00c5a61c",
              "IPY_MODEL_1d25eb82f76842f6a9cada56261d572d"
            ],
            "layout": "IPY_MODEL_4fb01993c94d4b16b80c7fd621836c81"
          }
        },
        "dd9fa1211e9e46f090af03840778fc2e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e2c237e82d86435aa7d05144eba14711": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e35173f87e23415fb0440149063f6dc9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8680dbd9f37a48029cc34127f4b5a137",
            "max": 570,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1ac2e3a6980447559b58996269f9d2bf",
            "value": 570
          }
        },
        "e394d322766a4241a5b9f64d79eb8c69": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e4b05b0367e0474ca4a350b0e5152e0e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ecb1f0d35c794ac4a6a06a0bcb03c502": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "eed6ee698076417f97db42e3f273de12": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0fbf1fd8314c42ecab0eeae86302d7c3",
              "IPY_MODEL_adbbaa541c674ab6a6959218af3e26e8",
              "IPY_MODEL_9ee1b496d072430588cd11405940d395"
            ],
            "layout": "IPY_MODEL_725b0a7f5000496b8f4f9a27a0d6accd"
          }
        },
        "f7d170c5e3ff470b88d8256c00c5a61c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e394d322766a4241a5b9f64d79eb8c69",
            "max": 466062,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c882f56940584f20ba5f05ff16471c04",
            "value": 466062
          }
        },
        "fafec77a2db84a77ade453a0016fa69b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
